{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "MNIST = k.datasets.fashion_mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 0) | (y_train == 1)]\n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 0) | (y_test == 1)]\n",
    "X_test = X_test[y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 24, 24, 8)         1160      \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 16)                73744     \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 75,625\n",
      "Trainable params: 75,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model = k.Sequential([\n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\"),\n",
    "    k.layers.Conv2D(filters=8, kernel_size=3, activation=\"relu\"),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(16, activation=\"relu\"),\n",
    "    k.layers.Dense(16, activation=\"relu\"),\n",
    "    k.layers.Dense(16, activation=\"relu\"),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3578\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 191us/step - loss: 0.1190\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 200us/step - loss: 0.0721\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 195us/step - loss: 0.0686\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 214us/step - loss: 0.0586\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 217us/step - loss: 0.0584\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 220us/step - loss: 0.0532\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 222us/step - loss: 0.0486\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 210us/step - loss: 0.0452\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 222us/step - loss: 0.0410\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 1000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=SGD(lr=0.001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.787"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + float(b)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def conv_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume \n",
    "    n_H = n_H_prev - f + 1\n",
    "    n_W = n_W_prev - f + 1\n",
    "    \n",
    "    # Initialize the output volume Z.\n",
    "    Z = np.zeros([m, n_H, n_W, n_C])\n",
    "    \n",
    "    for i in range(m):   # loop over the batch of training examples\n",
    "       \n",
    "        # Select ith training example's activation\n",
    "        a_prev = A_prev[i]\n",
    "       \n",
    "        for h in range(n_H):           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                                        \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                    a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron.\n",
    "                    weights = W[:, :, :, c]\n",
    "                    biases = b[:, :, :, c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
    "                                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b) = cache\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    for i in range(m):  # loop over the training examples\n",
    "\n",
    "        # select ith training example from A_prev and dA_prev\n",
    "        a_prev = A_prev[i, :, :, :]\n",
    "        da_prev = dA_prev[i, :, :, :]\n",
    "\n",
    "        for h in range(n_H):  # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):  # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):  # loop over the channels of the output volume\n",
    "\n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = h + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = w + f\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev\n",
    "                    a_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters\n",
    "                    da_prev[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "\n",
    "        # Set the ith training example's dA_prev\n",
    "        dA_prev[i, :, :, :] = da_prev\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert (dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def flatten(A_prev):\n",
    "    m, *shape = A_prev.shape\n",
    "    return A_prev.reshape(m, np.prod(shape)).T, A_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    #grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = None\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = X_train[:200, :, :].reshape((200, 28, 28, 1))\n",
    "y = y_train[:200].values.reshape(1,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conv2D_1 -> filters=16, kernal_size=3, activation=\"relu\" (None, 26, 26, 16) #160\n",
    "Flatten_1 -> (None, 10816)\n",
    "Dense_1 -> units=32, activation=\"relu\" (None, 32) #346144\n",
    "Dense_2 -> units=1, activation=\"sigmoid\" (None, 1) #33\n",
    "\"\"\"\n",
    "def CNN(X, Y, epochs, learning_rate=0.001):\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = dict()\n",
    "    parameters[\"ConvW1\"] = np.random.randn(3, 3, 1, 16) * 0.01\n",
    "    parameters[\"Convb1\"] = np.zeros((1,1,1,16))\n",
    "    parameters[\"W1\"] = np.random.randn(32, 10816) * 0.01\n",
    "    parameters[\"b1\"] = np.zeros((32,1))\n",
    "    parameters[\"W2\"] = np.random.randn(1, 32) * 0.01\n",
    "    parameters[\"b2\"] = np.zeros((1,1))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        Z, cache_conv1 = conv_forward(X, parameters[\"ConvW1\"], parameters[\"Convb1\"])\n",
    "        Z, cache_relu1 = relu(Z) # apply relu activation function\n",
    "        Z, cache_flatten1 = flatten(Z) # flatten activation and store previous shape\n",
    "        Z, cache_dense1 = linear_activation_forward(Z, parameters[\"W1\"], parameters[\"b1\"], \"relu\")\n",
    "        AL, cache_dense2 = linear_activation_forward(Z, parameters[\"W2\"], parameters[\"b2\"], \"sigmoid\")\n",
    "        cost = compute_cost(AL, Y)\n",
    "        print(\"Cost epoch \", epoch, \": \", cost, sep=\"\")\n",
    "        grads = dict()\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "        grads[\"dA2\"], grads[\"dW2\"], grads[\"db2\"] = linear_activation_backward(dAL, cache_dense2, \"sigmoid\")\n",
    "        grads[\"dA1\"], grads[\"dW1\"], grads[\"db1\"] = linear_activation_backward(grads[\"dA2\"], cache_dense1, \"relu\")\n",
    "        grads[\"flatten1\"] = grads[\"dA1\"].reshape(cache_flatten1)\n",
    "        grads[\"relu1\"] = relu_backward(grads[\"flatten1\"], cache_relu1)\n",
    "        grads[\"dConvA1\"], grads[\"dConvW1\"], grads[\"dConvb1\"] = conv_backward(grads[\"relu1\"], cache_conv1)\n",
    "        \n",
    "        parameters[\"ConvW1\"] = parameters[\"ConvW1\"]-learning_rate*grads[\"dConvW1\"]\n",
    "        parameters[\"Convb1\"] = parameters[\"Convb1\"]-learning_rate*grads[\"dConvb1\"]\n",
    "        parameters[\"W1\"] = parameters[\"W1\"]-learning_rate*grads[\"dW1\"]\n",
    "        parameters[\"b1\"] = parameters[\"b1\"]-learning_rate*grads[\"db1\"]\n",
    "        parameters[\"W2\"] = parameters[\"W2\"]-learning_rate*grads[\"dW2\"]\n",
    "        parameters[\"b2\"] = parameters[\"b2\"]-learning_rate*grads[\"db2\"]\n",
    "        \n",
    "    return parameters, AL\n",
    "\n",
    "def predict(X, parameters):\n",
    "    Z, cache_conv1 = conv_forward(X, parameters[\"ConvW1\"], parameters[\"Convb1\"])\n",
    "    Z, cache_relu1 = relu(Z) # apply relu activation function\n",
    "    Z, cache_flatten1 = flatten(Z) # flatten activation and store previous shape\n",
    "    Z, cache_dense1 = linear_activation_forward(Z, parameters[\"W1\"], parameters[\"b1\"], \"relu\")\n",
    "    AL, cache_dense2 = linear_activation_forward(Z, parameters[\"W2\"], parameters[\"b2\"], \"sigmoid\")\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 0: 0.6825200198454259\n",
      "Cost epoch 1: 0.6622611625597173\n",
      "Cost epoch 2: 0.6002733147172207\n",
      "Cost epoch 3: 0.4338124892608679\n"
     ]
    }
   ],
   "source": [
    "parameters, AL = CNN(X,y, epochs=10, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y.flatten(), AL.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-510147934383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-41cf550b4ca8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ConvW1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Convb1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_relu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply relu activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_flatten1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten activation and store previous shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7c24f6e3f36f>\u001b[0m in \u001b[0;36mconv_forward\u001b[0;34m(A_prev, W, b)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_single_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_slice_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7c24f6e3f36f>\u001b[0m in \u001b[0;36mconv_single_step\u001b[0;34m(a_slice_prev, W, b)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_slice_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Sum over all entries of the volume s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 1930\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = X_test[:200, :, :].reshape((200, 28, 28, 1))\n",
    "y = y_test[:200].values.reshape(1,200)\n",
    "predictions = predict(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + \" \" + str(output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], neurons)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.01\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "            \n",
    "        self.dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "        self.db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.01\n",
    "        self.b = np.zeros((1,1,1,self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "\n",
    "        # Define dimensions\n",
    "        assert(A_prev.shape[1:4] == self.input_shape[1:4])\n",
    "        (m, H_prev, W_prev, C_prev) = A_prev.shape\n",
    "        (_, H, W, C) = self.output_shape\n",
    "\n",
    "        # Initialize the output volume Z.\n",
    "        Z = np.zeros((m, H, W, C))\n",
    "\n",
    "        for i in range(m):   # loop over the batch of training examples\n",
    "\n",
    "            # Select ith training example's activation\n",
    "            a_prev = A_prev[i]\n",
    "\n",
    "            for h in range(H):           # loop over vertical axis of the output volume\n",
    "                for w in range(W):       # loop over horizontal axis of the output volume\n",
    "                    for c in range(C):   # loop over channels (= #filters) of the output volume\n",
    "\n",
    "                        vert_start = h\n",
    "                        vert_end = vert_start + self.filter_size\n",
    "                        horiz_start = w\n",
    "                        horiz_end = horiz_start + self.filter_size\n",
    "\n",
    "                        # Use the corners to define the (3D) slice of a_prev_pad \n",
    "                        a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                        # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron.\n",
    "                        weights = self.W[:, :, :, c]\n",
    "                        biases = self.b[:, :, :, c]\n",
    "                        # Element-wise product between a_slice_prev and W.\n",
    "                        s = np.multiply(a_slice_prev, weights)\n",
    "                        # Sum over all entries of the volume s.\n",
    "                        z = np.sum(s)\n",
    "                        # Add bias b to z. Cast b to a float() so that z results in a scalar value.\n",
    "                        z = z + float(biases)\n",
    "                        Z[i, h, w, c] = z\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "\n",
    "        # Define dimensions\n",
    "        assert(A_prev.shape[1:4] == self.input_shape[1:4])\n",
    "        (m, H_prev, W_prev, C_prev) = A_prev.shape\n",
    "        (_, H, W, C) = self.output_shape\n",
    "\n",
    "        # Initialize dA_prev, dW, db with the correct shapes\n",
    "        dA_prev = np.zeros((m, H_prev, W_prev, C_prev))\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "\n",
    "        for i in range(m):  # loop over the training examples\n",
    "\n",
    "            # select ith training example from A_prev and dA_prev\n",
    "            a_prev = A_prev[i, :, :, :]\n",
    "            da_prev = dA_prev[i, :, :, :]\n",
    "\n",
    "            for h in range(H):  # loop over vertical axis of the output volume\n",
    "                for w in range(W):  # loop over horizontal axis of the output volume\n",
    "                    for c in range(C):  # loop over the channels of the output volume\n",
    "\n",
    "                        # Find the corners of the current \"slice\"\n",
    "                        vert_start = h\n",
    "                        vert_end = h + self.filter_size\n",
    "                        horiz_start = w\n",
    "                        horiz_end = w + self.filter_size\n",
    "\n",
    "                        # Use the corners to define the slice from a_prev\n",
    "                        a_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                        # Update gradients for the window and the filter's parameters\n",
    "                        da_prev[vert_start:vert_end, horiz_start:horiz_end, :] += self.W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                        self.dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                        self.db[:, :, :, c] += dZ[i, h, w, c]\n",
    "\n",
    "            # Set the ith training example's dA_prev\n",
    "            dA_prev[i, :, :, :] = da_prev\n",
    "            \n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape)\n",
    "           \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "        \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (np.prod(shape), m)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.reshape(m, np.prod(shape)).T\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        self.parameters = dict()\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, verbose): \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        for epoch in range(epochs):\n",
    "            # FORWARD PROP\n",
    "            Z = X\n",
    "            for layer in self.layers:\n",
    "                Z = layer.forward(Z)\n",
    "            \n",
    "            # COST FUNCTION\n",
    "            cost = compute_cost(Z, Y)\n",
    "            history.append(cost)\n",
    "            if verbose == 1:\n",
    "                print(\"Cost epoch \", epoch, \": \", cost, sep=\"\")\n",
    "\n",
    "            # BACKWARD PROP\n",
    "            dA = - (np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                dA = layer.backward(dA)\n",
    "            \n",
    "            # UPDATE PARAMS\n",
    "            for layer in self.layers:\n",
    "                layer.update_params(learning_rate)\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"--------------------\")\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"--------------------\")\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 0: 0.6931469606498413\n",
      "Cost epoch 1: 0.6931068985323955\n",
      "Cost epoch 2: 0.6930668564751009\n",
      "Cost epoch 3: 0.693026834469955\n",
      "Cost epoch 4: 0.6929868325061279\n",
      "Cost epoch 5: 0.6929468505840662\n",
      "Cost epoch 6: 0.692906888678058\n",
      "Cost epoch 7: 0.6928669467972179\n",
      "Cost epoch 8: 0.6928270248991937\n",
      "Cost epoch 9: 0.6927871229977409\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 10\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(16, 3, (None, 28, 28, 1)),\n",
    "    ReLU((None, 26, 26, 16)),\n",
    "    Conv2D(16, 3, (None, 26, 26, 16)),\n",
    "    ReLU((None, 24, 24, 16)),\n",
    "    Flatten((None, 24, 24, 16)),\n",
    "    Dense(32, (9216, None), \"relu\"),\n",
    "    Dense(16, (32, None), \"relu\"),\n",
    "    Dense(16, (16, None), \"relu\"),\n",
    "    Dense(1, (16, None), \"sigmoid\")\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "history = model.fit(X, y, epochs=10, learning_rate=0.001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
