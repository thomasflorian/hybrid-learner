{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "#MNIST = k.datasets.fashion_mnist.load_data()\n",
    "MNIST = k.datasets.mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 0) | (y_train == 1)]\n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 0) | (y_test == 1)]\n",
    "X_test = X_test[y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                346144    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 346,337\n",
      "Trainable params: 346,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = \"random_normal\" # random_normal or glorot_uniform\n",
    "keras_model = k.Sequential([ \n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.9317\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1456\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0891\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0680\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0569\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0500\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0456\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0411\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0378\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0356\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 1000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-83c309ec1f3e>:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    #print(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple, trainable=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + \" \" + str(self.output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (neurons, input_shape[1])\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.01\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "            \n",
    "        self.dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "        self.db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.001\n",
    "        self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform convolution\n",
    "        Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "        # Add bias\n",
    "        Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z.numpy()\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "        \n",
    "        dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.db = tf.raw_ops.BiasAddGrad(out_backprop=dZ).numpy()\n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.ksize = [1, pool_size, pool_size, 1]\n",
    "        self.strides = [1, pool_size, pool_size, 1]\n",
    "        output_shape = (input_shape[0], input_shape[1]//pool_size, input_shape[2]//pool_size, input_shape[3])\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        Z = tf.raw_ops.MaxPool(input=A_prev, ksize=self.ksize, strides=self.strides, data_format='NHWC', padding=\"VALID\").numpy()\n",
    "        self.cache = (A_prev, Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev, Z = self.cache\n",
    "        dA_prev = tf.raw_ops.MaxPoolGrad(orig_input=A_prev, orig_output=Z, grad=dZ, ksize=self.ksize, strides=self.strides, padding=\"VALID\", data_format='NHWC').numpy()\n",
    "        return dA_prev\n",
    "    \n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "           \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (np.prod(shape), m)\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.flatten().reshape(m,np.prod(shape)).T\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.T.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        self.parameters = dict()\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, verbose): \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        for epoch in range(epochs):\n",
    "            # FORWARD PROP\n",
    "            Z = X\n",
    "            for layer in self.layers:\n",
    "                \n",
    "                if layer.__str__().split()[0] == \"combo_method\":\n",
    "                    Z = layer.forward(Z, Y)\n",
    "                else:\n",
    "                    Z = layer.forward(Z)\n",
    "                print(layer, Z.shape)\n",
    "            \n",
    "            # COST FUNCTION\n",
    "            #print(Z.shape, Z)\n",
    "            cost = compute_cost(Z, Y)\n",
    "            history.append(cost)\n",
    "            if verbose == 1:\n",
    "                print(\"Cost epoch \", epoch, \": \", cost, sep=\"\")\n",
    "            #print(\"GRADCHECK\",self.gradcheck(Z,Y))\n",
    "            # BACKWARD PROP\n",
    "            m = Y.shape[1]\n",
    "            dA = -(1/m)*(np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                dA = layer.backward(dA)\n",
    "                \n",
    "#             #dA = dA.numpy()\n",
    "#             clip_val = 0.25\n",
    "#             dA[np.where(dA > clip_val)] = clip_val\n",
    "#             dA[np.where(dA < -clip_val)] = -clip_val\n",
    "#             print('-'*100)\n",
    "#             #print(\"GRADIENTS\",dA.shape, dA)\n",
    "#             print('-'*100)\n",
    "            \n",
    "            # UPDATE PARAMS\n",
    "            for layer in self.layers:\n",
    "                layer.update_params(learning_rate)\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"-\"*25)\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    def _cost(self, X, Y):\n",
    "        epsilon=1e-7\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            if prop_layer.__str__().split()[0] == \"combo_method\":\n",
    "               #J1 = prop_layer.forward(Z+ epsilon, Y) \n",
    "               #J2 = prop_layer.forward(Z- epsilon, Y) \n",
    "                \n",
    "                #print(\"TESTING: \",(J1-J2)/(2*epsilon))\n",
    "                \n",
    "                Z = prop_layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = prop_layer.forward(Z)\n",
    "            #print(prop_layer, Z)\n",
    "        # COMPUTE COST\n",
    "        return compute_cost(Z, Y)\n",
    "    \n",
    "    def gradcheck(self, X, Y, epsilon=1e-7, numlayers=None):\n",
    "        self.approx_grads = []\n",
    "        self.true_grads = []\n",
    "        for layer in self.layers[:numlayers]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "                \n",
    "            for i in range(layer.W.size):\n",
    "                i = np.unravel_index(i, layer.W.shape)\n",
    "                Wi = layer.W[i]\n",
    "                layer.W[i] = Wi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi\n",
    "                #print(J1)\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "                \n",
    "            for i in range(layer.b.size):\n",
    "                i = np.unravel_index(i, layer.b.shape)\n",
    "                bi = layer.b[i]\n",
    "                layer.b[i] = bi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.b[i] = bi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.b[i] = bi\n",
    "                #print((J1-J2))\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "        \n",
    "        # FORWARD PROP\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            #print(prop_layer)\n",
    "            if prop_layer.__str__().split()[0] == \"combo_method\":\n",
    "                Z = prop_layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = prop_layer.forward(Z)\n",
    "        # BACKWARD PROP\n",
    "        \n",
    "        m = Y.shape[1]\n",
    "\n",
    "        dA = -(1/m)*(np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        \n",
    "        for layer in self.layers[:numlayers]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            self.true_grads = np.concatenate((self.true_grads, layer.dW.flatten(), layer.db.flatten()))\n",
    "        return np.sqrt(np.sum(np.square(self.true_grads-self.approx_grads)))/(np.sqrt(np.sum(np.square(self.true_grads)))+np.sqrt(np.sum(np.square(self.approx_grads))))\n",
    "\n",
    "    \n",
    "class combo_method(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__(input_shape, num_classes, False)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_features, batch_labels):\n",
    "        self.batch_features = np.transpose(batch_features).astype('float64')\n",
    "#         print(\"BATCH_FEATURES\", self.batch_features)\n",
    "#         print()\n",
    "        #print(self.batch_features.type)\n",
    "        self.batch_labels = tf.cast(batch_labels, 'double')\n",
    "        #print(self.batch_labels.shape, self.batch_features.shape)\n",
    "        \n",
    "        self.distances = self.calc_distance_mtx(self.batch_features, self.batch_features)\n",
    "#         print(\"DISTANCES\",self.distances)\n",
    "#         print()\n",
    "        self.class_0 = tf.cast(self.batch_labels[:] == 0, 'double')\n",
    "        self.class_1 = tf.cast(self.batch_labels[:] == 1, 'double')\n",
    "\n",
    "        aggregate = -1*tf.stack([tf.reduce_sum(tf.multiply(self.distances, self.class_0), 1), tf.reduce_sum(tf.multiply(self.distances, self.class_1), 1)], axis=1)\n",
    "#         print(\"AGGREGATE\",aggregate)\n",
    "#         print()\n",
    "        self.softmax = tf.math.divide_no_nan(aggregate, tf.reduce_sum(aggregate, 1, keepdims=True))\n",
    "#         #aggregate = -1*aggregate*(10**-7)\n",
    "#         print(\"SOFTMAX\",self.softmax)\n",
    "#         print(\"=\"*20)\n",
    "        #return tf.reshape(self.softmax[:,0], (1,1000))\n",
    "        return tf.reshape(self.softmax[:,0], (1,self.batch_features.shape[0]))\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        #clip_val = 0.5\n",
    "        #self.loss = loss\n",
    "        #dL_dS = -tf.math.divide_no_nan(self.batch_labels, self.softmax[:,0]) + tf.math.divide_no_nan((1-self.batch_labels),(1-self.softmax[:,0]))\n",
    "        print(\"DA\", dA)\n",
    "        dS_dZ = -tf.tensordot(self.softmax[:,0],self.softmax[:,0], axes=0)\n",
    "        dS_dZ = tf.linalg.set_diag(dS_dZ, tf.math.multiply_no_nan(self.softmax[:,0],(1-self.softmax[:,0])))\n",
    "        #print(dS_dZ)\n",
    "        \n",
    "        dD_dX = np.ones(self.batch_features.shape)\n",
    "        for h in range(self.batch_features.shape[1]):\n",
    "            dz_dx = np.ones(self.distances.shape) \n",
    "            #if h%10 == 0:\n",
    "             #   print(h, \"out of\", self.batch_features.shape[1])\n",
    "            for i in range(self.distances.shape[0]):\n",
    "                #print(self.batch_features)\n",
    "                for j in range(self.distances.shape[1]):\n",
    "                    dz_dx[i,j] = -2*(self.batch_features[i,h] - self.batch_features[j,h])\n",
    "            dD_dX[:,h] = dz_dx.sum(axis=1)\n",
    "        #print(dD_dX)\n",
    "        #print(dA.shape, dS_dZ.shape, dD_dX.shape)\n",
    "        dL_dX = np.multiply(dA, dS_dZ)\n",
    "        print('-'*50)\n",
    "        print(\"DLDX\",dL_dX)\n",
    "        print('-'*50)\n",
    "        for i in range(dL_dX.shape[0]):\n",
    "            dD_dX = np.multiply(dL_dX[i,:].reshape(25,1),dD_dX)\n",
    "        #dL_dX = np.dot(dL_dX, dD_dX) \n",
    "        #print(dL_dX/10000000)\n",
    "#         dL_dX[np.where(dL_dX > clip_val)] = clip_val\n",
    "#         dL_dX[np.where(dL_dX < -clip_val)] = -clip_val\n",
    "#         print('-'*100)\n",
    "#         print(\"GRADIENTS\",dL_dX.shape, dL_dX)\n",
    "#         print('-'*100)\n",
    "        print(\"DDDX\",dD_dX)\n",
    "        return dD_dX\n",
    "    \n",
    "    def calc_distance_mtx(self, A, B):\n",
    "        \"\"\"\n",
    "        Computes squared pairwise distances between each elements of A and each elements of B.\n",
    "        Args:\n",
    "        A,    [m,d] matrix\n",
    "        B,    [n,d] matrix\n",
    "        Returns:\n",
    "        D,    [m,n] matrix of pairwise distances\n",
    "        \"\"\"\n",
    "        with tf.compat.v1.variable_scope('pairwise_dist'):\n",
    "            # squared norms of each row in A and B\n",
    "            na = tf.reduce_sum(tf.square(A), 1)\n",
    "            nb = tf.reduce_sum(tf.square(B), 1)\n",
    "\n",
    "            # na as a row and nb as a co\"lumn vectors\n",
    "            na = tf.reshape(na, [-1, 1])\n",
    "            nb = tf.reshape(nb, [1, -1])\n",
    "\n",
    "            # return pairwise euclidead difference matrix\n",
    "            D = tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0)\n",
    "        #print(D)\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D (None, 22, 22, 16) (1000, 22, 22, 16)\n",
      "Maxpool (None, 11, 11, 16) (1000, 11, 11, 16)\n",
      "Conv2D (None, 3, 3, 32) (1000, 3, 3, 32)\n",
      "Maxpool (None, 1, 1, 32) (1000, 1, 1, 32)\n",
      "Flatten (32, None) (32, 1000)\n",
      "combo_method 2 (1, 1000)\n",
      "Cost epoch 0: 0.2986363229780156\n",
      "DA [[ 0.00125278 -0.00126572 -0.00144655 -0.0014208  -0.00138503  0.00133812\n",
      "  -0.00124617 -0.00186897  0.00160335  0.0014803  -0.0012019   0.00126771\n",
      "   0.00131771 -0.00122745  0.00137474 -0.00119319  0.00194979  0.00143573\n",
      "  -0.00167843 -0.00133011  0.00141109 -0.00121313 -0.00123979  0.00136626\n",
      "   0.00150989  0.00130716 -0.00121644 -0.00142313 -0.00149887 -0.00121999\n",
      "   0.00171135 -0.00122124 -0.00123673  0.0013211   0.00137044  0.00126965\n",
      "   0.0013448  -0.00123832 -0.00123524 -0.00163452 -0.00120145  0.00137859\n",
      "   0.0014438  -0.00122762 -0.00122212 -0.00144784  0.0013683  -0.00155317\n",
      "  -0.00127807 -0.00122685  0.00137811 -0.00121267  0.00138225  0.00175694\n",
      "  -0.001215    0.0015871  -0.0015816   0.00140115 -0.00119468  0.00206339\n",
      "   0.00136986  0.00166529 -0.00152426  0.00139344 -0.00120182  0.00183021\n",
      "  -0.00120744 -0.00122737 -0.00121669  0.00129649 -0.00120412  0.00126461\n",
      "   0.001317    0.00126435 -0.00119681 -0.00124261 -0.00119442  0.00138772\n",
      "   0.00132359 -0.00121723 -0.00123539 -0.00123441 -0.0012285  -0.00145271\n",
      "   0.0013281  -0.00135383 -0.00134823 -0.00140446 -0.00120858 -0.00144964\n",
      "   0.00129733 -0.00146006 -0.00120534 -0.00123234  0.00129589  0.00129193\n",
      "   0.00133052  0.00144508 -0.00121945 -0.00123177 -0.00120429  0.00136759\n",
      "   0.00128988 -0.00129805 -0.00117547  0.00128177  0.00140203  0.00157987\n",
      "  -0.00120652 -0.00155341  0.00128463 -0.00121608 -0.00138259  0.00130659\n",
      "  -0.00140562 -0.00122337 -0.00121059 -0.00130395 -0.00119466  0.00130916\n",
      "   0.00144956  0.00133955  0.00156341 -0.00135277 -0.00138405 -0.00120057\n",
      "   0.00145361 -0.0014951  -0.00125134 -0.00124578 -0.00126444 -0.00120805\n",
      "   0.0012835   0.00145957 -0.00130494 -0.00126319  0.00137326  0.00135558\n",
      "  -0.0012368  -0.00124992  0.00134139 -0.00140073  0.00144702 -0.00132957\n",
      "  -0.00141744  0.00127615 -0.00120352  0.0014285   0.00144044  0.00152482\n",
      "   0.00167255  0.00168789  0.00128056 -0.00123956 -0.00125808 -0.00126239\n",
      "   0.00128819 -0.00123105 -0.00120131  0.00147831  0.00130883 -0.00128682\n",
      "  -0.00127728  0.00183242  0.00127369 -0.00145778  0.00127808  0.0013402\n",
      "  -0.0012444  -0.00121255  0.0016342  -0.00122429 -0.00133831  0.00126386\n",
      "  -0.00124633  0.00125565  0.00143973 -0.00120717  0.00133503 -0.001325\n",
      "   0.00136625 -0.0012143  -0.00121674 -0.00122943  0.00134707  0.00133475\n",
      "   0.00129467  0.00146303 -0.00161632 -0.00120258  0.00148025  0.00144663\n",
      "  -0.00121676  0.00158988 -0.00129151 -0.00123055 -0.00121482  0.00135473\n",
      "  -0.00122756  0.00182152  0.00139963 -0.00124365  0.00154446  0.0014716\n",
      "  -0.00127332 -0.00122966  0.00142326 -0.00146012  0.00152548 -0.00141804\n",
      "   0.00179847 -0.00122382  0.00154111  0.00148211 -0.00140048 -0.00127514\n",
      "  -0.00119476  0.00147088 -0.00121582  0.00143205  0.00158079  0.00151512\n",
      "  -0.0012483   0.0013551  -0.00119249  0.00170969  0.00147229 -0.00123817\n",
      "  -0.00131119 -0.00125575 -0.00119547 -0.00136313  0.00162499  0.00132928\n",
      "  -0.00125143 -0.00120251 -0.00121123 -0.00122348  0.00138245  0.00130735\n",
      "  -0.00119122  0.00166532  0.00127141 -0.00125275 -0.00119424 -0.00134802\n",
      "   0.001476   -0.00125445 -0.00121     0.00174392 -0.00136221  0.00131674\n",
      "  -0.00130144  0.00209289 -0.00118261  0.00140982 -0.00123464 -0.00139603\n",
      "   0.0015345  -0.00123845 -0.00119343  0.0013814  -0.00122811 -0.00117625\n",
      "  -0.00135081 -0.00130304 -0.00129574 -0.00127496 -0.00132474  0.00152256\n",
      "  -0.00126127 -0.00134776 -0.00132064  0.00140087  0.0014015  -0.00119486\n",
      "  -0.00129535  0.00145324  0.00157573 -0.00141579  0.00152504  0.00141105\n",
      "  -0.00130977  0.00134383  0.00134119 -0.00128508  0.00136485  0.00126264\n",
      "  -0.00133653  0.00177731  0.00135627  0.00211173 -0.00130879  0.0013351\n",
      "  -0.00130374  0.00171096 -0.00124756  0.00134891 -0.00122138 -0.00128432\n",
      "  -0.00154296  0.00199438 -0.00119885 -0.00121634  0.00129322 -0.00148094\n",
      "   0.00131026 -0.00121977  0.00129802  0.00135342 -0.0011935  -0.0012637\n",
      "   0.00132949  0.00131829  0.00353591  0.00132295 -0.00120658 -0.00126593\n",
      "  -0.00127824  0.00142854  0.0014726  -0.0015422  -0.00148661 -0.00150831\n",
      "   0.00130977  0.00146947 -0.00119589  0.00143961  0.00158166 -0.00128749\n",
      "   0.00227084 -0.00127028  0.00128609  0.00136536 -0.00122364  0.00129103\n",
      "   0.00140649 -0.00119407 -0.00127689  0.00138474 -0.00121044 -0.0013946\n",
      "  -0.0012027   0.00135156 -0.0012236  -0.00143527  0.0012555  -0.00120298\n",
      "  -0.00122101  0.0014055  -0.00150851 -0.00125561 -0.00120246 -0.00132836\n",
      "   0.001274   -0.00120257  0.00124149 -0.00142606  0.00130632 -0.0014763\n",
      "  -0.0011962   0.00130776  0.0013631   0.00146426 -0.00119841  0.00161724\n",
      "   0.00137535  0.001372    0.00128265 -0.00142651 -0.00118203  0.00154478\n",
      "   0.00136845  0.00147021 -0.00121834 -0.00123788 -0.00127766  0.00129013\n",
      "  -0.0013326   0.0013128  -0.00118416  0.00127909 -0.00141008 -0.00125405\n",
      "   0.00131327 -0.00128651 -0.00130801  0.00128366  0.00178119  0.00132463\n",
      "  -0.00122127  0.00126116  0.00199809  0.00127635 -0.00125368 -0.00120698\n",
      "  -0.0012337   0.00166767  0.00138169  0.00127626  0.00139562 -0.00123774\n",
      "  -0.00134417 -0.00117954  0.00130872  0.00142034  0.00130058 -0.00121513\n",
      "  -0.00125317  0.00131095  0.00139032 -0.00125996  0.00128168 -0.00140864\n",
      "  -0.00123124 -0.00128784 -0.00154559 -0.00120721 -0.00118161  0.00131613\n",
      "   0.0016703   0.00166575  0.00140793  0.00136556  0.00132071 -0.00119439\n",
      "   0.00129386 -0.00119764 -0.0014631   0.00125945  0.00135765 -0.0011878\n",
      "  -0.00120618  0.00142259  0.00139576 -0.00121176 -0.00119708 -0.0014499\n",
      "  -0.00120428  0.00138233  0.00136522 -0.00120786 -0.001189   -0.00119595\n",
      "   0.00131007 -0.00135424 -0.00121873 -0.00121357 -0.00129141  0.00127714\n",
      "   0.00197575  0.00134321 -0.00118163 -0.00126151 -0.00143258  0.001472\n",
      "  -0.00131105 -0.00120244  0.00126414  0.00133328 -0.00134843  0.00130814\n",
      "  -0.00150442  0.00133266  0.00185133  0.00137918 -0.00121083 -0.00150513\n",
      "  -0.00137264  0.0022036  -0.00144795 -0.0014203   0.00130198 -0.00129076\n",
      "   0.00128716  0.00125361 -0.00120732 -0.00127491  0.00175759  0.00169956\n",
      "   0.00129567 -0.00132638 -0.00123078 -0.00133051 -0.00143117  0.0012782\n",
      "  -0.0012052  -0.00138247 -0.00119287  0.00153102  0.00131291  0.00136317\n",
      "  -0.00156673  0.0012724  -0.00131396 -0.00141641 -0.00136118 -0.00121859\n",
      "  -0.00158856  0.00150644  0.00166878 -0.00126545  0.00134015 -0.00149224\n",
      "  -0.00121419 -0.00122469  0.00126927  0.00125759  0.00133467 -0.00122967\n",
      "  -0.00124423  0.00130222 -0.00125405  0.00127939  0.00157283  0.00131051\n",
      "  -0.00120313  0.00136913  0.00152208  0.00127986 -0.00129287 -0.0012028\n",
      "   0.00133777  0.00146051 -0.00119606 -0.00119582  0.00127794  0.00142167\n",
      "  -0.00120652  0.00141889  0.00161727 -0.00141877  0.00126072 -0.00123299\n",
      "  -0.00124906  0.00158797  0.00153064 -0.00135897  0.00130132 -0.00122427\n",
      "   0.00129813 -0.0012128   0.00126798  0.00127049 -0.00129519  0.00135207\n",
      "  -0.00119358 -0.0014276  -0.00119631 -0.00120577 -0.00142745 -0.00119883\n",
      "  -0.00119927 -0.00138558  0.00127919 -0.00135881 -0.0012008   0.0016665\n",
      "  -0.00119171 -0.00118853  0.0015175  -0.00143455 -0.00138158  0.00131607\n",
      "   0.00133087  0.00148241 -0.00149303 -0.00125428  0.00200443 -0.0012454\n",
      "   0.00138959 -0.00157714  0.00135921 -0.0012232  -0.00120018 -0.00118155\n",
      "   0.00136071 -0.00122435 -0.00125314  0.00180764 -0.0014126   0.0015535\n",
      "  -0.00138755 -0.00126972  0.00163607 -0.00133785 -0.00140841  0.00140511\n",
      "  -0.00142566  0.00132433 -0.00120621 -0.00125456  0.00132494 -0.00124128\n",
      "  -0.00120172  0.00138052 -0.0012359   0.00148598 -0.00118695  0.00135484\n",
      "   0.001363    0.00217248 -0.00123905 -0.00121327  0.00126994 -0.00155461\n",
      "   0.00128721 -0.00126955 -0.00120752  0.00131212 -0.00120449 -0.00120765\n",
      "   0.00131083  0.00149896  0.00149697 -0.00119898 -0.00125605  0.00159898\n",
      "   0.00132285 -0.00122862 -0.00121578 -0.00119966  0.00126595 -0.00143289\n",
      "   0.00163437 -0.0013038   0.00148145  0.0012943   0.00157456 -0.00139751\n",
      "   0.00136893  0.00132266  0.00126843 -0.00123785 -0.00119364  0.0015403\n",
      "   0.00138937 -0.00118965 -0.00159781 -0.00121555 -0.00124901  0.00142434\n",
      "   0.0015134  -0.00122441 -0.00121965 -0.00136333 -0.00139589  0.00150149\n",
      "  -0.00138683  0.00129851 -0.00121368  0.00130628  0.00128852 -0.00119013\n",
      "  -0.0012054  -0.00118618  0.00127569  0.00139801 -0.00119285  0.00136488\n",
      "  -0.00121299  0.00126567 -0.00140342  0.00136933 -0.00125772 -0.00139596\n",
      "   0.00153052 -0.0014397  -0.00118885  0.00153005  0.00162106  0.0012934\n",
      "   0.00133954  0.00137848  0.00156602 -0.00151543  0.00124039 -0.00119041\n",
      "  -0.0014621   0.00130489 -0.00122503  0.00136247 -0.00127796  0.00133015\n",
      "  -0.00128202 -0.00119719  0.00205032  0.00143094 -0.0012059  -0.00119655\n",
      "   0.00142381  0.00129314 -0.0012281  -0.00124093  0.00141938 -0.00134365\n",
      "  -0.00122674  0.00128251 -0.00119308  0.0013985   0.00140607 -0.00122114\n",
      "  -0.00127854 -0.00119114 -0.00121704  0.00130399 -0.00122753 -0.00119565\n",
      "   0.0015481  -0.00121597  0.00181395  0.00129746 -0.00141233  0.00214419\n",
      "  -0.0015288  -0.00119182 -0.00124006 -0.00118891  0.00182132 -0.00120318\n",
      "  -0.00123547  0.00139997  0.00144772 -0.00131324  0.00137694  0.00145579\n",
      "   0.00134206 -0.00136667  0.00131772 -0.00146859  0.00139681 -0.00135353\n",
      "  -0.00119728 -0.00122305 -0.00121779 -0.00129414 -0.00123543  0.00134504\n",
      "   0.00132823 -0.00123853  0.00140904  0.00141044 -0.0012251   0.00161701\n",
      "  -0.00122998 -0.00127457 -0.00121168  0.00136    -0.00123482 -0.00122716\n",
      "   0.00128549  0.00127903  0.00152305 -0.00124738  0.00155244 -0.0013577\n",
      "   0.00141188  0.00138514  0.00150875 -0.00119797  0.00137089  0.00156787\n",
      "   0.00144322  0.00136224  0.00126173 -0.00122201  0.00130154 -0.00119655\n",
      "   0.00136648  0.00149496 -0.001252   -0.0012029  -0.00122458 -0.00119211\n",
      "   0.00147228  0.00133818 -0.00120904 -0.00152407 -0.00119956 -0.00119619\n",
      "   0.00133672  0.0013468   0.00146143 -0.00120477 -0.00141954 -0.00143033\n",
      "   0.00171489  0.00139943 -0.00123006 -0.0014545   0.00134072 -0.00119773\n",
      "   0.00133089 -0.00132491 -0.00123261  0.00172646 -0.00122086  0.00127631\n",
      "   0.00133203  0.00183069 -0.00120576  0.00146391 -0.0011988   0.00130893\n",
      "  -0.00122219 -0.0012262  -0.00137516 -0.00126602  0.00171897 -0.00121791\n",
      "   0.00151394  0.00124932  0.00128458  0.00141288 -0.00144774 -0.00119983\n",
      "  -0.0012689  -0.00128258 -0.00120731  0.00144576 -0.00123222 -0.00120695\n",
      "  -0.00126635  0.0012737  -0.00119143  0.00131839  0.00139972 -0.00130273\n",
      "  -0.00144025 -0.00120917 -0.00194916  0.0012633   0.00136376 -0.00128745\n",
      "   0.00126381  0.00146536 -0.00126248 -0.00141668 -0.00219593  0.00124842\n",
      "  -0.00124145 -0.0011898   0.00131493 -0.00118924 -0.00123662  0.00156668\n",
      "   0.00138936 -0.00128712  0.00131469 -0.00120483  0.00128623 -0.00137289\n",
      "  -0.00125799 -0.00120707 -0.00119412  0.00248184  0.00143129 -0.00124017\n",
      "  -0.00120645  0.00124688 -0.00124775 -0.00122152 -0.00134173 -0.00119077\n",
      "   0.00129205 -0.00120769 -0.00119234 -0.00120241  0.0015402  -0.00132332\n",
      "   0.00129629  0.00144644  0.00150861  0.00131008 -0.0012754  -0.00123664\n",
      "   0.00179534  0.00127649 -0.00146591  0.00153355 -0.00118846  0.00128649\n",
      "   0.00137145  0.00144103 -0.0012139   0.00126985  0.00139892 -0.00118737\n",
      "  -0.00130243 -0.00119804  0.00131387 -0.00124144  0.00146623 -0.00126716\n",
      "   0.00145538  0.00164327 -0.00126112  0.001886    0.00135962  0.00141694\n",
      "   0.00124413 -0.00126047  0.00132075  0.00133201 -0.001207    0.00129269\n",
      "  -0.00120913 -0.00122415 -0.0012144  -0.00121172  0.00131951 -0.00126969\n",
      "  -0.00127311 -0.0012778   0.0013875  -0.00119945  0.00136543  0.0012672\n",
      "   0.00136692 -0.00154783 -0.00122074 -0.0013003   0.00135562 -0.00137743\n",
      "  -0.00122591  0.00168717 -0.00126848  0.00166522  0.00150997 -0.00178767\n",
      "  -0.00147175  0.00128369 -0.00120594 -0.00120752  0.00134189  0.00146644\n",
      "  -0.00144533  0.0013832   0.00129234 -0.00119837 -0.00121451  0.00152869\n",
      "  -0.00120305 -0.00122351  0.0012869   0.00125161 -0.00121364 -0.00124516\n",
      "  -0.00122221 -0.00118927  0.00135927  0.0012649  -0.00142288  0.00142826\n",
      "   0.00139021 -0.00126253  0.00152472  0.0013031  -0.00119054  0.0015495\n",
      "  -0.00140252  0.00134756 -0.001335    0.00135487  0.00164775 -0.00120499\n",
      "  -0.00148012  0.00146178 -0.00123987  0.00157786  0.00194777 -0.00152893\n",
      "  -0.00120532 -0.00151421  0.00135749 -0.00157539  0.00153172  0.00145628\n",
      "  -0.00137134  0.00158126  0.0015572   0.00126149  0.00149257 -0.00131145\n",
      "  -0.00145625  0.00166397 -0.00139634 -0.00135627 -0.00123001 -0.00118975\n",
      "   0.00150646 -0.00150121  0.00142772 -0.00138483  0.0015259   0.00215806\n",
      "  -0.00133444 -0.00123171 -0.00146745 -0.00147269]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-d2d6766fe881>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Create and train modela\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-174-70e35c2c728d>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, epochs, learning_rate, verbose)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                 \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;31m#             #dA = dA.numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-174-70e35c2c728d>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dA)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;31m#print(self.batch_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                     \u001b[0mdz_dx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[0mdD_dX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdz_dx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m#print(dD_dX)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 1000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(16, 7, (None, 28, 28, 1)),\n",
    "    #ReLU((None, 22, 22, 16)),\n",
    "    Maxpool((None, 22, 22, 16)),\n",
    "    Conv2D(32, 9, (None, 11, 11, 16)),\n",
    "    #ReLU((None, 8, 8, 32)),\n",
    "    Maxpool((None, 3, 3, 32)),\n",
    "    Flatten((None, 1, 1, 32)),\n",
    "    #Dense(32, (5184, None), \"relu\"),\n",
    "    #Dense(1, (32, None), \"sigmoid\")\n",
    "    combo_method((64, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train modela\n",
    "model = Model(layers)\n",
    "history = model.fit(X, y, epochs=15, learning_rate=0.00001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA [[ 0.05940199 -0.05739828 -0.05925192 -0.05779887 -0.05726435  0.0601368\n",
      "  -0.05741711 -0.0743176   0.06368447  0.05839941 -0.0554213   0.05768546\n",
      "   0.05938394 -0.05795801  0.05913187 -0.05428207  0.07720047  0.05713932\n",
      "  -0.06520205 -0.05512086  0.05823621 -0.05708099 -0.0580739   0.05743607\n",
      "   0.06453875]]\n",
      "--------------------------------------------------\n",
      "DLDX [[ 0.01306487  0.01306487  0.01306487  0.01306487  0.01306487 -0.00657712\n",
      "   0.01306487  0.01306487 -0.00773586 -0.00600965  0.01306487 -0.00577646\n",
      "  -0.00633122  0.01306487 -0.00624888  0.01306487 -0.01215048 -0.00559808\n",
      "   0.01306487  0.01306487 -0.00595634  0.01306487  0.01306487 -0.005695\n",
      "  -0.00801489]\n",
      " [-0.01352095 -0.0121246   0.0278754   0.0278754   0.0278754  -0.01403303\n",
      "   0.0278754   0.0278754  -0.01650535 -0.01282227  0.0278754  -0.01232473\n",
      "  -0.01350838  0.0278754  -0.01333271  0.0278754  -0.02592445 -0.01194414\n",
      "   0.0278754   0.0278754  -0.01270854  0.0278754   0.0278754  -0.01215094\n",
      "  -0.01710068]\n",
      " [-0.01309796  0.02700334 -0.01299666  0.02700334  0.02700334 -0.01359402\n",
      "   0.02700334  0.02700334 -0.015989   -0.01242114  0.02700334 -0.01193916\n",
      "  -0.01308578  0.02700334 -0.01291561  0.02700334 -0.02511343 -0.01157048\n",
      "   0.02700334  0.02700334 -0.01231097  0.02700334  0.02700334 -0.01177081\n",
      "  -0.01656571]\n",
      " [-0.01342724  0.0276822   0.0276822  -0.0123178   0.0276822  -0.01393577\n",
      "   0.0276822   0.0276822  -0.01639096 -0.0127334   0.0276822  -0.01223931\n",
      "  -0.01341475  0.0276822  -0.01324031  0.0276822  -0.02574477 -0.01186136\n",
      "   0.0276822   0.0276822  -0.01262046  0.0276822   0.0276822  -0.01206672\n",
      "  -0.01698217]\n",
      " [-0.01355257  0.02794059  0.02794059  0.02794059 -0.01205941 -0.01406585\n",
      "   0.02794059  0.02794059 -0.01654395 -0.01285226  0.02794059 -0.01235355\n",
      "  -0.01353997  0.02794059 -0.01336389  0.02794059 -0.02598508 -0.01197207\n",
      "   0.02794059  0.02794059 -0.01273826  0.02794059  0.02794059 -0.01217936\n",
      "  -0.01714068]\n",
      " [-0.00649675  0.01339399  0.01339399  0.01339399  0.01339399  0.01339399\n",
      "   0.01339399  0.01339399 -0.00793074 -0.00616104  0.01339399 -0.00592197\n",
      "  -0.00649071  0.01339399 -0.0064063   0.01339399 -0.01245657 -0.0057391\n",
      "   0.01339399  0.01339399 -0.00610639  0.01339399  0.01339399 -0.00583847\n",
      "  -0.0082168 ]\n",
      " [-0.01351652  0.02786626  0.02786626  0.02786626  0.02786626 -0.01402843\n",
      "  -0.01213374  0.02786626 -0.01649994 -0.01281806  0.02786626 -0.01232069\n",
      "  -0.01350395  0.02786626 -0.01332834  0.02786626 -0.02591594 -0.01194022\n",
      "   0.02786626  0.02786626 -0.01270437  0.02786626  0.02786626 -0.01214695\n",
      "  -0.01709508]\n",
      " [-0.01044274  0.02152922  0.02152922  0.02152922  0.02152922 -0.01083824\n",
      "   0.02152922 -0.01847078 -0.0127477  -0.00990312  0.02152922 -0.00951885\n",
      "  -0.01043303  0.02152922 -0.01029735  0.02152922 -0.02002243 -0.00922491\n",
      "   0.02152922  0.02152922 -0.00981528  0.02152922  0.02152922 -0.00938463\n",
      "  -0.0132075 ]\n",
      " [-0.00721566  0.01487613  0.01487613  0.01487613  0.01487613 -0.00748894\n",
      "   0.01487613  0.01487613  0.01487613 -0.0068428   0.01487613 -0.00657728\n",
      "  -0.00720895  0.01487613 -0.00711521  0.01487613 -0.01383498 -0.00637417\n",
      "   0.01487613  0.01487613 -0.00678211  0.01487613  0.01487613 -0.00648453\n",
      "  -0.00912604]\n",
      " [-0.00611282  0.01260246  0.01260246  0.01260246  0.01260246 -0.00634433\n",
      "   0.01260246  0.01260246 -0.00746206  0.01260246  0.01260246 -0.00557201\n",
      "  -0.00610713  0.01260246 -0.00602771  0.01260246 -0.01172044 -0.00539994\n",
      "   0.01260246  0.01260246 -0.00574553  0.01260246  0.01260246 -0.00549344\n",
      "  -0.00773122]\n",
      " [-0.01400327  0.02886977  0.02886977  0.02886977  0.02886977 -0.01453362\n",
      "   0.02886977  0.02886977 -0.01709413 -0.01327966 -0.01113023 -0.01276438\n",
      "  -0.01399025  0.02886977 -0.01380831  0.02886977 -0.02684922 -0.01237021\n",
      "   0.02886977  0.02886977 -0.01316188  0.02886977  0.02886977 -0.01258438\n",
      "  -0.0177107 ]\n",
      " [-0.00594834  0.01226337  0.01226337  0.01226337  0.01226337 -0.00617363\n",
      "   0.01226337  0.01226337 -0.00726129 -0.00564097  0.01226337  0.01226337\n",
      "  -0.00594281  0.01226337 -0.00586553  0.01226337 -0.01140508 -0.00525465\n",
      "   0.01226337  0.01226337 -0.00559094  0.01226337  0.01226337 -0.00534563\n",
      "  -0.0075232 ]\n",
      " [-0.00633314  0.01305669  0.01305669  0.01305669  0.01305669 -0.006573\n",
      "   0.01305669  0.01305669 -0.00773102 -0.00600588  0.01305669 -0.00577284\n",
      "   0.01305669  0.01305669 -0.00624497  0.01305669 -0.01214287 -0.00559457\n",
      "   0.01305669  0.01305669 -0.00595261  0.01305669  0.01305669 -0.00569143\n",
      "  -0.00800987]\n",
      " [-0.01339037  0.02760619  0.02760619  0.02760619  0.02760619 -0.01389751\n",
      "   0.02760619  0.02760619 -0.01634595 -0.01269844  0.02760619 -0.0122057\n",
      "  -0.01337792 -0.01239381 -0.01320395  0.02760619 -0.02567408 -0.01182879\n",
      "   0.02760619  0.02760619 -0.01258581  0.02760619  0.02760619 -0.01203359\n",
      "  -0.01693553]\n",
      " [-0.00627743  0.01294183  0.01294183  0.01294183  0.01294183 -0.00651518\n",
      "   0.01294183  0.01294183 -0.00766301 -0.00595305  0.01294183 -0.00572206\n",
      "  -0.00627159  0.01294183  0.01294183  0.01294183 -0.01203605 -0.00554536\n",
      "   0.01294183  0.01294183 -0.00590025  0.01294183  0.01294183 -0.00564137\n",
      "  -0.00793941]\n",
      " [-0.01429716  0.02947566  0.02947566  0.02947566  0.02947566 -0.01483864\n",
      "   0.02947566  0.02947566 -0.01745289 -0.01355837  0.02947566 -0.01303226\n",
      "  -0.01428386  0.02947566 -0.01409811 -0.01052434 -0.02741271 -0.01262982\n",
      "   0.02947566  0.02947566 -0.01343811  0.02947566  0.02947566 -0.0128485\n",
      "  -0.0180824 ]\n",
      " [-0.0093492   0.01927474  0.01927474  0.01927474  0.01927474 -0.00970329\n",
      "   0.01927474  0.01927474 -0.0114128  -0.00886609  0.01927474 -0.00852206\n",
      "  -0.00934051  0.01927474 -0.00921904  0.01927474  0.01927474 -0.0082589\n",
      "   0.01927474  0.01927474 -0.00878745  0.01927474  0.01927474 -0.00840189\n",
      "  -0.01182445]\n",
      " [-0.00581976  0.01199827  0.01199827  0.01199827  0.01199827 -0.00604017\n",
      "   0.01199827  0.01199827 -0.00710432 -0.00551903  0.01199827 -0.00530487\n",
      "  -0.00581434  0.01199827 -0.00573873  0.01199827 -0.01115853  0.01199827\n",
      "   0.01199827  0.01199827 -0.00547007  0.01199827  0.01199827 -0.00523007\n",
      "  -0.00736056]\n",
      " [-0.01190268  0.02453911  0.02453911  0.02453911  0.02453911 -0.01235348\n",
      "   0.02453911  0.02453911 -0.01452989 -0.01128762  0.02453911 -0.01084963\n",
      "  -0.01189161  0.02453911 -0.01173697  0.02453911 -0.02282166 -0.01051459\n",
      "  -0.01546089  0.02453911 -0.01118751  0.02453911  0.02453911 -0.01069664\n",
      "  -0.01505397]\n",
      " [-0.0140796   0.02902712  0.02902712  0.02902712  0.02902712 -0.01461283\n",
      "   0.02902712  0.02902712 -0.0171873  -0.01335205  0.02902712 -0.01283395\n",
      "  -0.0140665   0.02902712 -0.01388358  0.02902712 -0.02699556 -0.01243763\n",
      "   0.02902712 -0.01097288 -0.01323362  0.02902712  0.02902712 -0.01265298\n",
      "  -0.01780723]\n",
      " [-0.00607558  0.01252568  0.01252568  0.01252568  0.01252568 -0.00630568\n",
      "   0.01252568  0.01252568 -0.0074166  -0.00576163  0.01252568 -0.00553806\n",
      "  -0.00606993  0.01252568 -0.00599099  0.01252568 -0.01164903 -0.00536704\n",
      "   0.01252568  0.01252568  0.01252568  0.01252568  0.01252568 -0.00545997\n",
      "  -0.00768411]\n",
      " [-0.01359611  0.02803035  0.02803035  0.02803035  0.02803035 -0.01411104\n",
      "   0.02803035  0.02803035 -0.0165971  -0.01289354  0.02803035 -0.01239324\n",
      "  -0.01358346  0.02803035 -0.01340682  0.02803035 -0.02606855 -0.01201053\n",
      "   0.02803035  0.02803035 -0.01277918 -0.01196965  0.02803035 -0.01221848\n",
      "  -0.01719574]\n",
      " [-0.01336365  0.0275511   0.0275511   0.0275511   0.0275511  -0.01386977\n",
      "   0.0275511   0.0275511  -0.01631333 -0.0126731   0.0275511  -0.01218135\n",
      "  -0.01335122  0.0275511  -0.0131776   0.0275511  -0.02562285 -0.01180518\n",
      "   0.0275511   0.0275511  -0.01256069  0.0275511  -0.0124489  -0.01200958\n",
      "  -0.01690174]\n",
      " [-0.00588993  0.01214294  0.01214294  0.01214294  0.01214294 -0.006113\n",
      "   0.01214294  0.01214294 -0.00718998 -0.00558557  0.01214294 -0.00536884\n",
      "  -0.00588445  0.01214294 -0.00580793  0.01214294 -0.01129308 -0.00520305\n",
      "   0.01214294  0.01214294 -0.00553603  0.01214294  0.01214294  0.01214294\n",
      "  -0.00744931]\n",
      " [-0.00737697  0.01520869  0.01520869  0.01520869  0.01520869 -0.00765636\n",
      "   0.01520869  0.01520869 -0.00900524 -0.00699577  0.01520869 -0.00672432\n",
      "  -0.00737011  0.01520869 -0.00727427  0.01520869 -0.01414426 -0.00651667\n",
      "   0.01520869  0.01520869 -0.00693372  0.01520869  0.01520869 -0.0066295\n",
      "   0.01520869]]\n",
      "--------------------------------------------------\n",
      "DDDX [[ 7.79263372e-50  0.00000000e+00 -2.92655735e-50 ... -1.17456984e-49\n",
      "  -6.48733854e-50  8.14446712e-51]\n",
      " [-1.22341290e-42 -0.00000000e+00 -1.71140026e-42 ... -2.07028324e-42\n",
      "  -1.82003234e-42 -1.23151806e-43]\n",
      " [ 1.22253434e-42 -0.00000000e+00  4.94861768e-43 ... -2.68454798e-42\n",
      "   9.18009785e-42 -1.36272567e-43]\n",
      " ...\n",
      " [-1.45767486e-42 -0.00000000e+00 -1.77785825e-42 ... -1.84692981e-42\n",
      "   2.51001623e-42 -1.27934101e-43]\n",
      " [ 5.90713750e-51  0.00000000e+00  2.75317204e-51 ... -2.41533887e-50\n",
      "   6.37868319e-51  6.27080038e-52]\n",
      " [-9.96943945e-47  0.00000000e+00 -4.89179273e-47 ...  2.70090089e-48\n",
      "   1.13084983e-47  2.28566144e-48]]\n",
      "1.0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true grads</th>\n",
       "      <th>approx grads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.248931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.287120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.562034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2720 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true grads  approx grads\n",
       "0            0.0     -0.248931\n",
       "1            0.0      0.034409\n",
       "2            0.0     -0.028725\n",
       "3            0.0     -0.287120\n",
       "4            0.0     -0.562034\n",
       "...          ...           ...\n",
       "2715         0.0      0.001596\n",
       "2716         0.0     -0.000065\n",
       "2717         0.0      0.000499\n",
       "2718         0.0      0.000904\n",
       "2719         0.0      0.000583\n",
       "\n",
       "[2720 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 25\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(16, 13, (None, 28, 28, 1)),\n",
    "    ReLU((None, 16, 16, 16)),\n",
    "    Maxpool((None, 16, 16, 16)),\n",
    "    #Conv2D(32, 9, (None, 11, 11, 16)),\n",
    "    #ReLU((None, 8, 8, 32)),\n",
    "    #Maxpool((None, 3, 3, 32)),\n",
    "    Flatten((None, 8, 8, 16)),\n",
    "    #Dense(32, (5184, None), \"relu\"),\n",
    "    #Dense(1, (1024, None), \"sigmoid\")\n",
    "    combo_method((1024, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train modela\n",
    "model = Model(layers)\n",
    "print(model.gradcheck(X,y))\n",
    "pd.DataFrame(np.column_stack((model.true_grads, model.approx_grads)), columns=['true grads', 'approx grads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_test[:m].values.reshape(1,m).astype(float)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cost epoch 0: 0.6931474473201465\n",
    "Cost epoch 1: 0.6931451399684077\n",
    "Cost epoch 2: 0.6931428337870434\n",
    "Cost epoch 3: 0.6931405288546488\n",
    "Cost epoch 4: 0.6931382251409215\n",
    "Cost epoch 5: 0.6931359225810582\n",
    "Cost epoch 6: 0.6931336211552712\n",
    "Cost epoch 7: 0.6931313208861655\n",
    "Cost epoch 8: 0.693129021802918\n",
    "Cost epoch 9: 0.6931267238456827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
