{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        0\n",
      "3        1\n",
      "6        1\n",
      "8        1\n",
      "14       1\n",
      "        ..\n",
      "59972    0\n",
      "59979    1\n",
      "59984    1\n",
      "59987    0\n",
      "59994    1\n",
      "Name: training targets, Length: 12665, dtype: uint8\n",
      "(12665, 28, 28) (2115, 28, 28)\n",
      "(12665,) (2115,)\n",
      "\n",
      "(5923,) (6742,)\n",
      "(980,) (1135,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "#MNIST = k.datasets.fashion_mnist.load_data()\n",
    "MNIST = k.datasets.mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 0) | (y_train == 1)]\n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 0) | (y_test == 1)]\n",
    "X_test = X_test[y_test.index]\n",
    "\n",
    "# y_train[y_train==5] = 1\n",
    "# y_test[y_test==5] = 1\n",
    "# X_train[X_train==5] = 1\n",
    "# X_test[X_test==5] = 1\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print()\n",
    "print(y_train[y_train == 0].shape, y_train[y_train == 1].shape)\n",
    "print(y_test[y_test == 0].shape, y_test[y_test == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 65,921\n",
      "Trainable params: 65,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = \"random_normal\" # random_normal or glorot_uniform\n",
    "keras_model = k.Sequential([ \n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.MaxPooling2D((3,3)),\n",
    "    #k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.1952\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0755\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0583\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0508\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0465\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0434\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0411\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0391\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0371\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0360\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 12000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 2000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = tf.raw_ops.Sigmoid(x=Z).numpy()\n",
    "    cache = A\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = tf.raw_ops.Relu(features=Z).numpy()\n",
    "    \n",
    "    cache = A\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = tf.raw_ops.ReluGrad(gradients=dA, features=Z).numpy()\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    A = cache\n",
    "    dZ = tf.raw_ops.SigmoidGrad(y=A, dy=dA).numpy()\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple, trainable=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + \" \" + str(self.output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (neurons, input_shape[1])\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.05\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "        self.dZ = dZ \n",
    "        self.dW = np.dot(dZ, A_prev.T)/m\n",
    "        self.db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.05\n",
    "        self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform convolution\n",
    "        Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "        # Add bias\n",
    "        Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z.numpy()\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "        self.dZ = dZ\n",
    "        self.A_prev = A_prev\n",
    "        dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()/m\n",
    "        self.db = np.average(np.sum(dZ, axis=(1,2)), axis=0)\n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "        \n",
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.ksize = [1, pool_size, pool_size, 1]\n",
    "        self.strides = [1, pool_size, pool_size, 1]\n",
    "        output_shape = (input_shape[0], input_shape[1]//pool_size, input_shape[2]//pool_size, input_shape[3])\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        Z = tf.raw_ops.MaxPool(input=A_prev, ksize=self.ksize, strides=self.strides, data_format='NHWC', padding=\"VALID\").numpy()\n",
    "        self.cache = (A_prev, Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev, Z = self.cache\n",
    "        dA_prev = tf.raw_ops.MaxPoolGrad(orig_input=A_prev, orig_output=Z, grad=dZ, ksize=self.ksize, strides=self.strides, padding=\"VALID\", data_format='NHWC').numpy()\n",
    "        return dA_prev\n",
    "\n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "      \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = tf.raw_ops.Relu(features=Z).numpy()\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = tf.raw_ops.ReluGrad(gradients=dA, features=Z).numpy()\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (np.prod(shape), m)\n",
    "        super().__init__(input_shape, output_shape, trainable=False)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.flatten().reshape(m,np.prod(shape)).T\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.T.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, batch_size, verbose): \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X, Y.T))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size, drop_remainder=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "#                 print(\"\\nBATCH SIZE\", y_batch.numpy().T.shape,end=\"\\n\")\n",
    "                y_batch = y_batch.numpy().T\n",
    "                # FORWARD PROP\n",
    "                Z = x_batch\n",
    "                for layer in self.layers:    \n",
    "                    if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                        Z = layer.forward(Z, y_batch)\n",
    "                    else:\n",
    "                        Z = layer.forward(Z)\n",
    "                    #print(layer, Z.shape)\n",
    "\n",
    "                # COST FUNCTION\n",
    "                cost = compute_cost(Z, y_batch)\n",
    "                history.append(cost)\n",
    "                if verbose == 1:\n",
    "                    print(\"Step {:.0f} in epoch {:.0f} - Cost: {:.8f}\\r\".format(step, epoch, cost), end=\"\")\n",
    "\n",
    "                # BACKWARD PROP\n",
    "                dA = - (np.divide(y_batch, Z) - np.divide(1 - y_batch, 1 - Z)) # derivative of cost with respect to Z\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    dA = layer.backward(dA)\n",
    "\n",
    "                # UPDATE PARAMS\n",
    "                for layer in self.layers:\n",
    "                    layer.update_params(learning_rate)\n",
    "            print(\"\\n\\n\", \"=\"*75, \"\\n\")\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                Z = layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = layer.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"-\"*25)\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    def _cost(self, X, Y):\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            Z = prop_layer.forward(Z)\n",
    "        # COMPUTE COST\n",
    "        return compute_cost(Z, Y)\n",
    "    \n",
    "    def gradcheck(self, X, Y, epsilon=1e-7, start=None, end=None):\n",
    "        self.approx_grads = []\n",
    "        self.true_grads = []\n",
    "        for layer in self.layers[start:end]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            for i in range(layer.W.size):\n",
    "                i = np.unravel_index(i, layer.W.shape)\n",
    "                Wi = layer.W[i]\n",
    "                layer.W[i] = Wi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "                \n",
    "            for i in range(layer.b.size):\n",
    "                i = np.unravel_index(i, layer.b.shape)\n",
    "                bi = layer.b[i]\n",
    "                layer.b[i] = bi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.b[i] = bi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.b[i] = bi\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "        \n",
    "        # FORWARD PROP\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            Z = prop_layer.forward(Z)\n",
    "        # BACKWARD PROP\n",
    "        dA = - (np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        \n",
    "        for layer in self.layers[start:end]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            self.true_grads = np.concatenate((self.true_grads, layer.dW.flatten(), layer.db.flatten()))\n",
    "        self.approx_grads = np.array(self.approx_grads)\n",
    "        self.true_grads = np.array(self.true_grads)\n",
    "        return np.sqrt(np.sum(np.square(self.true_grads-self.approx_grads)))/(np.sqrt(np.sum(np.square(self.true_grads)))+np.sqrt(np.sum(np.square(self.approx_grads))))\n",
    "    \n",
    "        \n",
    "    \n",
    "class knn_differentiable(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__(input_shape, num_classes, False)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_features, batch_labels):\n",
    "        self.batch_features = np.transpose(batch_features).astype('float')\n",
    "#         print(\"BATCH_FEATURES\\n\", self.batch_features)\n",
    "#         print()\n",
    "        self.batch_labels = batch_labels.astype('float')\n",
    "        \n",
    "        self.distances = self.calc_distance_mtx(self.batch_features, self.batch_features) * (1/100000)\n",
    "        #self.distances = np.divide(1, self.distances, where=self.distances!=0)\n",
    "#         print(\"DISTANCES\\n\",self.distances,self.distances.shape)\n",
    "#         print()\n",
    "        \n",
    "        self.class_0 = np.array(self.batch_labels[:] == 0).astype('float')\n",
    "        self.class_1 = np.array(self.batch_labels[:] == 1).astype('float')\n",
    "        self.num_0 = np.count_nonzero(self.batch_labels == 0)\n",
    "        self.num_1 = np.count_nonzero(self.batch_labels == 1)\n",
    "#         print(\"SIZES\", self.num_0, self.num_1)\n",
    "#         print()\n",
    "\n",
    "        self.aggregate = np.stack([np.sum(np.multiply(self.distances, self.class_0), 1) / self.num_0, np.sum(np.multiply(self.distances, self.class_1), 1) / self.num_1], axis=1)\n",
    "#         print(\"AGGREGATE\\n\",self.aggregate,self.aggregate.shape)\n",
    "#         print()\n",
    "        \n",
    "        exp = np.exp(-self.aggregate)\n",
    "#         print(\"EXP OF AGGREGATE\\n\",exp)\n",
    "#         print(exp[:,0].shape)\n",
    "#         print()\n",
    "        \n",
    "        self.softmax = np.divide(exp[:,1], np.sum(exp, 1))\n",
    "#         print(\"SOFTMAX\\n\",self.softmax[:50])\n",
    "#         print(\"=\"*20)\n",
    "#         print()\n",
    "        \n",
    "        return np.reshape(self.softmax, (1,self.distances.shape[0]))\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        # Overall what we need here:\n",
    "        # d(TL)/d(X = features) = d(TL)/D(L) * d(L)/d(S) * \n",
    "        #     [(d(S)/d(A_0) * d(A_0)/d(D_0) * d(D_0)/d(X)) + (d(S)/d(A_1) * d(A_1)/d(D_1) * d(D_1)/d(X))]\n",
    "        #\n",
    "        # Shapes:\n",
    "        # d(TL)/d(L)  -  (N x 1)           N = number of samples (images)\n",
    "        # d(L)/d(S)   -  (N x 1)\n",
    "        # d(S)/d(Ai)  -  (N x 1)\n",
    "        # d(Ai)/d(Di) -  (N x N)\n",
    "        # d(Di)/d(X)  -  (N x f)           f = number of features (output by cnn part)\n",
    "        # final output:  (N x f)\n",
    "        \n",
    "        \n",
    "        # d(TL)/d(L) = -1/m * sum(d(L)/d(S))  <-- Maybe don't need this at all, done before calling backward\n",
    "        \n",
    "        \n",
    "        # d(L)/d(S) = (Y/S) - (1-Y)/(1-S)    <-- Y = true labels, S = softmax labels from column 0 (softmax on class 0)\n",
    "        dL_dS = dA.T   # <-- Really dTL_dS\n",
    "        \n",
    "#         print(\"dL_dS\", dL_dS.shape, dL_dS[:25])\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(S)/d(A) = - (e^-A0 * e^-A1) / (e^-2A0 + e^-A0-A1 + e^-2A1)\n",
    "        # This calc is for A_0 and A_1, A_1 is positive version of this\n",
    "        #np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))\n",
    "        dS_dA0 = -np.divide(np.multiply(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])), \n",
    "                            np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))).reshape(self.distances.shape[0],1)\n",
    "        \n",
    "        # np.add(np.add(np.exp(-2*self.aggregate[:,0]), np.exp(-2*self.aggregate[:,1])), \n",
    "        #                           np.exp(np.multiply(self.aggregate[:,0], self.aggregate[:,1])))\n",
    "        \n",
    "        \n",
    "        dS_dA1 = -dS_dA0\n",
    "#         print(\"dS_dA0\", dS_dA0.shape, dS_dA0[:25]) (1000,) (1000,1)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(A)/d(D) = 1/len(class) * sum(d(D)/d(X))   <-- Same calc for A_0 and A_1, len(class) = how many of this class there are either num_0 or num_1\n",
    "        # possibly don't need this part either since its calculated within loops???\n",
    "        \n",
    "        \n",
    "        # d(D)/d(X) = 2(x_i - x_j)   <-- x_i and x_j are the two feature vectors used in the distance\n",
    "        #distances_0 = \n",
    "        dA0_dX = np.ones(self.batch_features.shape)\n",
    "        # iterate over rows in distance matrix\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD_dX = np.zeros(self.batch_features.shape[1])\n",
    "            # iterate over the columns in distance matrix\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                # only calculate derivatives for indices of class 0\n",
    "                if self.class_0[:,j] == 0:\n",
    "                    # first calc derivative of distance formula, then them all together\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD_dX = np.add(dD_dX, deriv)\n",
    "            # replace the corresponding derivatives for each row (sample)\n",
    "            dA0_dX[i,:] = dD_dX\n",
    "        \n",
    "        # Same as loop above but on indices of class 1\n",
    "        dA1_dX = np.ones(self.batch_features.shape)\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD_dX = np.zeros(self.batch_features.shape[1])\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                if self.class_0[:,j] == 1:\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD_dX = np.add(dD_dX, deriv)\n",
    "            dA1_dX[i,:] = dD_dX\n",
    "        \n",
    "\n",
    "        # divide by the number of samples in each class since these were averaged in forward\n",
    "        dA0_dX = dA0_dX * (1/self.num_0) * (1/100000)\n",
    "        dA1_dX = dA1_dX * (1/self.num_1) * (1/100000)\n",
    "#         print(\"dD0_dX\", dD0_dX.shape, dD0_dX)\n",
    "#         print()\n",
    "#         print(\"dD1_dX\", dD0_dX.shape, dD1_dX)\n",
    "#         print()\n",
    "\n",
    "        # add together the terms in brackets from the full formula\n",
    "        dS_dX = np.add(np.multiply(dS_dA0, dA0_dX), np.multiply(dS_dA1, dA1_dX))\n",
    "        # final step of the chain rule\n",
    "        dL_dX = np.multiply(dL_dS, dS_dX)\n",
    "#         print(\"dL_dX\", dL_dX.shape, dL_dX)\n",
    "        \n",
    "#         print('='*50)\n",
    "        return dL_dX\n",
    "        #return np.zeros(self.batch_features.shape)\n",
    "    \n",
    "    def calc_distance_mtx(self, A, B):\n",
    "        \"\"\"\n",
    "        Computes squared pairwise distances between each elements of A and each elements of B.\n",
    "        Args:\n",
    "        A,    [m,d] matrix\n",
    "        B,    [n,d] matrix\n",
    "        Returns:\n",
    "        D,    [m,n] matrix of pairwise distances\n",
    "        \"\"\"\n",
    "        with tf.compat.v1.variable_scope('pairwise_dist'):\n",
    "            # squared norms of each row in A and B\n",
    "            na = tf.reduce_sum(tf.square(A), 1)\n",
    "            nb = tf.reduce_sum(tf.square(B), 1)\n",
    "\n",
    "            # na as a row and nb as a co\"lumn vectors\n",
    "            na = tf.reshape(na, [-1, 1])\n",
    "            nb = tf.reshape(nb, [1, -1])\n",
    "\n",
    "            # return pairwise euclidead difference matrix\n",
    "            D = tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0)\n",
    "        return D.numpy()\n",
    "    \n",
    "    def calc_cosine_sim(self, A, B):\n",
    "        from scipy.spatial.distance import pdist\n",
    "        from scipy.spatial.distance import squareform\n",
    "        cos_sim = squareform(pdist(A, metric='cosine'))\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Code below for knn layer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 in epoch 0 - Cost: 0.06487071\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.06487073\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.06487074\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.06487075\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.06487076\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.06487077\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.06487079\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.06487081\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.06487082\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.06487083\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.791963577270508\n",
      "\n",
      "[0.9853427895981087]\n",
      "Accuracy 1 - 0.9853427895981087\n",
      "\n",
      "[0.9848628192999054, 0.9877010406811731]\n",
      "Accuracy 2 - 0.9862819299905392\n",
      "\n",
      "[0.9853515625, 0.9892578125]\n",
      "Accuracy 3 - 0.9873046875\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9962085308056873\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 1\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.08272573\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.08272574\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.08272576\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.08272577\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.08272578\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.08272579\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.08272581\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.08272583\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.08272584\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.08272585\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.780238628387451\n",
      "\n",
      "[0.9839243498817967]\n",
      "Accuracy 1 - 0.9839243498817967\n",
      "\n",
      "[0.9877010406811731, 0.9877010406811731]\n",
      "Accuracy 2 - 0.9877010406811731\n",
      "\n",
      "[0.9873046875, 0.9873046875]\n",
      "Accuracy 3 - 0.9873046875\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9957345971563982\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 2\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.12267136\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.12267139\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.12267142\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.12267144\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.12267146\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.12267149\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.12267152\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.12267156\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.12267159\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.12267162\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.800419330596924\n",
      "\n",
      "[0.9853427895981087]\n",
      "Accuracy 1 - 0.9853427895981087\n",
      "\n",
      "[0.9848628192999054, 0.9877010406811731]\n",
      "Accuracy 2 - 0.9862819299905392\n",
      "\n",
      "[0.986328125, 0.9892578125]\n",
      "Accuracy 3 - 0.98779296875\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9962085308056873\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 3\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.02861484\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.02861484\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.02861485\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.02861485\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.02861485\n",
      "\n",
      " =========================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 in epoch 5 - Cost: 0.02861486\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.02861486\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.02861486\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.02861487\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.02861487\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.784221649169922\n",
      "\n",
      "[0.9900709219858156]\n",
      "Accuracy 1 - 0.9900709219858156\n",
      "\n",
      "[0.988647114474929, 0.9914853358561968]\n",
      "Accuracy 2 - 0.990066225165563\n",
      "\n",
      "[0.98828125, 0.9931640625]\n",
      "Accuracy 3 - 0.99072265625\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9966824644549763\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 4\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.04314414\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.04314415\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.04314415\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.04314416\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.04314416\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.04314417\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.04314417\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.04314418\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.04314419\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.04314419\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.7497966289520264\n",
      "\n",
      "[0.9872340425531915]\n",
      "Accuracy 1 - 0.9872340425531915\n",
      "\n",
      "[0.9867549668874173, 0.988647114474929]\n",
      "Accuracy 2 - 0.9877010406811731\n",
      "\n",
      "[0.986328125, 0.9892578125]\n",
      "Accuracy 3 - 0.98779296875\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9966824644549763\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 5\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.09283837\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.09283840\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.09283842\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.09283844\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.09283847\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.09283848\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.09283850\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.09283852\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.09283854\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.09283856\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.7550485134124756\n",
      "\n",
      "[0.9839243498817967]\n",
      "Accuracy 1 - 0.9839243498817967\n",
      "\n",
      "[0.9848628192999054, 0.9867549668874173]\n",
      "Accuracy 2 - 0.9858088930936613\n",
      "\n",
      "[0.9853515625, 0.98828125]\n",
      "Accuracy 3 - 0.98681640625\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9966824644549763\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 6\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.09560933\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.09560935\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.09560936\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.09560939\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.09560941\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.09560943\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.09560945\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.09560947\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.09560950\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.09560951\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.7511534690856934\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9810874704491725]\n",
      "Accuracy 1 - 0.9810874704491725\n",
      "\n",
      "[0.9829706717123936, 0.9848628192999054]\n",
      "Accuracy 2 - 0.9839167455061495\n",
      "\n",
      "[0.9833984375, 0.9853515625]\n",
      "Accuracy 3 - 0.984375\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.995260663507109\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 7\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.07188285\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.07188286\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.07188288\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.07188289\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.07188291\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.07188292\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.07188293\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.07188295\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.07188296\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.07188297\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.788607597351074\n",
      "\n",
      "[0.983451536643026]\n",
      "Accuracy 1 - 0.983451536643026\n",
      "\n",
      "[0.9839167455061495, 0.9877010406811731]\n",
      "Accuracy 2 - 0.9858088930936613\n",
      "\n",
      "[0.9833984375, 0.9892578125]\n",
      "Accuracy 3 - 0.986328125\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9962085308056873\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 8\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.05542698\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.05542699\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.05542700\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.05542701\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.05542702\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.05542703\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.05542703\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.05542704\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.05542705\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.05542706\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.7460873126983643\n",
      "\n",
      "[0.9839243498817967]\n",
      "Accuracy 1 - 0.9839243498817967\n",
      "\n",
      "[0.9858088930936613, 0.9877010406811731]\n",
      "Accuracy 2 - 0.9867549668874172\n",
      "\n",
      "[0.9853515625, 0.9873046875]\n",
      "Accuracy 3 - 0.986328125\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.995260663507109\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 9\n",
      "\n",
      "Step 0 in epoch 0 - Cost: 0.03667135\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.03667136\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.03667136\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.03667136\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.03667137\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.03667137\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.03667137\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.03667138\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.03667138\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.03667138\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 2.7653377056121826\n",
      "\n",
      "[0.9858156028368794]\n",
      "Accuracy 1 - 0.9858156028368794\n",
      "\n",
      "[0.9867549668874173, 0.988647114474929]\n",
      "Accuracy 2 - 0.9877010406811731\n",
      "\n",
      "[0.9873046875, 0.9892578125]\n",
      "Accuracy 3 - 0.98828125\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
      "Accuracy 4 - 0.9962085308056873\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average Execution Time: 2.771287441253662   ---   Average Accuracies: [0.98501182 0.98680227 0.98730469 0.99611374]\n"
     ]
    }
   ],
   "source": [
    "# Change below three parameters for experiments \n",
    "train_size = 12665\n",
    "batch_size = 512\n",
    "learning = 0.0000001\n",
    "\n",
    "\n",
    "\n",
    "times = []\n",
    "accuracy_lst = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    # Select only m samples\n",
    "    m = train_size\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "    y = y_train[:m].values.reshape(1,m)\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "        Flatten((None, 8, 8, 32)),\n",
    "        knn_differentiable((2048, None), 2)\n",
    "    ]\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=learning, verbose=1, batch_size=batch_size)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Runs through each testing batch size for the table and gathers accuracies\n",
    "    batch_sizes = [2115, 1057, batch_size, 10]\n",
    "    for j in range(len(batch_sizes)):\n",
    "        m = batch_sizes[j]\n",
    "        n = 0\n",
    "        accuracies = []\n",
    "        for l in range(y_test.values.shape[0]//m):\n",
    "            X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "            y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "            predictions = model.predict(X, y)\n",
    "            #print(predictions[:,:10])\n",
    "            score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "            accuracies.append(score)\n",
    "\n",
    "            n = m\n",
    "            m += batch_sizes[j]\n",
    "\n",
    "        print(accuracies)\n",
    "        avg_acc = np.average(accuracies)\n",
    "        accuracy_lst[j] += avg_acc\n",
    "        print(\"Accuracy\", j+1, \"-\", avg_acc)\n",
    "        print()\n",
    "    print(\"+\"*50, \"end of loop\", i+1)\n",
    "    print()\n",
    "\n",
    "# Output average execution time and average accuracies in a list of the same order as shown in tables\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracies:\", np.divide(accuracy_lst, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment code for fully connected layers (MNIST datasets done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 in epoch 0 - Cost: 7.54589347\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 3.06980805\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 1.68318198\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.97364412\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 1.33794635\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 1.43123341\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.58608754\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 1.07697159\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.84939766\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.57045606\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.5428719520568848\n",
      "\n",
      "[[0.99511791 0.92058401 0.97629122 0.9386256  0.01631713 0.99007027\n",
      "  0.01573945 0.11298821 0.95109685 0.96370594]]\n",
      "[0.6985]\n",
      "Average accuracy: 0.6985\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 1\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 6.941841538\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 2.58244754\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.70241166\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 1.02453776\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.43092630\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.51488321\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.72251659\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.50647978\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.77831006\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.24712487\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.4911537170410156\n",
      "\n",
      "[[9.79620267e-01 9.99942648e-01 9.75098239e-01 9.99177956e-01\n",
      "  1.73340497e-04 9.97205772e-01 1.39710897e-04 1.41148449e-02\n",
      "  9.79450507e-01 9.72156158e-01]]\n",
      "[0.8895]\n",
      "Average accuracy: 0.8895\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 2\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 5.26657721\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 2.92961380\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.92788166\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.43861322\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.41941699\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.24006083\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.38175248\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.25005212\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.21038483\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.19959141\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.4993829727172852\n",
      "\n",
      "[[9.99232921e-01 8.17603202e-01 9.98424745e-01 7.93881019e-01\n",
      "  9.65360607e-04 9.48994977e-01 5.74251009e-01 4.36880589e-01\n",
      "  6.74291802e-01 3.57379761e-01]]\n",
      "[0.8565]\n",
      "Average accuracy: 0.8565\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 3\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 3.42329195\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 1.93080202\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.73286383\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.56940341\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.45063515\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.38103341\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.23662005\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.41984653\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.24155971\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.32084654\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.5772192478179932\n",
      "\n",
      "[[0.98969243 0.9473651  0.0229967  0.68835058 0.02608915 0.99965367\n",
      "  0.93353508 0.00692034 0.63580165 0.84661688]]\n",
      "[0.8065]\n",
      "Average accuracy: 0.8065\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 4\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 1.38115597\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 0.42939194\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.38850941\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.36124896\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.33743809\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.68441792\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.54562608\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.14361397\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.16721503\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.32717647\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.4811511039733887\n",
      "\n",
      "[[9.99773739e-01 9.02863323e-01 1.98782480e-01 3.62714798e-01\n",
      "  4.07041860e-04 9.98214968e-01 1.20090851e-02 8.49562526e-02\n",
      "  9.98563756e-01 9.99119043e-01]]\n",
      "[0.8795]\n",
      "Average accuracy: 0.8795\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 5\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 2.43818266\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 2.40213649\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 2.25053591\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 1.27874469\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 1.06636165\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 1.43456480\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 1.18051081\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.96733906\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.85704510\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.72956809\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.4944818019866943\n",
      "\n",
      "[[6.53238570e-02 9.53112969e-01 8.09780479e-03 8.20338186e-01\n",
      "  1.66342587e-04 9.42065276e-01 9.83923796e-01 4.65793183e-02\n",
      "  8.14722464e-01 9.32741912e-01]]\n",
      "[0.7825]\n",
      "Average accuracy: 0.7825\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 6\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 6.14792364\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 5.20365636\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 3.12189962\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 2.44499759\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 1.48653640\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.74468011\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.76559404\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.91388922\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.81333696\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.58081863\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.493518590927124\n",
      "\n",
      "[[0.99998721 0.02132199 0.97887034 0.79554984 0.00983417 0.34419039\n",
      "  0.00130098 0.99157122 0.99998953 0.94681862]]\n",
      "[0.7225]\n",
      "Average accuracy: 0.7225\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 7\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 4.82053701\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 4.07552369\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 3.01484491\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 3.27057700\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 2.52424953\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 1.93870411\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 1.41342011\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 1.41255354\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.88470796\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.74830404\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.502251148223877\n",
      "\n",
      "[[9.50532396e-01 9.01051367e-01 6.60832432e-01 8.97628538e-01\n",
      "  5.33923434e-02 8.25215017e-01 1.67195780e-04 3.39871997e-01\n",
      "  9.90041273e-01 9.74808251e-01]]\n",
      "[0.612]\n",
      "Average accuracy: 0.612\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 8\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 1.68519110\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 1.31378502\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.86015896\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 1.35298351\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.93618681\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.81231023\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.71627564\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 1.11195718\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.55903874\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.62095915\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.4669890403747559\n",
      "\n",
      "[[0.99790828 0.27685605 0.97232344 0.92566484 0.00347762 0.76412054\n",
      "  0.84895245 0.05779508 0.99947568 0.99713254]]\n",
      "[0.758]\n",
      "Average accuracy: 0.758\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 9\n",
      "\n",
      "Step 4 in epoch 0 - Cost: 2.41139030\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 1.33229580\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.82975138\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.18430630\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.30001318\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.23279616\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.48223608\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.33434655\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.44149731\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.31310141\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.496992826461792\n",
      "\n",
      "[[0.9998258  0.93629166 0.92927827 0.8065525  0.54733198 0.99983981\n",
      "  0.06987882 0.01586761 0.0864235  0.99862567]]\n",
      "[0.832]\n",
      "Average accuracy: 0.832\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average Execution Time: 1.504601240158081   ---   Average Accuracy: 0.7837500000000001\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "accuracy_lst = []\n",
    "for k in range(10):\n",
    "    # Select only m samples for fast training time during debugging\n",
    "    #np.random.seed(10)\n",
    "    m = 160\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "    y = y_train[:m].values.reshape(1,m)\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 13, 13, 32)),\n",
    "        Maxpool((None, 11, 11, 64), pool_size=2),\n",
    "        Flatten((None, 5, 5, 64)),\n",
    "        Dense(128, (1600, None), \"relu\"),\n",
    "        Dense(1, (128, None), \"sigmoid\")\n",
    "    ]\n",
    "\n",
    "    #     k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    #     k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    #     k.layers.MaxPooling2D((3,3)),\n",
    "    #     k.layers.Flatten(),\n",
    "    #     k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    #     k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=0.00001, verbose=1, batch_size=32)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 2000\n",
    "    n = 0\n",
    "    m = batch_size\n",
    "    accuracies = []\n",
    "    #print(y_test.values.shape[0]//m)\n",
    "    for i in range(y_test.values.shape[0]//m):\n",
    "        X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "        y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "        predictions = model.predict(X, y)\n",
    "        #print(predictions.type)\n",
    "        #print(y[:,:100])\n",
    "        print(predictions[:,:10])\n",
    "        score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "        #print(score)\n",
    "        accuracies.append(score)\n",
    "\n",
    "        n = m\n",
    "        m += batch_size\n",
    "        #print(m,n)\n",
    "\n",
    "    print(accuracies)\n",
    "    avg_acc = np.average(accuracies)\n",
    "    accuracy_lst.append(avg_acc)\n",
    "    print(\"Average accuracy:\", avg_acc)\n",
    "    print()\n",
    "    print(\"+\"*50, \"end of loop\", k+1)\n",
    "    print()\n",
    "    \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracy:\", np.average(accuracy_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 377 in epoch 0 - Cost: 0.01342024\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 110 in epoch 1 - Cost: 0.00157994\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19240/2275813350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mex_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19240/2197806562.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, epochs, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[0;32m    276\u001b[0m                         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m                         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m                     \u001b[1;31m#print(layer, Z.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19240/2197806562.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, A_prev)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NHWC'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"VALID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[1;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[1;32m--> 404\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[1;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[0;32m   5234\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5235\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5236\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   5237\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MaxPool\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5238\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ksize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = []\n",
    "accuracy_lst = []\n",
    "for k in range(10):\n",
    "    # Select only m samples for fast training time during debugging\n",
    "    #np.random.seed(10)\n",
    "    m = 12115\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "    y = y_train[:m].values.reshape(1,m)\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "        #Conv2D(32, 3, (None, 8, 8, 32)),\n",
    "        Flatten((None, 8, 8, 32)),\n",
    "        Dense(32, (2048, None), \"relu\"),\n",
    "        Dense(1, (32, None), \"sigmoid\")\n",
    "    ]\n",
    "\n",
    "    #     k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    #     k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    #     k.layers.MaxPooling2D((3,3)),\n",
    "    #     k.layers.Flatten(),\n",
    "    #     k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    #     k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=0.0002, verbose=1, batch_size=32)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 2115\n",
    "    n = 0\n",
    "    m = batch_size\n",
    "    accuracies = []\n",
    "    #print(y_test.values.shape[0]//m)\n",
    "    for i in range(y_test.values.shape[0]//m):\n",
    "        X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "        y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "        predictions = model.predict(X, y)\n",
    "        #print(predictions.type)\n",
    "        #print(y[:,:100])\n",
    "        print(predictions[:,:100])\n",
    "        score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "        #print(score)\n",
    "        accuracies.append(score)\n",
    "\n",
    "        n = m\n",
    "        m += batch_size\n",
    "        #print(m,n)\n",
    "\n",
    "    print(accuracies)\n",
    "    avg_acc = np.average(accuracies)\n",
    "    accuracy_lst.append(avg_acc)\n",
    "    print(\"Average accuracy:\", avg_acc)\n",
    "    print()\n",
    "    print(\"+\"*50, \"end of loop\", k+1)\n",
    "    print()\n",
    "    \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracy:\", np.average(accuracy_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
