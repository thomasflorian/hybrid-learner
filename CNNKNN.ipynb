{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        0\n",
      "2        0\n",
      "4        0\n",
      "10       0\n",
      "16       1\n",
      "        ..\n",
      "59985    0\n",
      "59989    1\n",
      "59991    1\n",
      "59996    1\n",
      "59998    0\n",
      "Name: training targets, Length: 12000, dtype: uint8\n",
      "(12000, 28, 28) (2000, 28, 28)\n",
      "(12000,) (2000,)\n",
      "\n",
      "(6000,) (6000,)\n",
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "MNIST = k.datasets.fashion_mnist.load_data()\n",
    "#MNIST = k.datasets.mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 0) | (y_train == 1)]\n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 0) | (y_test == 1)]\n",
    "X_test = X_test[y_test.index]\n",
    "\n",
    "# y_train[y_train==5] = 1\n",
    "# y_test[y_test==5] = 1\n",
    "# X_train[X_train==5] = 1\n",
    "# X_test[X_test==5] = 1\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print()\n",
    "print(y_train[y_train == 0].shape, y_train[y_train == 1].shape)\n",
    "print(y_test[y_test == 0].shape, y_test[y_test == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 65,921\n",
      "Trainable params: 65,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = \"random_normal\" # random_normal or glorot_uniform\n",
    "keras_model = k.Sequential([ \n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.MaxPooling2D((3,3)),\n",
    "    #k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.1952\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0755\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0583\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0508\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0465\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0434\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0411\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0391\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0371\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0360\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 12000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 2000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    #print(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple, trainable=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + \" \" + str(self.output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (neurons, input_shape[1])\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.01\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "#         print(\"BEFORE ACTIVATION\", Z)\n",
    "#         print(\"=\"*50)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "#         print(\"AFTER ACTIVATION\", A)\n",
    "#         print(\"=\"*50)\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "            \n",
    "        self.dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "        self.db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.001\n",
    "        self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform convolution\n",
    "        Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "        # Add bias\n",
    "        Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z.numpy()\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "        \n",
    "        dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.db = tf.raw_ops.BiasAddGrad(out_backprop=dZ).numpy()\n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.ksize = [1, pool_size, pool_size, 1]\n",
    "        self.strides = [1, pool_size, pool_size, 1]\n",
    "        output_shape = (input_shape[0], input_shape[1]//pool_size, input_shape[2]//pool_size, input_shape[3])\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        Z = tf.raw_ops.MaxPool(input=A_prev, ksize=self.ksize, strides=self.strides, data_format='NHWC', padding=\"VALID\").numpy()\n",
    "        self.cache = (A_prev, Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev, Z = self.cache\n",
    "        dA_prev = tf.raw_ops.MaxPoolGrad(orig_input=A_prev, orig_output=Z, grad=dZ, ksize=self.ksize, strides=self.strides, padding=\"VALID\", data_format='NHWC').numpy()\n",
    "        return dA_prev\n",
    "    \n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "           \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (np.prod(shape), m)\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.flatten().reshape(m,np.prod(shape)).T\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.T.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        self.parameters = dict()\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, verbose, batch_size=32): \n",
    "        #print(Y.shape)\n",
    "        # Prepare the training dataset with batches\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X, Y.T))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        for epoch in range(epochs):\n",
    "            for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "                #print(y_batch.numpy().T.shape)\n",
    "                y_batch = y_batch.numpy().T\n",
    "                # FORWARD PROP\n",
    "                Z = x_batch\n",
    "                for layer in self.layers:    \n",
    "                    if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                        Z = layer.forward(Z, y_batch)\n",
    "                    else:\n",
    "                        Z = layer.forward(Z)\n",
    "                    #print(layer, Z.shape)\n",
    "\n",
    "                # COST FUNCTION\n",
    "                \n",
    "                # Keras cost\n",
    "#                 loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#                 TL = loss(Y, Z)\n",
    "#                 print(\"ALT COST\", TL)\n",
    "                \n",
    "                # our cost\n",
    "                cost = compute_cost(Z, y_batch)\n",
    "                history.append(cost)\n",
    "                if verbose == 1:\n",
    "                    print(\"Step {:.0f} in epoch {:.0f} - Cost: {:.8f}\\r\".format(step, epoch, cost), end=\"\")\n",
    "\n",
    "                # BACKWARD PROP\n",
    "                m = y_batch.shape[1]\n",
    "                dA = -(1/m)*(np.divide(y_batch, Z) - np.divide(1 - y_batch, 1 - Z)) # derivative of cost with respect to Z\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    dA = layer.backward(dA)\n",
    "\n",
    "                # UPDATE PARAMS\n",
    "                for layer in self.layers:\n",
    "                    layer.update_params(learning_rate)\n",
    "            print(\"\\n\\n\", \"=\"*75, \"\\n\")\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                Z = layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = layer.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"-\"*25)\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    def _cost(self, X, Y):\n",
    "        epsilon=1e-7\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            if prop_layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "               #J1 = prop_layer.forward(Z+ epsilon, Y) \n",
    "               #J2 = prop_layer.forward(Z- epsilon, Y) \n",
    "                \n",
    "                #print(\"TESTING: \",(J1-J2)/(2*epsilon))\n",
    "                \n",
    "                Z = prop_layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = prop_layer.forward(Z)\n",
    "            #print(prop_layer, Z)\n",
    "        # COMPUTE COST\n",
    "        return compute_cost(Z, Y)\n",
    "    \n",
    "    def gradcheck(self, X, Y, epsilon=1e-7, numlayers=None):\n",
    "        self.approx_grads = []\n",
    "        self.true_grads = []\n",
    "        print(\"Starting loops...\")\n",
    "        j=1\n",
    "        for layer in self.layers[:numlayers]:\n",
    "            print(j, len(self.layers))\n",
    "            j += 1\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "                \n",
    "            for i in range(layer.W.size):\n",
    "                i = np.unravel_index(i, layer.W.shape)\n",
    "                Wi = layer.W[i]\n",
    "                layer.W[i] = Wi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi\n",
    "                #print(J1)\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "            print(\"Done W's\")  \n",
    "            for i in range(layer.b.size):\n",
    "                i = np.unravel_index(i, layer.b.shape)\n",
    "                bi = layer.b[i]\n",
    "                layer.b[i] = bi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.b[i] = bi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.b[i] = bi\n",
    "                #print((J1-J2))\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "            print(\"Done B's\")\n",
    "        \n",
    "        # FORWARD PROP\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            #print(prop_layer)\n",
    "            if prop_layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                Z = prop_layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = prop_layer.forward(Z)\n",
    "                \n",
    "        # BACKWARD PROP\n",
    "        m = Y.shape[1]\n",
    "        dA = -(1/m)*(np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        \n",
    "        for layer in self.layers[:numlayers]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            self.true_grads = np.concatenate((self.true_grads, layer.dW.flatten(), layer.db.flatten()))\n",
    "        return np.sqrt(np.sum(np.square(self.true_grads-self.approx_grads)))/(np.sqrt(np.sum(np.square(self.true_grads)))+np.sqrt(np.sum(np.square(self.approx_grads))))\n",
    "\n",
    "    \n",
    "class knn_differentiable(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__(input_shape, num_classes, False)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_features, batch_labels):\n",
    "        self.batch_features = np.transpose(batch_features).astype('float')\n",
    "#         print(\"BATCH_FEATURES\\n\", self.batch_features)\n",
    "#         print()\n",
    "        self.batch_labels = batch_labels.astype('float')\n",
    "        \n",
    "        self.distances = self.calc_distance_mtx(self.batch_features, self.batch_features) / 1000\n",
    "        #self.distances = np.divide(1, self.distances, where=self.distances!=0)\n",
    "#         print(\"DISTANCES\\n\",self.distances)\n",
    "#         print()\n",
    "        \n",
    "        self.class_0 = np.array(self.batch_labels[:] == 0).astype('float')\n",
    "        self.class_1 = np.array(self.batch_labels[:] == 1).astype('float')\n",
    "        self.num_0 = np.count_nonzero(self.batch_labels == 0)\n",
    "        self.num_1 = np.count_nonzero(self.batch_labels == 1)\n",
    "#         print(\"SIZES\", self.num_0, self.num_1)\n",
    "#         print()\n",
    "\n",
    "        self.aggregate = np.stack([np.sum(np.multiply(self.distances, self.class_0), 1) / self.num_0, np.sum(np.multiply(self.distances, self.class_1), 1) / self.num_1], axis=1)\n",
    "#         print(\"AGGREGATE\\n\", self.aggregate)\n",
    "#         print()\n",
    "        \n",
    "        exp = np.exp(-self.aggregate)\n",
    "#         print(\"EXP OF AGGREGATE\\n\",exp)\n",
    "#         print(exp[:,0].shape)\n",
    "#         print()\n",
    "        \n",
    "        self.softmax = np.divide(exp[:,1], np.sum(exp, 1))\n",
    "#         print(\"SOFTMAX\\n\",self.softmax[:50])\n",
    "#         print(\"=\"*20)\n",
    "#         print()\n",
    "        \n",
    "        return np.reshape(self.softmax, (1,self.distances.shape[0]))\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        # Overall what we need here:\n",
    "        # d(TL)/d(X = features) = d(TL)/D(L) * d(L)/d(S) * \n",
    "        #     [(d(S)/d(A_0) * d(A_0)/d(D_0) * d(D_0)/d(X)) + (d(S)/d(A_1) * d(A_1)/d(D_1) * d(D_1)/d(X))]\n",
    "        #\n",
    "        # Shapes:\n",
    "        # d(TL)/d(L)  -  (N x 1)           N = number of samples (images)\n",
    "        # d(L)/d(S)   -  (N x 1)\n",
    "        # d(S)/d(Ai)  -  (N x 1)\n",
    "        # d(Ai)/d(Di) -  (N x N)\n",
    "        # d(Di)/d(X)  -  (N x f)           f = number of features (output by cnn part)\n",
    "        # final output:  (N x f)\n",
    "        \n",
    "        \n",
    "        # d(TL)/d(L) = -1/m * sum(d(L)/d(S))  <-- Maybe don't need this at all, done before calling backward\n",
    "        \n",
    "        \n",
    "        # d(L)/d(S) = (Y/S) - (1-Y)/(1-S)    <-- Y = true labels, S = softmax labels from column 0 (softmax on class 0)\n",
    "        dL_dS = dA.T   # <-- Really dTL_dS\n",
    "        \n",
    "#         print(\"dL_dS\", dL_dS.shape, dL_dS[:25])\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(S)/d(A) = - (e^-A0 * e^-A1) / (e^-2A0 + e^-A0-A1 + e^-2A1)\n",
    "        # This calc is for A_0 and A_1, A_1 is positive version of this\n",
    "        #np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))\n",
    "        dS_dA0 = -np.divide(np.multiply(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])), \n",
    "                            np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))).reshape(self.distances.shape[0],1)\n",
    "        \n",
    "        # np.add(np.add(np.exp(-2*self.aggregate[:,0]), np.exp(-2*self.aggregate[:,1])), \n",
    "        #                           np.exp(np.multiply(self.aggregate[:,0], self.aggregate[:,1])))\n",
    "        \n",
    "        \n",
    "        dS_dA1 = -dS_dA0\n",
    "#         print(\"dS_dA0\", dS_dA0.shape, dS_dA0[:25]) (1000,) (1000,1)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(A)/d(D) = 1/len(class) * sum(d(D)/d(X))   <-- Same calc for A_0 and A_1, len(class) = how many of this class there are either num_0 or num_1\n",
    "        # possibly don't need this part either since its calculated within loops???\n",
    "        \n",
    "        \n",
    "        # d(D)/d(X) = 2(x_i - x_j)   <-- x_i and x_j are the two feature vectors used in the distance\n",
    "        \n",
    "        dA0_dX = np.ones(self.batch_features.shape)\n",
    "        dA1_dX = np.ones(self.batch_features.shape)\n",
    "        # iterate over rows in distance matrix\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD0_dX = np.zeros(self.batch_features.shape[1])\n",
    "            dD1_dX = np.zeros(self.batch_features.shape[1])\n",
    "            # iterate over the columns in distance matrix\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                # only calculate derivatives for indices of class 0\n",
    "                if self.class_0[:,j] == 0:\n",
    "                    # first calc derivative of distance formula, then them all together\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD0_dX = np.add(dD0_dX, deriv)\n",
    "                if self.class_0[:,j] == 1:\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD1_dX = np.add(dD1_dX, deriv)\n",
    "            # replace the corresponding derivatives for each row (sample)\n",
    "            dA0_dX[i,:] = dD0_dX\n",
    "            dA1_dX[i,:] = dD1_dX\n",
    "            \n",
    "\n",
    "        \n",
    "        # Same as loop above but on indices of class 1\n",
    "#         dA1_dX = np.ones(self.batch_features.shape)\n",
    "#         for i in range(self.distances.shape[0]):\n",
    "#             dD1_dX = np.zeros(self.batch_features.shape[1])\n",
    "#             for j in range(self.distances.shape[1]):\n",
    "#                 if self.class_0[:,j] == 1:\n",
    "#                     deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "#                     dD1_dX = np.add(dD_dX, deriv)\n",
    "#             dA1_dX[i,:] = dD_dX\n",
    "        \n",
    "\n",
    "        # divide by the number of samples in each class since these were averaged in forward\n",
    "        dA0_dX = dA0_dX * (1/self.num_0) *(1/1000)\n",
    "        dA1_dX = dA1_dX * (1/self.num_1) *(1/1000)\n",
    "#         print(\"dA0_dX\", dA0_dX.shape, dA0_dX)\n",
    "#         print()\n",
    "#         print(\"dA1_dX\", dA0_dX.shape, dA1_dX)\n",
    "#         print()\n",
    "\n",
    "        # add together the terms in brackets from the full formula\n",
    "        dS_dX = np.add(np.multiply(dS_dA0, dA0_dX), np.multiply(dS_dA1, dA1_dX))\n",
    "        # final step of the chain rule\n",
    "        dL_dX = np.multiply(dL_dS, dS_dX)\n",
    "#         print(\"dL_dX\", dL_dX.shape, dL_dX)\n",
    "        \n",
    "#         print('='*50)\n",
    "        return dL_dX\n",
    "        #return np.zeros(self.batch_features.shape)\n",
    "    \n",
    "    def calc_distance_mtx(self, A, B):\n",
    "        \"\"\"\n",
    "        Computes squared pairwise distances between each elements of A and each elements of B.\n",
    "        Args:\n",
    "        A,    [m,d] matrix\n",
    "        B,    [n,d] matrix\n",
    "        Returns:\n",
    "        D,    [m,n] matrix of pairwise distances\n",
    "        \"\"\"\n",
    "        with tf.compat.v1.variable_scope('pairwise_dist'):\n",
    "            # squared norms of each row in A and B\n",
    "            na = tf.reduce_sum(tf.square(A), 1)\n",
    "            nb = tf.reduce_sum(tf.square(B), 1)\n",
    "\n",
    "            # na as a row and nb as a co\"lumn vectors\n",
    "            na = tf.reshape(na, [-1, 1])\n",
    "            nb = tf.reshape(nb, [1, -1])\n",
    "\n",
    "            # return pairwise euclidead difference matrix\n",
    "            D = tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0)\n",
    "        return D.numpy()\n",
    "    \n",
    "    def calc_cosine_sim(self, A, B):\n",
    "        from scipy.spatial.distance import pdist\n",
    "        from scipy.spatial.distance import squareform\n",
    "        cos_sim = squareform(pdist(A, metric='cosine'))\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 in epoch 0 - Cost: 0.49679252\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 0.45039604\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.46189597\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.39845785\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.36905287\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.32708245\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.19627846\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.35042567\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.35950467\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.43452208\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 1.3720808029174805\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "#np.random.seed(10)\n",
    "m = 160\n",
    "\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "    Maxpool((None, 22, 22, 32), pool_size=3),\n",
    "    Flatten((None, 1, 1, 32)),\n",
    "    knn_differentiable((2048, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "start_time = time.time()\n",
    "history = model.fit(X, y, epochs=10, learning_rate=0.0015, verbose=1, batch_size=32)\n",
    "end_time = time.time()\n",
    "print(\"Execution Time:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99983465e-01 9.99962001e-01 9.99890129e-01 9.99564080e-01\n",
      "  7.95259059e-05 9.99910000e-01 3.89135064e-02 9.22022435e-01\n",
      "  9.99995664e-01 9.99738728e-01]]\n",
      "[[0.05655314 0.99991938 0.99560484 0.16382735 0.99970644 0.9873207\n",
      "  0.00274944 0.00280653 0.99955826 0.08131807]]\n",
      "[[9.99995333e-01 3.69113974e-06 5.79847204e-04 1.80639226e-08\n",
      "  6.70020890e-06 9.99999430e-01 9.99999978e-01 9.99999639e-01\n",
      "  9.99999990e-01 9.99999759e-01]]\n",
      "[[1.09060440e-08 9.99043566e-01 1.91733830e-04 9.99999929e-01\n",
      "  9.99999590e-01 9.99991582e-01 5.05068565e-07 9.99861671e-01\n",
      "  8.73754404e-01 9.99999951e-01]]\n",
      "[[9.25621911e-01 3.01707739e-04 9.99269375e-01 6.66484195e-03\n",
      "  6.90141299e-01 9.94148582e-01 9.99521574e-01 9.97602935e-01\n",
      "  5.90601053e-02 3.08269594e-01]]\n",
      "[[9.99989553e-01 9.99982042e-01 9.87337093e-01 8.38147042e-04\n",
      "  3.43528813e-03 5.14914788e-04 9.99979197e-01 5.17714912e-04\n",
      "  7.33791755e-07 3.71202202e-01]]\n",
      "[[9.99988207e-01 1.66894544e-03 7.84444246e-07 5.13597278e-04\n",
      "  9.99979251e-01 9.33749254e-01 9.99987465e-01 1.63382336e-07\n",
      "  3.46355477e-05 9.99997993e-01]]\n",
      "[[9.99999565e-01 3.01324749e-06 9.99999850e-01 2.78560501e-01\n",
      "  9.99999926e-01 9.99999686e-01 9.99999911e-01 1.24043355e-01\n",
      "  2.58855598e-09 9.99997168e-01]]\n",
      "[[9.63660165e-01 2.42461323e-03 3.83205436e-08 9.99996359e-01\n",
      "  2.33587989e-02 9.99993034e-01 4.45231021e-03 4.43491612e-02\n",
      "  9.99840683e-01 3.43945579e-02]]\n",
      "[[9.93613914e-01 9.99856253e-01 9.99894566e-01 6.03776437e-05\n",
      "  9.99852164e-01 1.67235410e-03 9.99729828e-01 8.80820587e-05\n",
      "  8.11666514e-04 8.13062842e-02]]\n",
      "[[2.64498455e-05 1.50145042e-02 9.99997218e-01 6.38014849e-03\n",
      "  2.11948142e-06 9.99994717e-01 7.73747550e-01 5.88045003e-05\n",
      "  9.99992755e-01 9.99854542e-01]]\n",
      "[[0.99886957 0.99570275 0.20516058 0.99995738 0.60728725 0.99995577\n",
      "  0.04082789 0.99955246 0.99990938 0.97828433]]\n",
      "[[0.9997607  0.00184183 0.04982697 0.00853225 0.99176427 0.96531664\n",
      "  0.98042232 0.99941277 0.00990486 0.00857162]]\n",
      "[[8.91283158e-01 1.41715676e-06 3.41035759e-06 9.99999124e-01\n",
      "  8.90950129e-01 3.20733814e-04 1.98426048e-05 4.79027979e-04\n",
      "  9.99977500e-01 9.99898570e-01]]\n",
      "[[9.99949091e-01 1.59560067e-07 9.92472029e-01 9.98489998e-01\n",
      "  1.01100554e-06 3.01397389e-05 9.99999166e-01 3.27642503e-06\n",
      "  9.99947642e-01 8.21812347e-06]]\n",
      "[[0.99474003 0.15595399 0.03260022 0.99732559 0.01912034 0.90432897\n",
      "  0.04518819 0.88846809 0.98036314 0.82583557]]\n",
      "[[4.34194013e-01 7.69366665e-06 9.99998041e-01 1.57506275e-04\n",
      "  5.90093280e-07 9.99999763e-01 9.99955250e-01 9.99991805e-01\n",
      "  9.99999356e-01 9.99507197e-01]]\n",
      "[[0.9741789  0.08473445 0.96375536 0.98819273 0.65831249 0.96617534\n",
      "  0.41958794 0.60188612 0.96053075 0.0041824 ]]\n",
      "[[0.15866383 0.99321516 0.99810813 0.99925639 0.9998831  0.99988333\n",
      "  0.99856524 0.01026732 0.99993336 0.07065739]]\n",
      "[[0.90958702 0.81104754 0.99997832 0.41618702 0.01318722 0.01825074\n",
      "  0.99978556 0.39165854 0.66329058 0.10273157]]\n",
      "[[5.65971627e-05 9.99997573e-01 9.99999700e-01 7.81545733e-02\n",
      "  9.99997436e-01 2.97695537e-08 7.16581715e-03 1.64834542e-02\n",
      "  1.58619771e-04 6.90567119e-06]]\n",
      "[[9.99910312e-01 4.34640547e-01 1.00299645e-06 1.76143147e-04\n",
      "  9.99980663e-01 9.99453449e-01 9.99990781e-01 7.07963349e-01\n",
      "  8.31487031e-05 2.57819407e-03]]\n",
      "[[0.00276493 0.9908561  0.97759194 0.01789538 0.73168122 0.99819801\n",
      "  0.95693374 0.99431715 0.98672239 0.868186  ]]\n",
      "[[0.99998605 0.99898973 0.99994445 0.99998978 0.01284465 0.9650757\n",
      "  0.0059942  0.99238669 0.99997141 0.99901616]]\n",
      "[[0.99963569 0.99990425 0.99982591 0.86091665 0.00866674 0.88381949\n",
      "  0.99994472 0.00123607 0.51936994 0.90015919]]\n",
      "[[9.99535375e-01 9.99994078e-01 9.96057184e-07 9.99988383e-01\n",
      "  2.02263121e-02 5.29987840e-03 1.19570900e-02 1.48377157e-02\n",
      "  9.99994374e-01 1.01203025e-03]]\n",
      "[[9.99870851e-01 2.10689536e-04 2.63061564e-02 2.11122446e-02\n",
      "  4.52367094e-03 9.99940614e-01 2.77572313e-04 9.99884049e-01\n",
      "  8.85006015e-03 5.77743275e-02]]\n",
      "[[9.99973445e-01 9.99821469e-01 9.99966525e-01 3.63085646e-02\n",
      "  6.69292692e-06 1.82576950e-05 8.57264636e-01 8.13563495e-05\n",
      "  9.99996005e-01 9.99975266e-01]]\n",
      "[[9.99997836e-01 5.94341677e-01 9.99997952e-01 1.51345161e-03\n",
      "  3.83894652e-04 9.99999759e-01 9.99999761e-01 3.59989799e-05\n",
      "  9.99999738e-01 9.99999902e-01]]\n",
      "[[9.99998012e-01 2.34811636e-06 2.15891665e-04 3.09806867e-06\n",
      "  2.42228876e-04 9.99998412e-01 9.99997816e-01 9.99999521e-01\n",
      "  9.83389567e-01 9.99985895e-01]]\n",
      "[[5.76548720e-04 5.20068674e-03 9.99621471e-01 7.01982226e-03\n",
      "  2.69742494e-01 2.19780424e-01 2.90201525e-04 9.93092010e-01\n",
      "  9.80500881e-01 9.98877614e-01]]\n",
      "[[9.99889206e-01 9.99859797e-01 2.47304604e-02 9.99939702e-01\n",
      "  9.99987464e-01 2.14361328e-04 5.53379794e-07 1.23977088e-03\n",
      "  1.16578260e-02 9.99961866e-01]]\n",
      "[[0.99949721 0.10492381 0.99119289 0.00437557 0.99988905 0.98219654\n",
      "  0.08744748 0.99987455 0.50377733 0.01332956]]\n",
      "[[9.99999978e-01 1.00000000e+00 8.33903462e-04 9.97101692e-01\n",
      "  1.00000000e+00 1.00000000e+00 2.04002464e-14 9.99999996e-01\n",
      "  9.99999999e-01 1.00000000e+00]]\n",
      "[[0.20899042 0.99193471 0.05863395 0.99954463 0.2103172  0.04010071\n",
      "  0.11117865 0.45327165 0.12723495 0.99997832]]\n",
      "[[1.54631092e-04 6.86427532e-02 9.99836537e-01 1.94607018e-03\n",
      "  1.04870929e-02 9.99916109e-01 9.99753824e-01 1.98259155e-03\n",
      "  5.36243783e-04 2.27995477e-01]]\n",
      "[[4.38211000e-04 9.99480435e-01 4.22122365e-02 6.70198470e-04\n",
      "  8.58035085e-03 9.99328369e-01 5.11698395e-02 1.65682888e-01\n",
      "  2.79900632e-04 9.99951034e-01]]\n",
      "[[9.97447629e-01 9.97818270e-01 9.99623845e-01 9.58477800e-01\n",
      "  9.99608239e-01 9.36230428e-05 9.99585051e-01 8.55017958e-01\n",
      "  9.97633101e-01 9.05284717e-01]]\n",
      "[[9.99009629e-01 1.53336978e-01 5.43333591e-01 9.98692040e-01\n",
      "  9.99551562e-01 9.05705302e-01 9.94357971e-01 9.98151339e-01\n",
      "  9.25398326e-01 8.06430108e-04]]\n",
      "[[9.95706231e-01 1.55632756e-06 9.99990426e-01 9.99906680e-01\n",
      "  4.53675895e-07 3.06650992e-07 1.53347978e-06 1.95787331e-03\n",
      "  9.99998462e-01 9.99845809e-01]]\n",
      "[[9.99646430e-01 9.99589446e-01 1.48102724e-04 9.99888700e-01\n",
      "  9.99945154e-01 9.99887473e-01 1.52046808e-03 9.99902848e-01\n",
      "  9.99954898e-01 9.47198741e-01]]\n",
      "[[6.86999509e-06 9.06047838e-01 4.70133497e-03 1.25732095e-02\n",
      "  9.99979957e-01 9.99971960e-01 9.99975924e-01 7.27493744e-01\n",
      "  3.67898250e-05 9.99143332e-01]]\n",
      "[[9.99395204e-01 9.99748182e-01 9.99403317e-01 9.99929309e-01\n",
      "  7.30246204e-02 9.99805128e-01 8.80688577e-02 2.51185504e-05\n",
      "  3.56881241e-02 9.99676843e-01]]\n",
      "[[0.99949812 0.84740731 0.99965397 0.98136132 0.00729109 0.99929376\n",
      "  0.99953795 0.00127987 0.87435436 0.00234097]]\n",
      "[[0.99994203 0.30976815 0.00166895 0.37026023 0.99964873 0.9981234\n",
      "  0.9869132  0.99997836 0.00174876 0.03083322]]\n",
      "[[7.05793565e-03 9.93541914e-01 2.78902188e-05 7.09584719e-02\n",
      "  9.99945238e-01 9.99964838e-01 9.91340978e-01 9.02269318e-05\n",
      "  8.55311561e-01 9.99994240e-01]]\n",
      "[[9.99990180e-01 4.36095280e-06 9.91324460e-01 9.99995685e-01\n",
      "  9.99997517e-01 6.07274239e-01 9.32104597e-06 2.62248603e-01\n",
      "  1.38244284e-02 1.71361143e-05]]\n",
      "[[9.99984566e-01 6.19101362e-03 9.99952419e-01 9.99913415e-01\n",
      "  9.98387263e-01 3.14421238e-02 6.03867066e-05 9.99968340e-01\n",
      "  8.47994024e-01 9.99988556e-01]]\n",
      "[[1.29039243e-05 9.99999980e-01 9.99618450e-01 9.98483632e-01\n",
      "  2.22135397e-08 9.99999990e-01 9.99999985e-01 5.13089312e-08\n",
      "  9.99999987e-01 3.55831711e-06]]\n",
      "[[4.42221938e-01 8.82616085e-02 7.47831310e-02 9.99973864e-01\n",
      "  9.99811327e-01 9.99976913e-01 9.99491707e-01 4.04830670e-08\n",
      "  7.21232977e-02 9.99983550e-01]]\n",
      "[[9.99923218e-01 5.59462036e-01 9.34145565e-01 1.33601711e-02\n",
      "  9.99655116e-01 5.64654856e-04 5.96200903e-03 6.34427167e-04\n",
      "  9.99953547e-01 9.99935136e-01]]\n",
      "[[9.99958906e-01 9.99995883e-01 9.99994085e-01 8.37627906e-02\n",
      "  9.68974190e-01 9.11225111e-04 9.99969352e-01 1.24116343e-03\n",
      "  9.99995823e-01 9.95908717e-01]]\n",
      "[[5.49000787e-01 9.99842537e-01 9.99995374e-01 9.99963479e-01\n",
      "  6.17728975e-01 7.85512237e-04 2.38280189e-02 9.99894605e-01\n",
      "  9.69515013e-01 9.99971241e-01]]\n",
      "[[8.52165355e-01 3.97947564e-04 6.98856497e-05 3.66287571e-04\n",
      "  5.21266973e-04 3.19343745e-05 3.90390274e-06 5.09478646e-04\n",
      "  9.97381921e-01 9.84735679e-01]]\n",
      "[[9.99994718e-01 9.99998837e-01 9.99989694e-01 9.99998661e-01\n",
      "  1.59105779e-03 1.17521060e-02 9.99998454e-01 9.99997858e-01\n",
      "  4.48624132e-06 9.99985859e-01]]\n",
      "[[1.37296485e-04 1.07865385e-03 5.03041347e-01 9.90887118e-01\n",
      "  9.99954143e-01 5.45045659e-04 1.19559415e-04 9.99977863e-01\n",
      "  9.99814145e-01 9.99885404e-01]]\n",
      "[[9.99998857e-01 5.42508017e-06 9.99655873e-01 9.99995162e-01\n",
      "  1.31765043e-06 8.67512049e-01 4.23808447e-05 9.99991827e-01\n",
      "  2.56328893e-07 2.57236953e-02]]\n",
      "[[6.64615736e-01 4.81536716e-08 4.13164292e-03 9.99977494e-01\n",
      "  9.99928793e-01 9.99999084e-01 9.99982890e-01 8.93262300e-04\n",
      "  9.99951103e-01 9.99993841e-01]]\n",
      "[[9.52152633e-01 6.14558550e-04 9.99892842e-01 9.99856374e-01\n",
      "  1.19387721e-02 9.99991437e-01 7.70879092e-01 2.95926826e-02\n",
      "  9.99792938e-01 9.99996459e-01]]\n",
      "[[3.20891400e-05 9.99932876e-01 9.99987831e-01 1.58485757e-03\n",
      "  2.72415584e-03 9.99977232e-01 9.99986930e-01 9.99966911e-01\n",
      "  9.80515588e-01 8.32745326e-01]]\n",
      "[[4.51955132e-04 9.99964613e-01 1.24968232e-01 2.39220139e-06\n",
      "  9.99933365e-01 1.67179690e-02 7.03926103e-05 9.99580152e-01\n",
      "  9.99931215e-01 9.83553448e-01]]\n",
      "[[9.97514098e-01 9.99975469e-01 9.99716672e-01 9.99835710e-01\n",
      "  5.38975730e-03 1.21172994e-03 3.31099956e-01 2.70415056e-01\n",
      "  4.68516047e-07 9.99895203e-01]]\n",
      "[[9.99971556e-01 9.99990976e-01 1.76659894e-04 3.24883480e-03\n",
      "  9.99938124e-01 3.30687887e-01 7.62627318e-03 6.14352837e-01\n",
      "  6.06104265e-01 9.85526668e-01]]\n",
      "[[5.26138441e-01 9.99999977e-01 9.99999614e-01 9.99999957e-01\n",
      "  9.99999788e-01 7.90899682e-08 9.99999954e-01 1.53523395e-03\n",
      "  1.26638093e-06 9.99999075e-01]]\n",
      "[[9.99950061e-01 7.18633004e-06 2.59424176e-01 9.99960815e-01\n",
      "  9.45295726e-01 7.32742116e-01 9.99960072e-01 9.99907020e-01\n",
      "  9.22473826e-03 9.99800693e-01]]\n",
      "[[9.11503617e-04 6.45343717e-13 9.99999982e-01 1.17303336e-06\n",
      "  2.24370820e-06 3.13449648e-02 1.79280315e-01 9.99999994e-01\n",
      "  7.22230814e-04 6.42700241e-06]]\n",
      "[[7.22170685e-04 7.74718340e-01 9.99956513e-01 4.41160096e-03\n",
      "  9.60234297e-02 1.79571748e-02 9.99911732e-01 1.38712272e-02\n",
      "  9.99809825e-01 9.99897816e-01]]\n",
      "[[6.46240563e-02 1.55895382e-01 4.61925794e-01 9.99798235e-01\n",
      "  2.85505565e-04 2.28842662e-02 9.96858007e-01 4.21478819e-02\n",
      "  1.68248995e-02 9.44420746e-01]]\n",
      "[[5.49910600e-03 9.99954417e-01 6.53363667e-02 9.99978125e-01\n",
      "  9.37343064e-03 9.99970600e-01 9.99997025e-01 1.43379383e-05\n",
      "  9.99204041e-01 9.99921996e-01]]\n",
      "[[9.99993750e-01 2.56256273e-03 3.24166241e-02 2.41288839e-01\n",
      "  3.17835794e-03 2.69992521e-02 9.99969686e-01 9.99327318e-01\n",
      "  6.09514844e-04 2.81276849e-01]]\n",
      "[[0.99988893 0.9691673  0.99943749 0.49329937 0.05380882 0.99947023\n",
      "  0.99803365 0.48386511 0.00209057 0.53777102]]\n",
      "[[2.50002583e-01 2.14374239e-04 1.45685172e-04 9.99598425e-01\n",
      "  9.99982091e-01 9.99989359e-01 3.73757315e-04 3.16292403e-07\n",
      "  3.37543809e-02 9.99805206e-01]]\n",
      "[[9.99976993e-01 1.99599851e-06 9.92244135e-01 1.34441585e-01\n",
      "  3.50015216e-06 9.99978760e-01 9.99996538e-01 1.48445652e-03\n",
      "  2.87819740e-02 1.31005508e-05]]\n",
      "[[1.02238277e-02 6.12420395e-01 9.99423630e-01 9.97758226e-01\n",
      "  7.20790889e-04 2.10053627e-03 9.99044648e-01 9.97272081e-01\n",
      "  9.98568036e-01 2.82785884e-03]]\n",
      "[[3.69350747e-03 9.99619789e-01 9.99997799e-01 9.89666824e-01\n",
      "  8.51952235e-04 2.79635546e-04 1.67457969e-04 9.99976299e-01\n",
      "  9.99993878e-01 9.99998683e-01]]\n",
      "[[0.00128487 0.31800181 0.92265515 0.0142873  0.01969041 0.00115336\n",
      "  0.00237082 0.88472209 0.94765841 0.09015946]]\n",
      "[[0.00342579 0.99817236 0.99521087 0.00120973 0.99829121 0.08348001\n",
      "  0.99390965 0.03961215 0.01527205 0.98458221]]\n",
      "[[0.99316113 0.0158081  0.99951594 0.99875728 0.00489695 0.00817603\n",
      "  0.99808627 0.99694504 0.99770871 0.96149562]]\n",
      "[[1.88236476e-04 4.46207659e-02 7.03157582e-05 8.76292324e-01\n",
      "  9.99995012e-01 5.38029659e-01 9.99974873e-01 4.28178687e-07\n",
      "  3.80674122e-02 9.99989811e-01]]\n",
      "[[9.99996194e-01 7.39033428e-04 1.71683238e-08 9.99999310e-01\n",
      "  9.99999345e-01 9.99998790e-01 9.99999685e-01 9.15104211e-01\n",
      "  3.92831649e-03 5.57285606e-03]]\n",
      "[[9.99993646e-01 8.45200669e-01 7.25431012e-03 9.15455852e-01\n",
      "  9.99996367e-01 5.52161938e-02 9.99985078e-01 3.60959576e-05\n",
      "  9.99995785e-01 9.98426090e-01]]\n",
      "[[5.78257926e-06 1.35700462e-07 9.99999517e-01 2.41275881e-02\n",
      "  9.99998748e-01 9.99895954e-01 9.99999614e-01 2.06297506e-07\n",
      "  5.54669095e-05 1.77244182e-05]]\n",
      "[[2.58462345e-05 3.15903371e-03 1.93635195e-01 5.01803108e-05\n",
      "  4.52779532e-01 7.86390094e-06 2.50547221e-04 9.99983319e-01\n",
      "  9.99884150e-01 9.99988564e-01]]\n",
      "[[9.99973315e-01 1.17543653e-04 6.64522587e-01 9.82123084e-01\n",
      "  9.99950885e-01 1.85566223e-01 9.99965764e-01 9.99927127e-01\n",
      "  9.99862203e-01 9.96992572e-02]]\n",
      "[[8.34418140e-01 9.99999836e-01 9.99999456e-01 1.41734842e-04\n",
      "  4.08296606e-08 9.99999962e-01 1.08049390e-05 3.10966705e-07\n",
      "  9.99999965e-01 2.81397784e-05]]\n",
      "[[1.17525010e-02 2.36013459e-04 9.92937245e-01 9.99799064e-01\n",
      "  2.51285412e-02 3.96378840e-02 2.14456286e-03 2.30272605e-02\n",
      "  9.99731734e-01 4.30943859e-03]]\n",
      "[[3.84276395e-06 9.99979722e-01 1.01922846e-03 9.99971575e-01\n",
      "  9.99976832e-01 2.39298553e-02 4.93091990e-04 2.11441778e-05\n",
      "  9.99926245e-01 6.39429189e-03]]\n",
      "[[9.99980056e-01 9.99995798e-01 7.23132237e-01 4.40386554e-01\n",
      "  9.99994296e-01 9.99973337e-01 9.99995297e-01 9.99985904e-01\n",
      "  1.40345126e-06 9.99999249e-01]]\n",
      "[[3.02558485e-04 9.99963875e-01 9.99993466e-01 1.59497687e-01\n",
      "  9.99990383e-01 1.14621221e-03 9.95530878e-01 9.99988509e-01\n",
      "  1.12824539e-02 4.00008872e-04]]\n",
      "[[9.99958040e-01 9.99994875e-01 9.99996804e-01 9.99978402e-01\n",
      "  9.99564761e-01 2.25679915e-03 1.51814467e-02 1.27771023e-04\n",
      "  6.57013041e-05 9.99988945e-01]]\n",
      "[[0.97819907 0.0142878  0.99922165 0.99178843 0.72829069 0.99913934\n",
      "  0.0127081  0.00922781 0.01105823 0.99984592]]\n",
      "[[9.99845191e-01 9.99978107e-01 6.18440669e-06 9.99988605e-01\n",
      "  9.99884513e-01 9.99926657e-01 9.99990316e-01 9.99996406e-01\n",
      "  9.99943712e-01 1.54663745e-03]]\n",
      "[[1.68122063e-06 6.57922919e-07 9.99995696e-01 7.66251983e-01\n",
      "  7.54250008e-01 1.30704898e-03 9.99968489e-01 9.99997833e-01\n",
      "  9.99985970e-01 9.99997503e-01]]\n",
      "[[0.00338546 0.99776681 0.87006375 0.94152285 0.99950627 0.89878037\n",
      "  0.99958032 0.82517965 0.00332768 0.0115423 ]]\n",
      "[[3.68517237e-06 9.99999775e-01 9.98833696e-01 9.99998228e-01\n",
      "  1.39099244e-08 9.99999911e-01 1.51075941e-06 9.99999849e-01\n",
      "  1.21497268e-03 2.53294374e-03]]\n",
      "[[9.99861831e-01 6.20545082e-05 9.99931282e-01 9.99889061e-01\n",
      "  8.70929664e-01 9.99984305e-01 7.62131158e-04 9.99996413e-01\n",
      "  7.49897927e-02 9.99983544e-01]]\n",
      "[[9.22830298e-07 9.99999108e-01 2.19974444e-01 2.48005488e-01\n",
      "  9.99998582e-01 9.99999577e-01 6.22888257e-09 1.87371861e-04\n",
      "  1.92301699e-06 5.91029976e-01]]\n",
      "[[9.96961156e-01 6.39938378e-04 9.99614025e-01 2.38216256e-03\n",
      "  6.63589004e-04 1.17400253e-02 9.99512439e-01 3.67307921e-04\n",
      "  9.67385023e-01 7.79883805e-02]]\n",
      "[[9.99992197e-01 5.29412028e-05 9.99965287e-01 5.96726170e-02\n",
      "  9.99942245e-01 9.10125392e-01 9.99390960e-01 9.46795966e-01\n",
      "  3.03470954e-05 4.22129606e-05]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99999978e-01 9.99778281e-01 8.64693528e-02 5.13855214e-04\n",
      "  3.85319125e-04 1.67251072e-05 1.11072197e-05 9.99998352e-01\n",
      "  9.99999788e-01 9.99999829e-01]]\n",
      "[[4.90296610e-01 5.39746135e-04 9.98299382e-01 9.99999487e-01\n",
      "  9.99993507e-01 3.52709893e-08 1.15247454e-06 4.83426420e-05\n",
      "  9.99708392e-01 2.59681710e-04]]\n",
      "[[0.99976713 0.0290819  0.00214536 0.99989322 0.99707528 0.01178428\n",
      "  0.26464851 0.99995505 0.99786    0.48236318]]\n",
      "[[0.97833163 0.0141022  0.99977387 0.0186434  0.98994577 0.01542723\n",
      "  0.95561553 0.01730981 0.99987963 0.99444607]]\n",
      "[[6.48286562e-04 1.37222361e-05 9.99495030e-01 1.48528676e-01\n",
      "  3.13880250e-03 3.96625213e-03 2.41232967e-06 9.99948937e-01\n",
      "  8.14384810e-01 9.99622180e-01]]\n",
      "[[0.02375426 0.92600214 0.99998095 0.00448609 0.99996141 0.0324249\n",
      "  0.09011936 0.99996544 0.53922937 0.99998928]]\n",
      "[[0.16505605 0.99870423 0.99724875 0.99969606 0.9457354  0.00261905\n",
      "  0.05553425 0.08567346 0.00489406 0.01040536]]\n",
      "[[9.99989308e-01 4.16662841e-05 9.99883758e-01 9.99894945e-01\n",
      "  9.99957353e-01 1.82170830e-01 7.40645326e-01 7.97559831e-02\n",
      "  1.06206366e-05 9.99716719e-01]]\n",
      "[[9.99964047e-01 9.99997993e-01 9.44523642e-01 9.99994909e-01\n",
      "  9.99998002e-01 9.99545025e-01 3.82355280e-05 1.34665426e-06\n",
      "  9.99999634e-01 1.40264615e-05]]\n",
      "[[9.99874783e-01 1.01634981e-04 9.97342297e-01 9.99786054e-01\n",
      "  2.37161078e-02 9.99851777e-01 9.96210192e-01 2.25635708e-03\n",
      "  9.98320597e-01 1.68017584e-01]]\n",
      "[[9.99460612e-01 9.99951796e-01 9.99921855e-01 7.99859703e-01\n",
      "  9.99953131e-01 7.19057422e-03 9.99529445e-01 6.69134517e-06\n",
      "  9.94627843e-01 2.94515781e-03]]\n",
      "[[8.32828513e-04 9.65869707e-01 9.99997218e-01 5.92905556e-01\n",
      "  9.99843979e-01 9.99992597e-01 1.05319479e-03 5.86300366e-02\n",
      "  9.99998053e-01 9.99998084e-01]]\n",
      "[[9.96608459e-01 9.83204710e-01 1.40018168e-01 3.04628034e-03\n",
      "  5.26044990e-01 9.97495922e-01 5.21681350e-02 9.97833804e-01\n",
      "  3.78369987e-04 1.65179564e-01]]\n",
      "[[9.83104471e-01 4.20762454e-04 3.56350508e-04 9.99323974e-01\n",
      "  2.70913682e-04 9.99528465e-01 5.87149901e-02 5.95980787e-01\n",
      "  4.77264108e-01 9.99182554e-01]]\n",
      "[[8.62325726e-01 8.69721660e-05 1.92946274e-04 2.00422860e-01\n",
      "  6.11049311e-01 9.06226061e-01 9.96738832e-01 9.94337999e-01\n",
      "  1.36897378e-03 7.62440587e-03]]\n",
      "[[9.99861020e-01 9.99336209e-01 2.59235025e-02 9.99638121e-01\n",
      "  1.11430678e-04 9.99304118e-01 9.99864659e-01 2.43018324e-01\n",
      "  1.27680569e-03 9.99948070e-01]]\n",
      "[[0.99888118 0.99997294 0.9998314  0.91279087 0.99980608 0.99934301\n",
      "  0.99989012 0.00132457 0.19918709 0.01651007]]\n",
      "[[1.14251056e-06 5.11764397e-07 6.26650546e-04 9.92247108e-01\n",
      "  9.99999994e-01 8.47895061e-06 9.99999786e-01 1.23259446e-07\n",
      "  5.44323120e-03 9.99999976e-01]]\n",
      "[[9.99930261e-01 5.90256947e-05 9.99999342e-01 9.99676490e-01\n",
      "  3.88308239e-06 5.33914535e-01 9.99958923e-01 9.99933826e-01\n",
      "  9.97087014e-01 9.99992756e-01]]\n",
      "[[9.95609660e-01 9.99098855e-01 9.99929340e-01 9.99816491e-01\n",
      "  5.16965334e-01 2.53183004e-04 1.00091466e-01 3.56213773e-07\n",
      "  9.90528547e-01 2.84228677e-03]]\n",
      "[[2.10428772e-02 8.77993411e-01 9.99996567e-01 9.99993631e-01\n",
      "  2.17210416e-03 1.50370905e-01 1.55776404e-05 9.99769938e-01\n",
      "  1.40428756e-05 9.99227212e-01]]\n",
      "[[9.98995593e-01 5.72213957e-05 9.90437878e-01 9.99484740e-01\n",
      "  9.24799477e-01 9.99011460e-01 9.97344166e-01 9.99466962e-01\n",
      "  2.35230416e-04 9.99389622e-01]]\n",
      "[[0.99996548 0.99993452 0.9999519  0.99997322 0.99998784 0.0428367\n",
      "  0.99999541 0.99999289 0.0051765  0.00682687]]\n",
      "[[9.99999914e-01 8.54469245e-07 9.99999756e-01 1.41786201e-01\n",
      "  9.99999387e-01 7.44176218e-10 9.99999931e-01 9.99991090e-01\n",
      "  9.99999558e-01 6.69996163e-03]]\n",
      "[[7.79949599e-01 2.31300291e-04 9.99830499e-01 4.47963848e-05\n",
      "  9.99273243e-01 9.39011608e-01 9.93901474e-01 9.98108374e-01\n",
      "  3.14236373e-05 7.53355295e-01]]\n",
      "[[9.99989252e-01 9.99986865e-01 7.98989291e-01 1.08887007e-05\n",
      "  9.99983690e-01 9.99942536e-01 9.99911853e-01 1.54792841e-03\n",
      "  9.99988302e-01 9.80212653e-01]]\n",
      "[[9.99999166e-01 9.99968144e-01 9.99999955e-01 9.99999817e-01\n",
      "  9.99999984e-01 9.99999958e-01 9.99999928e-01 5.02297031e-04\n",
      "  9.99999985e-01 2.66052694e-07]]\n",
      "[[7.63568079e-05 9.99977548e-01 9.99999216e-01 9.99996560e-01\n",
      "  9.99976671e-01 8.10787111e-03 9.99981609e-01 9.16329284e-04\n",
      "  5.07373462e-01 6.92848310e-02]]\n",
      "[[9.99971180e-01 1.74659577e-06 1.76772803e-05 9.99960789e-01\n",
      "  7.92435404e-02 9.99990240e-01 9.99356088e-01 9.99953180e-01\n",
      "  9.99983852e-01 6.46304062e-04]]\n",
      "[[7.73435686e-07 1.34953809e-05 9.99999826e-01 3.85660043e-01\n",
      "  9.99999859e-01 9.99988244e-01 9.99998258e-01 3.39440561e-04\n",
      "  1.24310778e-08 9.99999868e-01]]\n",
      "[[9.99999946e-01 9.99994281e-01 1.62843273e-05 2.75604069e-07\n",
      "  9.99999958e-01 9.99999496e-01 5.84602503e-01 1.24937571e-05\n",
      "  9.99522063e-01 4.86332316e-09]]\n",
      "[[9.99995888e-01 3.54942946e-04 7.67488646e-01 1.03997601e-02\n",
      "  9.99988273e-01 4.50406756e-01 9.99981393e-01 9.70171433e-02\n",
      "  2.29847924e-06 9.99965154e-01]]\n",
      "[[9.99997151e-01 3.30003831e-01 7.80311317e-01 8.87253188e-02\n",
      "  9.99980472e-01 1.18921877e-04 3.32077068e-01 9.99304893e-01\n",
      "  7.01521551e-05 9.99964568e-01]]\n",
      "[[3.60802731e-04 3.31917974e-05 8.18665571e-09 1.33072880e-11\n",
      "  1.83628977e-05 9.99999978e-01 9.99999999e-01 1.29043588e-03\n",
      "  9.99999952e-01 1.00000000e+00]]\n",
      "[[1.11140480e-01 9.99912550e-01 2.20602981e-04 2.64718922e-07\n",
      "  4.72145358e-03 4.86944366e-04 7.69830065e-02 8.71837233e-02\n",
      "  9.99872021e-01 9.99989148e-01]]\n",
      "[[9.99988366e-01 3.15408723e-07 1.12593185e-04 6.31736243e-01\n",
      "  9.99993075e-01 9.99995052e-01 9.99803868e-01 9.98951120e-01\n",
      "  9.99992209e-01 3.51155180e-02]]\n",
      "[[5.87849650e-04 9.99972405e-01 1.01527927e-04 8.38145463e-01\n",
      "  2.14380419e-05 9.99994103e-01 9.99997344e-01 9.99978998e-01\n",
      "  5.35295614e-02 7.68755316e-06]]\n",
      "[[5.34528461e-04 9.86592050e-01 8.96812352e-02 9.91163126e-04\n",
      "  8.52299551e-02 9.98910797e-01 9.92613187e-01 9.91953515e-01\n",
      "  9.84085549e-01 8.90631889e-05]]\n",
      "[[9.96443863e-03 9.99992850e-01 7.32907679e-03 9.99974205e-01\n",
      "  9.99995233e-01 9.99972034e-01 2.27900500e-07 9.99961705e-01\n",
      "  9.97999629e-01 9.99908797e-01]]\n",
      "[[0.13027807 0.11792619 0.17481375 0.04925448 0.97200499 0.96098688\n",
      "  0.92750902 0.01385309 0.01182883 0.99929072]]\n",
      "[[9.99678581e-01 1.05086954e-03 5.56920082e-04 9.99933951e-01\n",
      "  9.99911604e-01 5.06698711e-04 1.71102211e-03 9.99935174e-01\n",
      "  9.99686766e-01 9.78271735e-01]]\n",
      "[[6.09127462e-05 6.54203916e-01 9.95840360e-01 9.99976373e-01\n",
      "  9.99978949e-01 7.80338423e-02 9.76428634e-01 3.33468809e-05\n",
      "  3.31854822e-04 9.99967202e-01]]\n",
      "[[9.99923752e-01 9.99979970e-01 9.99846329e-01 8.41044274e-07\n",
      "  5.67187274e-04 9.99692534e-01 9.99091114e-01 9.99411271e-01\n",
      "  9.33951981e-01 9.99772042e-01]]\n",
      "[[9.99995996e-01 1.34281983e-06 2.66257069e-05 9.99991989e-01\n",
      "  4.85070413e-04 9.99993002e-01 9.79304022e-01 8.86414948e-01\n",
      "  2.51860497e-01 9.99109302e-01]]\n",
      "[[9.99977588e-01 2.13330287e-04 2.61053832e-07 3.48654353e-06\n",
      "  5.64943218e-08 2.96294135e-07 9.99999746e-01 2.40812142e-07\n",
      "  9.19672764e-03 9.99990294e-01]]\n",
      "[[9.99984608e-01 2.07208616e-01 9.99989360e-01 2.08837686e-01\n",
      "  9.99096319e-01 4.08637540e-04 9.99991109e-01 1.21388575e-01\n",
      "  9.99237433e-01 1.14363780e-04]]\n",
      "[[9.98847348e-01 5.12585675e-01 9.99955329e-01 3.01996609e-02\n",
      "  5.66572537e-05 2.29363708e-02 9.99157684e-01 9.99810905e-01\n",
      "  9.99947053e-01 8.70847178e-04]]\n",
      "[[9.66773295e-01 9.95392945e-01 9.98262104e-01 8.19424515e-01\n",
      "  1.66493861e-04 9.98083314e-01 8.55640807e-01 9.95219884e-01\n",
      "  4.89610288e-02 9.39945783e-01]]\n",
      "[[9.99516738e-01 9.99863315e-01 9.98389275e-01 9.99653253e-01\n",
      "  9.99816529e-01 7.21297342e-02 3.38305397e-04 1.31886727e-03\n",
      "  8.43814258e-03 3.69558278e-02]]\n",
      "[[7.42702404e-08 9.96881524e-01 9.99999982e-01 9.99997563e-01\n",
      "  4.53529650e-05 1.73524585e-06 4.61530258e-06 7.49631234e-01\n",
      "  9.99999926e-01 9.99999804e-01]]\n",
      "[[0.99997191 0.99998524 0.99994373 0.0518759  0.21427857 0.92256151\n",
      "  0.99992915 0.00291306 0.00284755 0.99987284]]\n",
      "[[1.26167645e-03 9.99987888e-01 9.99698000e-01 9.99947107e-01\n",
      "  1.53763783e-04 9.99954851e-01 9.99796952e-01 6.72399269e-01\n",
      "  9.99788252e-01 9.99950663e-01]]\n",
      "[[0.00131221 0.9958185  0.01116673 0.99970441 0.00195303 0.03735791\n",
      "  0.19423235 0.23977622 0.00142597 0.94844928]]\n",
      "[[9.97248471e-01 9.99997110e-01 3.23707733e-03 1.86527181e-09\n",
      "  6.37448447e-02 8.87428949e-01 9.39931186e-01 9.99980138e-01\n",
      "  4.62982049e-02 9.99997817e-01]]\n",
      "[[3.64289638e-01 9.49050873e-01 1.47491826e-04 4.41987925e-03\n",
      "  5.84826594e-03 1.47423894e-01 9.14356099e-04 9.97628850e-01\n",
      "  8.69556564e-05 9.89845916e-01]]\n",
      "[[8.73937020e-01 3.14350300e-04 3.56410888e-05 1.29602887e-01\n",
      "  9.96868547e-01 1.38731474e-01 9.92970989e-01 9.97934790e-01\n",
      "  8.75664307e-01 5.16494403e-04]]\n",
      "[[9.99974560e-01 5.44914930e-04 9.99971056e-01 9.99540314e-01\n",
      "  6.26893583e-03 5.85047656e-04 3.41804559e-01 7.58489762e-03\n",
      "  9.99669733e-01 8.98797265e-02]]\n",
      "[[9.99999229e-01 2.68260955e-05 9.99854451e-01 9.99999439e-01\n",
      "  2.60219797e-04 8.04197559e-06 9.05737892e-05 4.44900107e-07\n",
      "  1.31251310e-04 1.29471005e-03]]\n",
      "[[9.99986726e-01 4.03408195e-05 1.79707205e-01 9.99945669e-01\n",
      "  9.99982321e-01 9.21026089e-01 9.99944210e-01 4.74498474e-03\n",
      "  3.53638823e-02 9.81987500e-01]]\n",
      "[[6.06823685e-06 9.50900305e-01 3.62222449e-06 6.37885994e-03\n",
      "  6.57994233e-05 2.40582524e-05 9.99999422e-01 9.99984853e-01\n",
      "  9.97435999e-01 2.75930050e-01]]\n",
      "[[0.00131069 0.99569122 0.00459134 0.34625985 0.96848594 0.58423526\n",
      "  0.96058995 0.24550626 0.93984119 0.69415559]]\n",
      "[[9.37375782e-01 4.27920066e-01 9.77565868e-01 9.91404502e-01\n",
      "  3.49719891e-04 7.25848366e-01 9.66324811e-01 9.34954538e-02\n",
      "  9.94792937e-01 2.61455377e-03]]\n",
      "[[0.01176719 0.99926968 0.97945744 0.02726764 0.97665466 0.0090254\n",
      "  0.99828912 0.76545797 0.99690237 0.00941512]]\n",
      "[[0.99955048 0.99065076 0.01482264 0.99507877 0.47171018 0.00310372\n",
      "  0.99910448 0.08168322 0.99940045 0.99860431]]\n",
      "[[9.53904777e-03 9.99971913e-01 9.98342990e-01 3.43036574e-06\n",
      "  4.46826505e-02 9.99999819e-01 9.99797993e-01 3.67382522e-06\n",
      "  2.19236835e-06 9.99999607e-01]]\n",
      "[[9.99993397e-01 3.82713524e-03 9.99957417e-01 9.99990362e-01\n",
      "  9.99969439e-01 9.68432115e-01 8.00334848e-03 9.98580922e-01\n",
      "  3.18383059e-04 9.49365499e-01]]\n",
      "[[0.99980068 0.99143859 0.99714343 0.00197103 0.00281675 0.02297822\n",
      "  0.0153916  0.99884962 0.99902221 0.00397239]]\n",
      "[[9.99996177e-01 1.17440077e-01 2.84335463e-06 8.30302989e-05\n",
      "  9.99982329e-01 5.16897642e-01 4.29001128e-03 9.99974379e-01\n",
      "  1.23035431e-01 4.18823417e-05]]\n",
      "[[9.00161632e-01 9.99351117e-01 9.99896044e-01 9.98878319e-01\n",
      "  9.73622844e-01 9.98509324e-01 9.99849673e-01 9.99931776e-01\n",
      "  1.22865955e-04 7.01577551e-04]]\n",
      "[[5.83878818e-05 9.99999996e-01 3.64019471e-07 9.99999855e-01\n",
      "  1.16931617e-05 1.16534482e-05 9.99999530e-01 9.99999961e-01\n",
      "  9.99998740e-01 2.43675537e-10]]\n",
      "[[9.96407586e-01 9.99828476e-01 9.94677318e-01 4.53400202e-07\n",
      "  4.59236532e-04 9.46202562e-01 9.99862483e-01 9.99397271e-01\n",
      "  9.99788302e-01 8.72636946e-01]]\n",
      "[[1.15747931e-08 3.57823196e-05 9.99996832e-01 9.99862847e-01\n",
      "  4.17976511e-07 9.99999023e-01 9.99977674e-01 7.80928803e-03\n",
      "  1.99287619e-04 9.99999937e-01]]\n",
      "[[9.99988481e-01 3.07076160e-01 9.99973077e-01 9.99997857e-01\n",
      "  4.89127104e-05 9.99993787e-01 9.99988391e-01 8.88393959e-01\n",
      "  1.78660692e-03 5.00860535e-02]]\n",
      "[[9.99793942e-01 9.99939342e-01 9.99970044e-01 1.29587471e-04\n",
      "  9.99954268e-01 8.65142211e-01 9.89780908e-01 5.06047402e-04\n",
      "  1.55845167e-05 9.99950914e-01]]\n",
      "[[1.10810425e-10 9.71019275e-01 9.99999982e-01 1.01868640e-01\n",
      "  9.99999973e-01 3.43003802e-05 9.99999943e-01 9.99999905e-01\n",
      "  9.99538535e-01 9.99999984e-01]]\n",
      "[[9.99990044e-01 2.65528499e-05 9.99979629e-01 2.29521061e-06\n",
      "  9.99994314e-01 8.04726953e-03 9.99986099e-01 6.68724329e-03\n",
      "  9.69440356e-01 4.49553937e-01]]\n",
      "[[9.99932338e-01 6.51431167e-03 3.59224474e-04 9.99984831e-01\n",
      "  9.99997782e-01 9.99997203e-01 1.31982494e-07 9.99999213e-01\n",
      "  9.99998560e-01 1.93150271e-06]]\n",
      "[[9.99995699e-01 9.99998829e-01 5.85532118e-06 1.16274518e-04\n",
      "  9.99988032e-01 9.99996420e-01 9.99957312e-01 9.97838097e-01\n",
      "  1.61885851e-02 9.99997978e-01]]\n",
      "[[9.99986227e-01 9.93434652e-01 5.82426389e-05 9.96941281e-02\n",
      "  9.99999869e-01 1.06133567e-03 9.99989222e-01 4.41378774e-07\n",
      "  9.99902039e-01 9.99990573e-01]]\n",
      "[[9.99999216e-01 1.70095880e-08 3.14627360e-03 9.99998227e-01\n",
      "  9.99943095e-01 9.99998672e-01 9.99997916e-01 9.99998900e-01\n",
      "  9.99999317e-01 9.99862770e-01]]\n",
      "[[9.99993957e-01 5.41283371e-01 3.39738468e-04 9.96115956e-03\n",
      "  9.99991644e-01 8.82788963e-04 1.24088511e-03 5.46278210e-02\n",
      "  7.72728839e-01 9.99994984e-01]]\n",
      "[[9.92231741e-01 6.44697322e-01 5.76411874e-03 9.85143860e-01\n",
      "  9.90221502e-01 9.27179763e-03 8.60460476e-04 4.49521142e-01\n",
      "  9.95458945e-01 1.87171164e-02]]\n",
      "[[9.99902605e-01 4.80699321e-01 9.07380570e-05 9.99982595e-01\n",
      "  9.99498415e-01 9.99970444e-01 9.99903213e-01 3.60220039e-02\n",
      "  1.91519693e-01 5.93792105e-04]]\n",
      "[[8.75679887e-05 6.67506752e-01 9.99706690e-01 3.32021054e-03\n",
      "  9.51363591e-01 2.92578828e-01 9.99928408e-01 5.80044409e-05\n",
      "  4.34268224e-02 8.17468853e-01]]\n",
      "[[9.99971805e-01 9.98456604e-01 1.07875139e-03 1.14492997e-01\n",
      "  7.39428903e-01 9.46385884e-01 1.64266549e-04 9.99872345e-01\n",
      "  3.66145551e-03 9.99777845e-01]]\n",
      "[[2.66720113e-01 7.77712445e-01 9.99994601e-01 3.68192808e-05\n",
      "  7.17438380e-01 1.71885779e-03 9.99997317e-01 9.99998728e-01\n",
      "  9.99999531e-01 2.26939799e-04]]\n",
      "[[9.99999585e-01 1.17418297e-08 8.24759883e-04 9.99999925e-01\n",
      "  3.93855431e-08 9.99999946e-01 6.77099693e-01 9.99999797e-01\n",
      "  9.66052450e-02 5.44789747e-03]]\n",
      "[[2.30187714e-02 9.99974395e-01 2.00387198e-04 1.16675294e-03\n",
      "  9.99933520e-01 9.99234266e-01 9.99198828e-01 2.11065307e-04\n",
      "  9.99721870e-01 1.45473411e-01]]\n",
      "[[9.98603632e-01 9.99946706e-01 3.82330747e-03 9.30093239e-01\n",
      "  9.99756546e-01 9.99232053e-01 3.95330133e-02 9.37032806e-01\n",
      "  9.99843892e-04 3.79764787e-05]]\n",
      "[[1.26762407e-01 9.98902206e-01 2.64390912e-01 9.97272250e-01\n",
      "  8.55074733e-04 9.98584444e-01 6.23751113e-02 1.83320393e-03\n",
      "  9.97276636e-01 5.12532896e-02]]\n",
      "[[9.99927215e-01 9.99401843e-01 9.97673380e-01 9.98944575e-01\n",
      "  1.40657386e-05 9.98698581e-01 1.67525995e-01 5.79485276e-02\n",
      "  1.13666952e-04 9.99819737e-01]]\n",
      "[[1.86063588e-09 9.99999906e-01 9.99999982e-01 9.99999995e-01\n",
      "  2.83601262e-03 9.99999980e-01 5.27939444e-05 9.99999994e-01\n",
      "  9.99999700e-01 9.99999993e-01]]\n",
      "[[1.76468082e-05 2.20319107e-01 7.51691275e-01 9.99864005e-01\n",
      "  9.99873534e-01 1.20088561e-01 5.15345198e-04 9.99817641e-01\n",
      "  2.42413286e-03 8.28326548e-01]]\n",
      "[[9.98227281e-01 2.99910218e-09 3.75326349e-04 6.87754728e-03\n",
      "  9.99994463e-01 3.56064007e-04 9.99997825e-01 9.99655920e-01\n",
      "  1.17001169e-03 1.87231882e-02]]\n",
      "[[9.99992904e-01 1.01946518e-01 4.01380204e-04 3.54821373e-03\n",
      "  9.99927711e-01 9.99985648e-01 9.99952940e-01 9.99992508e-01\n",
      "  1.10748014e-04 4.82199931e-05]]\n",
      "[[0.78529094 0.83738316 0.97748946 0.91997893 0.99635854 0.99993868\n",
      "  0.99964794 0.00607493 0.00337297 0.99995007]]\n",
      "[[7.13082377e-03 1.54290359e-04 7.74135280e-01 3.24922269e-02\n",
      "  9.34863373e-01 9.96773163e-01 1.18916439e-03 5.53052393e-01\n",
      "  9.95921591e-01 1.79284688e-02]]\n",
      "[[9.93956082e-01 3.79088384e-04 9.99874964e-01 9.99904899e-01\n",
      "  2.78230259e-03 9.17062676e-01 1.95830215e-03 9.77084477e-01\n",
      "  9.99722228e-01 9.57740595e-01]]\n",
      "[[3.31585386e-01 9.86153111e-01 2.90232442e-02 9.91429353e-01\n",
      "  9.97490392e-01 9.92935428e-01 6.50109322e-02 1.61680510e-04\n",
      "  7.58661129e-01 9.08951438e-01]]\n",
      "[[9.99981000e-01 4.36529405e-04 5.15261747e-05 9.99999955e-01\n",
      "  1.15072181e-01 1.20422049e-07 9.99999934e-01 9.99999809e-01\n",
      "  7.62551171e-03 9.99999195e-01]]\n",
      "[[1.06986510e-05 1.14874846e-06 9.99975628e-01 9.99972452e-01\n",
      "  2.89059614e-06 1.61224679e-01 9.81135301e-01 9.99993158e-01\n",
      "  9.99989177e-01 9.99982087e-01]]\n",
      "[0.9, 1.0, 1.0, 0.7, 1.0, 1.0, 0.9, 1.0, 0.9, 1.0, 0.9, 0.9, 1.0, 0.8, 0.8, 0.9, 1.0, 0.8, 1.0, 0.7, 1.0, 0.9, 1.0, 0.7, 0.6, 1.0, 0.9, 0.9, 0.9, 0.9, 0.8, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.7, 0.8, 1.0, 0.9, 1.0, 0.8, 0.8, 0.7, 1.0, 1.0, 0.9, 0.9, 0.9, 0.8, 0.9, 0.9, 0.8, 0.7, 0.9, 0.8, 1.0, 0.9, 1.0, 0.9, 1.0, 0.8, 1.0, 0.9, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 0.7, 1.0, 1.0, 0.8, 0.9, 1.0, 1.0, 0.8, 0.9, 1.0, 0.9, 1.0, 0.8, 0.6, 0.9, 0.9, 0.9, 1.0, 0.8, 0.9, 0.9, 1.0, 1.0, 1.0, 0.8, 0.9, 0.9, 1.0, 1.0, 0.9, 0.8, 1.0, 1.0, 0.9, 1.0, 0.9, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 0.9, 0.9, 0.9, 1.0, 1.0, 0.8, 0.9, 0.9, 1.0, 1.0, 0.9, 0.9, 0.8, 1.0, 1.0, 0.9, 0.8, 1.0, 0.7, 1.0, 1.0, 0.9, 0.9, 1.0, 0.7, 0.9, 0.9, 0.9, 0.8, 0.9, 0.8, 1.0, 0.9, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.8, 1.0, 0.9, 1.0, 1.0, 0.8, 1.0, 0.9, 0.9, 0.9, 0.9, 1.0, 1.0, 0.9, 0.9, 0.8, 0.8, 1.0, 0.7, 0.7, 0.8, 0.9, 1.0, 0.9, 0.9, 0.8, 1.0, 0.7, 1.0, 1.0, 0.7, 0.9, 1.0, 0.8, 1.0, 1.0]\n",
      "Average accuracy: 0.909\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n = 0\n",
    "m = batch_size\n",
    "accuracies = []\n",
    "#print(y_test.values.shape[0]//m)\n",
    "for i in range(y_test.values.shape[0]//m):\n",
    "    X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "    y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "    predictions = model.predict(X, y)\n",
    "    #print(predictions.type)\n",
    "    #print(y[:,:100])\n",
    "    print(predictions[:,:100])\n",
    "    score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "    #print(score)\n",
    "    accuracies.append(score)\n",
    "    \n",
    "    n = m\n",
    "    m += batch_size\n",
    "    #print(m,n)\n",
    "\n",
    "print(accuracies)\n",
    "print(\"Average accuracy:\", np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03876595 0.94995055 0.74143983 ... 0.98458477 0.98108052 0.77276491]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8211666666666667"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "m = 12000\n",
    "X = X_train[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "y = y_train[n:m].values.reshape(1,m-n).astype(float)\n",
    "predictions = model.predict(X, y)\n",
    "#print(predictions.type)\n",
    "#print(y)\n",
    "print(predictions)\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 in epoch 0 - Cost: 0.69144499\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 1 - Cost: 0.68998352\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 2 - Cost: 0.68979612\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 3 - Cost: 0.68706695\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 4 - Cost: 0.68553497\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 5 - Cost: 0.68555434\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 6 - Cost: 0.68278765\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 7 - Cost: 0.68330999\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 8 - Cost: 0.68118162\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 4 in epoch 9 - Cost: 0.68022033\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Execution Time: 0.9606995582580566\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "#np.random.seed(10)\n",
    "m = 160\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "    Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "    #Conv2D(32, 3, (None, 8, 8, 32)),\n",
    "    Flatten((None, 8, 8, 32)),\n",
    "    Dense(32, (2048, None), \"relu\"),\n",
    "    Dense(1, (32, None), \"sigmoid\")\n",
    "]\n",
    "\n",
    "#     k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "#     k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "#     k.layers.MaxPooling2D((3,3)),\n",
    "#     k.layers.Flatten(),\n",
    "#     k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "#     k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "start_time = time.time()\n",
    "history = model.fit(X, y, epochs=10, learning_rate=0.0005, verbose=1, batch_size=32)\n",
    "end_time = time.time()\n",
    "print(\"Execution Time:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50767553 0.50726451 0.50433937 0.50417584 0.48556223 0.50759367\n",
      "  0.49380459 0.49369202 0.50906104 0.50509067 0.49452294 0.50864094\n",
      "  0.50523759 0.49761844 0.50672254 0.5039751  0.48845146 0.48566275\n",
      "  0.50731911 0.49454783 0.50268147 0.48468444 0.487439   0.48528741\n",
      "  0.48575077 0.50497878 0.50905081 0.50570598 0.5098411  0.50679853\n",
      "  0.48419234 0.49467045 0.4844502  0.50608318 0.5046738  0.50471515\n",
      "  0.48756663 0.49529079 0.49050552 0.50802571 0.49603712 0.48510718\n",
      "  0.50704946 0.48686413 0.493576   0.50292618 0.5075331  0.50429494\n",
      "  0.49183003 0.492618   0.50532188 0.50667749 0.49840631 0.48825203\n",
      "  0.48990474 0.4860423  0.50388053 0.48621927 0.4863515  0.49181023\n",
      "  0.50597512 0.48879419 0.48382139 0.48701753 0.50356677 0.49445742\n",
      "  0.50345005 0.4823904  0.48827532 0.50667005 0.50508459 0.48784151\n",
      "  0.50303933 0.4911853  0.50763473 0.5060124  0.50410227 0.48911704\n",
      "  0.48078854 0.5078441  0.4984369  0.4921302  0.48595497 0.50752724\n",
      "  0.48959561 0.50737889 0.48886398 0.48824205 0.50374687 0.48828252\n",
      "  0.50369737 0.50557391 0.50922574 0.48705608 0.50750005 0.48757622\n",
      "  0.50524228 0.49026063 0.4899567  0.49402106]]\n",
      "[0.9455]\n",
      "Average accuracy: 0.9455\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2000\n",
    "n = 0\n",
    "m = batch_size\n",
    "accuracies = []\n",
    "#print(y_test.values.shape[0]//m)\n",
    "for i in range(y_test.values.shape[0]//m):\n",
    "    X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "    y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "    predictions = model.predict(X, y)\n",
    "    #print(predictions.type)\n",
    "    #print(y[:,:100])\n",
    "    print(predictions[:,:100])\n",
    "    score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "    #print(score)\n",
    "    accuracies.append(score)\n",
    "    \n",
    "    n = m\n",
    "    m += batch_size\n",
    "    #print(m,n)\n",
    "\n",
    "print(accuracies)\n",
    "print(\"Average accuracy:\", np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00105568 0.76179028 0.30141488 ... 0.99516018 0.99575939 0.0893973 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9659166666666666"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "m = 12000\n",
    "X = X_train[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "y = y_train[n:m].values.reshape(1,m-n).astype(float)\n",
    "predictions = model.predict(X, y)\n",
    "#print(predictions.type)\n",
    "#print(y)\n",
    "print(predictions)\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loops...\n",
      "1 6\n",
      "Done W's\n",
      "Done B's\n",
      "2 6\n",
      "3 6\n",
      "Done W's\n",
      "Done B's\n",
      "4 6\n",
      "5 6\n",
      "6 6\n",
      "==================================================\n",
      "0.7606332886651564\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true grads</th>\n",
       "      <th>approx grads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.013448</td>\n",
       "      <td>0.052668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001167</td>\n",
       "      <td>-0.005450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008330</td>\n",
       "      <td>-0.024818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006284</td>\n",
       "      <td>-0.087154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.026957</td>\n",
       "      <td>-0.009739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21547</th>\n",
       "      <td>0.008351</td>\n",
       "      <td>-0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21548</th>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21549</th>\n",
       "      <td>-0.005742</td>\n",
       "      <td>-0.001199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>-0.010412</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21552 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       true grads  approx grads\n",
       "0       -0.013448      0.052668\n",
       "1       -0.001167     -0.005450\n",
       "2        0.008330     -0.024818\n",
       "3       -0.006284     -0.087154\n",
       "4       -0.026957     -0.009739\n",
       "...           ...           ...\n",
       "21547    0.008351     -0.000017\n",
       "21548    0.006610     -0.000027\n",
       "21549   -0.005742     -0.001199\n",
       "21550   -0.010412     -0.000043\n",
       "21551    0.006235      0.000099\n",
       "\n",
       "[21552 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "np.random.seed(10)\n",
    "m = 25\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(16, 7, (None, 28, 28, 1)),\n",
    "    #ReLU((None, 22, 22, 16)),\n",
    "    Maxpool((None, 22, 22, 16)),\n",
    "    Conv2D(16, 9, (None, 11, 11, 16)),\n",
    "    #ReLU((None, 8, 8, 32)),\n",
    "    Maxpool((None, 3, 3, 16)),\n",
    "    Flatten((None, 1, 1, 16)),\n",
    "    #Dense(32, (5184, None), \"relu\"),\n",
    "    #Dense(1, (32, None), \"sigmoid\")\n",
    "    knn_differentiable((64, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "print(model.gradcheck(X,y, epsilon=1e-7))\n",
    "pd.DataFrame(np.column_stack((model.true_grads, model.approx_grads)), columns=['true grads', 'approx grads'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
