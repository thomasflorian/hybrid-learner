{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027, 300, 300, 3) (1027,)\n",
      "(256, 300, 300, 3) (256,)\n"
     ]
    }
   ],
   "source": [
    "# Load Horses or Humans dataset\n",
    "X_train, y_train = tfds.as_numpy(tfds.load(\n",
    "    'horses_or_humans',\n",
    "    split='train',\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "X_test, y_test = tfds.as_numpy(tfds.load(\n",
    "    'horses_or_humans',\n",
    "    split='test',\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "MNIST = k.datasets.fashion_mnist.load_data()\n",
    "#MNIST = k.datasets.mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 0) | (y_train == 1)] \n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 0) | (y_test == 1)]\n",
    "X_test = X_test[y_test.index]\n",
    "\n",
    "# y_train[y_train==2] = 0\n",
    "# y_test[y_test==2] = 0\n",
    "# X_train[X_train==2] = 0\n",
    "# X_test[X_test==2] = 0\n",
    "\n",
    "# y_train[y_train==4] = 1\n",
    "# y_test[y_test==4] = 1\n",
    "# X_train[X_train==4] = 1\n",
    "# X_test[X_test==4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 65,921\n",
      "Trainable params: 65,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = \"random_normal\" # random_normal or glorot_uniform\n",
    "keras_model = k.Sequential([ \n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.MaxPooling2D((3,3)),\n",
    "    #k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2207\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0837\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0636\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0535\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0479\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0435\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0404\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0387\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0370\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0354\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 12000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dade\\AppData\\Local\\Temp/ipykernel_22896/2783948207.py:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9835"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 2000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = tf.raw_ops.Sigmoid(x=Z).numpy()\n",
    "    cache = A\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = tf.raw_ops.Relu(features=Z).numpy()\n",
    "    \n",
    "    cache = A\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = tf.raw_ops.ReluGrad(gradients=dA, features=Z).numpy()\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    A = cache\n",
    "    dZ = tf.raw_ops.SigmoidGrad(y=A, dy=dA).numpy()\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple, trainable=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.trainable = trainable\n",
    "        self.name = self.__class__.__name__ \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name + \" \" + str(self.output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (neurons, input_shape[1])\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.05\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "        self.dW = np.dot(dZ, A_prev.T)/m\n",
    "        self.db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "    \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.01\n",
    "        self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform convolution\n",
    "        Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "        # Add bias\n",
    "        Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z.numpy()\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "        self.dZ = dZ\n",
    "        self.A_prev = A_prev\n",
    "        dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()/m\n",
    "        self.db = np.average(np.sum(dZ, axis=(1,2)), axis=0)\n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "    \n",
    "    \n",
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.ksize = [1, pool_size, pool_size, 1]\n",
    "        self.strides = [1, pool_size, pool_size, 1]\n",
    "        output_shape = (input_shape[0], input_shape[1]//pool_size, input_shape[2]//pool_size, input_shape[3])\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        Z = tf.raw_ops.MaxPool(input=A_prev, ksize=self.ksize, strides=self.strides, data_format='NHWC', padding=\"VALID\").numpy()\n",
    "        self.cache = (A_prev, Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev, Z = self.cache\n",
    "        dA_prev = tf.raw_ops.MaxPoolGrad(orig_input=A_prev, orig_output=Z, grad=dZ, ksize=self.ksize, strides=self.strides, padding=\"VALID\", data_format='NHWC').numpy()\n",
    "        return dA_prev\n",
    "\n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "      \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = tf.raw_ops.Relu(features=Z).numpy()\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = tf.raw_ops.ReluGrad(gradients=dA, features=Z).numpy()\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (m, np.prod(shape))\n",
    "        super().__init__(input_shape, output_shape, trainable=False)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.reshape(m, np.prod(shape))\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.reshape(self.cache)\n",
    "    \n",
    "\n",
    "# class Flatten(Layer):\n",
    "#     def __init__(self, input_shape):\n",
    "#         m, *shape = input_shape\n",
    "#         output_shape = (np.prod(shape), m)\n",
    "#         super().__init__(input_shape, output_shape, trainable=False)\n",
    "        \n",
    "#     def forward(self, A_prev):\n",
    "#         m, *shape = A_prev.shape\n",
    "#         self.cache = A_prev.shape\n",
    "#         return A_prev.flatten().reshape(m,np.prod(shape)).T\n",
    "    \n",
    "#     def backward(self, dZ):\n",
    "#         return dZ.T.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, batch_size, verbose): \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        m = Y.shape[1]\n",
    "        chunks = np.linspace(0,m+1,m//batch_size+1).astype(int)\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(chunks)-1):\n",
    "                # FORWARD PROP\n",
    "                Z = X[chunks[i]:chunks[i+1]]\n",
    "                y = Y[:,chunks[i]:chunks[i+1]]\n",
    "                for layer in self.layers:\n",
    "                    Z = layer.forward(Z, y) if layer.name == \"knn_differentiable\" else layer.forward(Z)\n",
    "                pred, cost, dA = Z\n",
    "                # COST FUNCTION\n",
    "#                 cost = compute_cost(Z, y)\n",
    "#                 history.append(cost)\n",
    "                if verbose == 1 and i == 0:\n",
    "                    print(\"Cost epoch \", epoch, \": \", cost, sep=\"\")\n",
    "                \n",
    "                # BACKWARD PROP\n",
    "                for layer in reversed(self.layers):\n",
    "                    dA = layer.backward(dA)\n",
    "\n",
    "                # UPDATE PARAMS\n",
    "                for layer in self.layers:\n",
    "                    if layer.trainable:\n",
    "                        layer.update_params(learning_rate)\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, Z, Y):\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z, Y) if layer.name == \"knn_differentiable\" else layer.forward(Z)\n",
    "        return Z[0]\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"-\"*25)\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    def cost(self, Z, Y): \n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z, Y) if layer.name == \"knn_differentiable\" else layer.forward(Z)\n",
    "        # COMPUTE COST\n",
    "        return Z[1]\n",
    "    \n",
    "    def gradcheck(self, X, Y, epsilon=1e-7, start=None, end=None):\n",
    "        self.approx_grads = []\n",
    "        self.true_grads = []\n",
    "        for layer in self.layers[start:end]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            for i in range(layer.W.size):\n",
    "                i = np.unravel_index(i, layer.W.shape)\n",
    "                Wi = layer.W[i]\n",
    "                layer.W[i] = Wi + epsilon\n",
    "                J1 = self.cost(X, Y)\n",
    "                layer.W[i] = Wi - epsilon\n",
    "                J2 = self.cost(X, Y)\n",
    "                layer.W[i] = Wi\n",
    "                self.approx_grads.append(-(J1-J2)/(2*epsilon))\n",
    "                \n",
    "            for i in range(layer.b.size):\n",
    "                i = np.unravel_index(i, layer.b.shape)\n",
    "                bi = layer.b[i]\n",
    "                layer.b[i] = bi + epsilon\n",
    "                J1 = self.cost(X, Y)\n",
    "                layer.b[i] = bi - epsilon\n",
    "                J2 = self.cost(X, Y)\n",
    "                layer.b[i] = bi\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "        \n",
    "        # FORWARD PROP\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z, Y) if layer.name == \"knn_differentiable\" else layer.forward(Z)\n",
    "        # BACKWARD PROP\n",
    "        pred, cost, dA = Z\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        \n",
    "        for layer in self.layers[start:end]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            self.true_grads = np.concatenate((self.true_grads, layer.dW.flatten(), layer.db.flatten()))\n",
    "        self.approx_grads = np.array(self.approx_grads)\n",
    "        self.true_grads = np.array(self.true_grads)\n",
    "        return np.sqrt(np.sum(np.square(self.true_grads-self.approx_grads)))/(np.sqrt(np.sum(np.square(self.true_grads)))+np.sqrt(np.sum(np.square(self.approx_grads))))\n",
    "    \n",
    "        \n",
    "    \n",
    "class knn_differentiable(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__(input_shape, num_classes, False)\n",
    "        \n",
    "    def forward(self, batch_features, batch_labels):\n",
    "        self.batch_features = tf.convert_to_tensor(batch_features, dtype=tf.float64)\n",
    "        self.batch_labels = tf.transpose(tf.convert_to_tensor(batch_labels, dtype=tf.float64))\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(self.batch_features)\n",
    "            m = self.batch_labels.shape[0]\n",
    "            norm = tf.reduce_sum(tf.square(self.batch_features), 1, keepdims=True)\n",
    "            self.D = (1/100)*tf.maximum(tf.transpose(norm) - 2*tf.matmul(self.batch_features, self.batch_features, False, True) + norm, 0.0)\n",
    "            #print(self.D)\n",
    "            exp = tf.math.exp(-self.D)\n",
    "            exp = tf.linalg.set_diag(exp, tf.zeros((m,), dtype=tf.dtypes.float64))\n",
    "            \n",
    "            p_ij = exp/tf.reduce_sum(exp, 1)\n",
    "            #print(tf.reduce_sum(p_ij,0))\n",
    "            #print(tf.reduce_sum(p_ij,1))\n",
    "            maxes = tf.argmax(p_ij, axis=1)\n",
    "            classes = tf.gather(self.batch_labels, maxes)\n",
    "            \n",
    "            bool_mtx = tf.cast(tf.math.logical_xor(tf.cast(self.batch_labels, tf.bool), tf.cast(tf.transpose(1-self.batch_labels), tf.bool)), tf.float64)\n",
    "            p_ij = tf.math.multiply(bool_mtx, p_ij)\n",
    "            p_i = tf.reduce_sum(p_ij, 1)\n",
    "            #print(p_i)\n",
    "            self.cost = 1-tf.reduce_sum(p_i)/m   \n",
    "\n",
    "            self.grad = tf.transpose(g.gradient(self.cost, self.batch_features)).numpy()\n",
    "        return (classes.numpy(), self.cost.numpy(), self.grad)\n",
    "            \n",
    "    def backward(self, dA):\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Code below for knn layer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Architecture used on class 0 and 1 for fashion\n",
    "#         Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "#         Maxpool((None, 26, 26, 32), pool_size=2),\n",
    "#         Conv2D(64, 3, (None, 13, 13, 32)),\n",
    "#         Maxpool((None, 11, 11, 64), pool_size=2),\n",
    "#         Flatten((None, 5, 5, 64)),\n",
    "#         knn_differentiable((1600, None), 2)\n",
    "\n",
    "\n",
    "# Layer architecture used on digits\n",
    "#         Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "#         Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "#         Flatten((None, 8, 8, 32)),\n",
    "#         knn_differentiable((2048, None), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 0: 0.22062698525032443\n",
      "Cost epoch 1: 0.22013950107829372\n",
      "Cost epoch 2: 0.21830102627435144\n",
      "Cost epoch 3: 0.215136109246578\n",
      "Cost epoch 4: 0.21084768708344015\n",
      "Cost epoch 5: 0.20554016554684484\n",
      "Cost epoch 6: 0.1995474139958482\n",
      "Cost epoch 7: 0.19301597192861064\n",
      "Cost epoch 8: 0.1860909941957687\n",
      "Cost epoch 9: 0.1790670195496481\n",
      "Cost epoch 10: 0.17207595126410602\n",
      "Cost epoch 11: 0.16532924083438494\n",
      "Cost epoch 12: 0.1589447861188944\n",
      "Cost epoch 13: 0.15291866252867603\n",
      "Cost epoch 14: 0.1473065923906458\n",
      "Cost epoch 15: 0.1421431210699251\n",
      "Cost epoch 16: 0.13739318094756026\n",
      "Cost epoch 17: 0.13303880338283403\n",
      "Cost epoch 18: 0.12904673734713312\n",
      "Cost epoch 19: 0.12538607411869107\n",
      "Execution Time: 2.794276237487793\n",
      "\n",
      "Accuracy 1 - 0.972\n",
      "\n",
      "Accuracy 2 - 0.967\n",
      "\n",
      "Accuracy 3 - 0.8996975806451613\n",
      "\n",
      "Accuracy 4 - 0.8786967418546366\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 1\n",
      "\n",
      "Cost epoch 0: 0.17803497499543952\n",
      "Cost epoch 1: 0.17560466614839254\n",
      "Cost epoch 2: 0.1722137775262942\n",
      "Cost epoch 3: 0.16793283906817824\n",
      "Cost epoch 4: 0.16300771349570198\n",
      "Cost epoch 5: 0.1576123407846366\n",
      "Cost epoch 6: 0.1519265656028469\n",
      "Cost epoch 7: 0.14617103506335405\n",
      "Cost epoch 8: 0.1404829101645526\n",
      "Cost epoch 9: 0.13501009002033704\n",
      "Cost epoch 10: 0.12986649397349292\n",
      "Cost epoch 11: 0.12499891541305042\n",
      "Cost epoch 12: 0.12045775043059903\n",
      "Cost epoch 13: 0.11624459765323425\n",
      "Cost epoch 14: 0.11236670417570727\n",
      "Cost epoch 15: 0.10877249082253981\n",
      "Cost epoch 16: 0.10550054869123038\n",
      "Cost epoch 17: 0.10253480535220316\n",
      "Cost epoch 18: 0.09982622671851216\n",
      "Cost epoch 19: 0.09739842739878535\n",
      "Execution Time: 2.7361865043640137\n",
      "\n",
      "Accuracy 1 - 0.9705\n",
      "\n",
      "Accuracy 2 - 0.964\n",
      "\n",
      "Accuracy 3 - 0.8946572580645161\n",
      "\n",
      "Accuracy 4 - 0.8756892230576441\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 2\n",
      "\n",
      "Cost epoch 0: 0.19355670140119263\n",
      "Cost epoch 1: 0.19315333928837264\n",
      "Cost epoch 2: 0.19209065165739536\n",
      "Cost epoch 3: 0.19037084625277712\n",
      "Cost epoch 4: 0.18806598003286545\n",
      "Cost epoch 5: 0.1852341703695426\n",
      "Cost epoch 6: 0.18195322046583895\n",
      "Cost epoch 7: 0.17828663283110413\n",
      "Cost epoch 8: 0.17435865157954178\n",
      "Cost epoch 9: 0.17022865534574128\n",
      "Cost epoch 10: 0.16597316964239506\n",
      "Cost epoch 11: 0.16166590885774035\n",
      "Cost epoch 12: 0.15737515019777293\n",
      "Cost epoch 13: 0.15316030971786987\n",
      "Cost epoch 14: 0.14904012955802837\n",
      "Cost epoch 15: 0.14504818750810888\n",
      "Cost epoch 16: 0.14120247437739686\n",
      "Cost epoch 17: 0.13753829637862047\n",
      "Cost epoch 18: 0.13408049491654306\n",
      "Cost epoch 19: 0.13080368689055433\n",
      "Execution Time: 2.806643009185791\n",
      "\n",
      "Accuracy 1 - 0.977\n",
      "\n",
      "Accuracy 2 - 0.974\n",
      "\n",
      "Accuracy 3 - 0.9102822580645161\n",
      "\n",
      "Accuracy 4 - 0.8877192982456141\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 3\n",
      "\n",
      "Cost epoch 0: 0.24648560921742657\n",
      "Cost epoch 1: 0.24529935553148574\n",
      "Cost epoch 2: 0.24296218206312314\n",
      "Cost epoch 3: 0.23944580137598914\n",
      "Cost epoch 4: 0.23494243309991614\n",
      "Cost epoch 5: 0.2295172225582116\n",
      "Cost epoch 6: 0.22336541552019384\n",
      "Cost epoch 7: 0.21661267799652717\n",
      "Cost epoch 8: 0.20949264583333493\n",
      "Cost epoch 9: 0.2021435721253545\n",
      "Cost epoch 10: 0.1947018356423611\n",
      "Cost epoch 11: 0.1873534727558377\n",
      "Cost epoch 12: 0.18025946519529523\n",
      "Cost epoch 13: 0.17353400693300447\n",
      "Cost epoch 14: 0.16718237423738525\n",
      "Cost epoch 15: 0.16125774797546832\n",
      "Cost epoch 16: 0.15571998073083848\n",
      "Cost epoch 17: 0.15057862177134673\n",
      "Cost epoch 18: 0.14584020447263957\n",
      "Cost epoch 19: 0.1414908055551698\n",
      "Execution Time: 2.9259634017944336\n",
      "\n",
      "Accuracy 1 - 0.9695\n",
      "\n",
      "Accuracy 2 - 0.9664999999999999\n",
      "\n",
      "Accuracy 3 - 0.9032258064516129\n",
      "\n",
      "Accuracy 4 - 0.8802005012531329\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 4\n",
      "\n",
      "Cost epoch 0: 0.19618769322986473\n",
      "Cost epoch 1: 0.19707139765661053\n",
      "Cost epoch 2: 0.19757409751643684\n",
      "Cost epoch 3: 0.19767871856104913\n",
      "Cost epoch 4: 0.19739217539633858\n",
      "Cost epoch 5: 0.19667786335575577\n",
      "Cost epoch 6: 0.19554986073068148\n",
      "Cost epoch 7: 0.19395878329638572\n",
      "Cost epoch 8: 0.1919529122192306\n",
      "Cost epoch 9: 0.18956132445414386\n",
      "Cost epoch 10: 0.1868105678268288\n",
      "Cost epoch 11: 0.18369776322295728\n",
      "Cost epoch 12: 0.1802904069448361\n",
      "Cost epoch 13: 0.17663099859785447\n",
      "Cost epoch 14: 0.1727612051443682\n",
      "Cost epoch 15: 0.1687280897715061\n",
      "Cost epoch 16: 0.1645918199318963\n",
      "Cost epoch 17: 0.1603855123664245\n",
      "Cost epoch 18: 0.1561502400999728\n",
      "Cost epoch 19: 0.15193179591404016\n",
      "Execution Time: 2.9675042629241943\n",
      "\n",
      "Accuracy 1 - 0.972\n",
      "\n",
      "Accuracy 2 - 0.964\n",
      "\n",
      "Accuracy 3 - 0.9047379032258065\n",
      "\n",
      "Accuracy 4 - 0.8822055137844612\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 5\n",
      "\n",
      "Cost epoch 0: 0.1854704375497308\n",
      "Cost epoch 1: 0.1832261578161647\n",
      "Cost epoch 2: 0.18033714472884588\n",
      "Cost epoch 3: 0.1769233243268875\n",
      "Cost epoch 4: 0.173055457031408\n",
      "Cost epoch 5: 0.16879297018344896\n",
      "Cost epoch 6: 0.16429046733199282\n",
      "Cost epoch 7: 0.1596627509841464\n",
      "Cost epoch 8: 0.154945515337509\n",
      "Cost epoch 9: 0.15023669714709942\n",
      "Cost epoch 10: 0.14557940239796152\n",
      "Cost epoch 11: 0.14100923368617801\n",
      "Cost epoch 12: 0.13656974802216482\n",
      "Cost epoch 13: 0.13230216472918865\n",
      "Cost epoch 14: 0.1282089816157881\n",
      "Cost epoch 15: 0.12433242643501463\n",
      "Cost epoch 16: 0.12067793080567113\n",
      "Cost epoch 17: 0.1172434914517293\n",
      "Cost epoch 18: 0.11402324620482729\n",
      "Cost epoch 19: 0.11102249089372507\n",
      "Execution Time: 2.8671412467956543\n",
      "\n",
      "Accuracy 1 - 0.9685\n",
      "\n",
      "Accuracy 2 - 0.969\n",
      "\n",
      "Accuracy 3 - 0.8971774193548387\n",
      "\n",
      "Accuracy 4 - 0.8822055137844611\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 6\n",
      "\n",
      "Cost epoch 0: 0.21023663005788806\n",
      "Cost epoch 1: 0.20964616237964873\n",
      "Cost epoch 2: 0.2081353699540125\n",
      "Cost epoch 3: 0.20575440546803525\n",
      "Cost epoch 4: 0.2025897682748654\n",
      "Cost epoch 5: 0.19871641140834673\n",
      "Cost epoch 6: 0.19425513218691048\n",
      "Cost epoch 7: 0.18928530713676484\n",
      "Cost epoch 8: 0.18391434221115122\n",
      "Cost epoch 9: 0.17830611940149432\n",
      "Cost epoch 10: 0.1726110324206207\n",
      "Cost epoch 11: 0.16690967881051755\n",
      "Cost epoch 12: 0.16131951442913683\n",
      "Cost epoch 13: 0.15589476125437907\n",
      "Cost epoch 14: 0.15070236271875825\n",
      "Cost epoch 15: 0.145796224617592\n",
      "Cost epoch 16: 0.14118511292602332\n",
      "Cost epoch 17: 0.1368675593650468\n",
      "Cost epoch 18: 0.132827018879771\n",
      "Cost epoch 19: 0.12905512192122104\n",
      "Execution Time: 2.8538942337036133\n",
      "\n",
      "Accuracy 1 - 0.967\n",
      "\n",
      "Accuracy 2 - 0.9584999999999999\n",
      "\n",
      "Accuracy 3 - 0.8956653225806451\n",
      "\n",
      "Accuracy 4 - 0.8756892230576441\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 7\n",
      "\n",
      "Cost epoch 0: 0.20677398630768806\n",
      "Cost epoch 1: 0.20639457531509386\n",
      "Cost epoch 2: 0.2056461291111481\n",
      "Cost epoch 3: 0.20454165867155916\n",
      "Cost epoch 4: 0.20303800804270133\n",
      "Cost epoch 5: 0.20115011358883195\n",
      "Cost epoch 6: 0.19891966026483088\n",
      "Cost epoch 7: 0.19634816867904537\n",
      "Cost epoch 8: 0.19347884957495487\n",
      "Cost epoch 9: 0.19032124388040006\n",
      "Cost epoch 10: 0.1869173809678184\n",
      "Cost epoch 11: 0.18333213250843372\n",
      "Cost epoch 12: 0.1795954821541541\n",
      "Cost epoch 13: 0.17574520667972926\n",
      "Cost epoch 14: 0.17183788933103106\n",
      "Cost epoch 15: 0.16792077676319384\n",
      "Cost epoch 16: 0.16402762766161638\n",
      "Cost epoch 17: 0.16015663705027294\n",
      "Cost epoch 18: 0.15636081299925175\n",
      "Cost epoch 19: 0.15266033320554373\n",
      "Execution Time: 2.8522284030914307\n",
      "\n",
      "Accuracy 1 - 0.964\n",
      "\n",
      "Accuracy 2 - 0.957\n",
      "\n",
      "Accuracy 3 - 0.9017137096774194\n",
      "\n",
      "Accuracy 4 - 0.8786967418546366\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 8\n",
      "\n",
      "Cost epoch 0: 0.27087017829778026\n",
      "Cost epoch 1: 0.2666784686911814\n",
      "Cost epoch 2: 0.26070568861525345\n",
      "Cost epoch 3: 0.25326551113449547\n",
      "Cost epoch 4: 0.2446388799444148\n",
      "Cost epoch 5: 0.23511609366513686\n",
      "Cost epoch 6: 0.22504161521632526\n",
      "Cost epoch 7: 0.2147178368829863\n",
      "Cost epoch 8: 0.20449921423702855\n",
      "Cost epoch 9: 0.19460494465720846\n",
      "Cost epoch 10: 0.18522723779394268\n",
      "Cost epoch 11: 0.17650227940816354\n",
      "Cost epoch 12: 0.16847356569105487\n",
      "Cost epoch 13: 0.16114705705858878\n",
      "Cost epoch 14: 0.15450470607039435\n",
      "Cost epoch 15: 0.14846937028216622\n",
      "Cost epoch 16: 0.14300802616642616\n",
      "Cost epoch 17: 0.13805405482093358\n",
      "Cost epoch 18: 0.13355210704979936\n",
      "Cost epoch 19: 0.1294728895657946\n",
      "Execution Time: 2.9782233238220215\n",
      "\n",
      "Accuracy 1 - 0.9645\n",
      "\n",
      "Accuracy 2 - 0.9555\n",
      "\n",
      "Accuracy 3 - 0.8981854838709677\n",
      "\n",
      "Accuracy 4 - 0.8736842105263157\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 9\n",
      "\n",
      "Cost epoch 0: 0.2543682283122205\n",
      "Cost epoch 1: 0.2566141142148548\n",
      "Cost epoch 2: 0.25809326220202666\n",
      "Cost epoch 3: 0.2587628549609904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 4: 0.2585730968386948\n",
      "Cost epoch 5: 0.25749964719968754\n",
      "Cost epoch 6: 0.25559088305802047\n",
      "Cost epoch 7: 0.25289667979275654\n",
      "Cost epoch 8: 0.2494336272391965\n",
      "Cost epoch 9: 0.24520505154250527\n",
      "Cost epoch 10: 0.24032211295773787\n",
      "Cost epoch 11: 0.2347952845823592\n",
      "Cost epoch 12: 0.22875955912124046\n",
      "Cost epoch 13: 0.22233095945600057\n",
      "Cost epoch 14: 0.2156140279511657\n",
      "Cost epoch 15: 0.2087212753947314\n",
      "Cost epoch 16: 0.20175054137066462\n",
      "Cost epoch 17: 0.1948506855833967\n",
      "Cost epoch 18: 0.18805832032442882\n",
      "Cost epoch 19: 0.18148047799675648\n",
      "Execution Time: 2.7359862327575684\n",
      "\n",
      "Accuracy 1 - 0.9745\n",
      "\n",
      "Accuracy 2 - 0.9615\n",
      "\n",
      "Accuracy 3 - 0.9042338709677419\n",
      "\n",
      "Accuracy 4 - 0.881203007518797\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ end of loop 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average Execution Time: 2.8518046855926515   ---   Average Accuracies: [0.96995    0.9637     0.90095766 0.879599  ]\n"
     ]
    }
   ],
   "source": [
    "# Change below three parameters for experiments \n",
    "train_size = 160\n",
    "batch_size = 32\n",
    "learning = 0.01\n",
    "\n",
    "\n",
    "num_loops = 10\n",
    "times = []\n",
    "accuracy_lst = [0,0,0,0]\n",
    "for i in range(num_loops):\n",
    "    # Select only m samples\n",
    "    m = train_size\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(\"float64\")\n",
    "    y = y_train[:m].values.reshape(1,m).astype(\"float64\")\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 13, 13, 32)),\n",
    "        Maxpool((None, 11, 11, 64), pool_size=2),\n",
    "        Flatten((None, 5, 5, 64)),\n",
    "        knn_differentiable((1600, None), 2)\n",
    "    ]\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=learning, verbose=1, batch_size=batch_size)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Runs through each testing batch size for the table and gathers accuracies\n",
    "    batch_sizes = [2000, 1000, batch_size, 15]\n",
    "    for j in range(len(batch_sizes)):\n",
    "        m = batch_sizes[j]\n",
    "        n = 0\n",
    "        accuracies = []\n",
    "        for l in range(y_test.values.shape[0]//m):\n",
    "            X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "            y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "            predictions = model.predict(X, y)\n",
    "            \n",
    "            score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "            accuracies.append(score)\n",
    "\n",
    "            n = m\n",
    "            m += batch_sizes[j]\n",
    "        #print(predictions[:10])\n",
    "        avg_acc = np.average(accuracies)\n",
    "        accuracy_lst[j] += avg_acc\n",
    "        print(\"Accuracy\", j+1, \"-\", avg_acc)\n",
    "        print()\n",
    "    print(\"+\"*50, \"end of loop\", i+1)\n",
    "    print()\n",
    "\n",
    "# Output average execution time and average accuracies in a list of the same order as shown in tables\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracies:\", np.divide(accuracy_lst, num_loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22896/648635125.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Dade\\AppData\\Local\\Temp/ipykernel_22896/648635125.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    0.953      0.956      0.93548387 0.91378446 - 1 epoch, learning rate 0 to get baseline\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0.953      0.956      0.93548387 0.91378446 - 1 epoch, learning rate 0 to get baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_11796/956872343.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Dade\\AppData\\Local\\Temp/ipykernel_11796/956872343.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    0.948      0.9465     0.96068548 0.96491228\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0.962      0.954      0.9203629  0.90275689 - 10 epochs, learning rate 0.001   (end loss .2467)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.97       0.9615     0.90927419 0.89223058 - 20 epochs, learning rate 0.001   (end loss .1399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".977    .9703    .9228387    .884110276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horses and Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 0: 0.6740393804538951\n",
      "Cost epoch 1: 0.6201046191637205\n",
      "Cost epoch 2: 0.4919354838709674\n",
      "Cost epoch 3: 0.4919354838709674\n",
      "Cost epoch 4: 0.4919354838709674\n",
      "Cost epoch 5: 0.4919354838709674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22896/3433042826.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mex_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22896/2696275390.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, epochs, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[0;32m    296\u001b[0m                 \u001b[1;31m# BACKWARD PROP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m                     \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m                 \u001b[1;31m# UPDATE PARAMS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22896/2696275390.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dZ)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2DBackpropInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2DBackpropFilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[1;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[1;32m--> 404\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1256\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m       return conv2d_backprop_input_eager_fallback(\n\u001b[0m\u001b[0;32m   1259\u001b[0m           \u001b[0minput_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input_eager_fallback\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m-> 1352\u001b[1;33m   _result = _execute.execute(b\"Conv2DBackpropInput\", 1, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m   1353\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Change below three parameters for experiments \n",
    "train_size = 160\n",
    "batch_size = 32\n",
    "learning = 0.1\n",
    "\n",
    "num_loops = 10\n",
    "times = []\n",
    "accuracy_lst = [0,0,0,0]\n",
    "for i in range(num_loops):\n",
    "    # Select only m samples\n",
    "    m = train_size\n",
    "    X = X_train[:m, :, :].reshape((m, 300, 300, 3)).astype(\"float64\")\n",
    "    y = y_train[:m].reshape(1,m).astype(\"float64\")\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(16, 3, (None, 300, 300, 3)),\n",
    "        Maxpool((None, 298, 298, 16), pool_size=2),\n",
    "        Conv2D(32, 3, (None, 149, 149, 16)),\n",
    "        Maxpool((None, 147, 147, 32), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 73, 73, 32)),\n",
    "        Maxpool((None, 71, 71, 64), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 35, 35, 64)),\n",
    "        Maxpool((None, 33, 33, 64), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 16, 16, 64)),\n",
    "        Maxpool((None, 14, 14, 64), pool_size=2),\n",
    "        Flatten((None, 7, 7, 64)),\n",
    "        knn_differentiable((3136, None), 2)\n",
    "    ]\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=learning, verbose=1, batch_size=batch_size)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Runs through each testing batch size for the table and gathers accuracies\n",
    "    batch_sizes = [256, 128, batch_size, 10]\n",
    "    for j in range(len(batch_sizes)):\n",
    "        m = batch_sizes[j]\n",
    "        n = 0\n",
    "        accuracies = []\n",
    "        for l in range(y_test.shape[0]//m):\n",
    "            X = X_test[n:m, :, :].reshape((m-n, 300, 300, 3)).astype(\"float64\")\n",
    "            y = y_test[n:m].reshape(1,m-n).astype(float)\n",
    "            predictions = model.predict(X, y)\n",
    "            \n",
    "            score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "            accuracies.append(score)\n",
    "\n",
    "            n = m\n",
    "            m += batch_sizes[j]\n",
    "        print(predictions[:10])\n",
    "        avg_acc = np.average(accuracies)\n",
    "        accuracy_lst[j] += avg_acc\n",
    "        print(\"Accuracy\", j+1, \"-\", avg_acc)\n",
    "        print()\n",
    "    print(\"+\"*50, \"end of loop\", i+1)\n",
    "    print()\n",
    "\n",
    "# Output average execution time and average accuracies in a list of the same order as shown in tables\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracies:\", np.divide(accuracy_lst, num_loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5       0.4921875 0.5390625 0.556 - 1 epoch, learning rate of 0 to get baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9140625  0.89453125 0.8203125  0.748 - 10 epoch, learning rate of .005    (end loss 0.1209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9609375  0.9296875  0.87890625 0.768 - 10 epoch, learning rate of .005    (end loss 0.0514)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
