{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        0\n",
      "2        0\n",
      "4        0\n",
      "8        1\n",
      "9        1\n",
      "        ..\n",
      "59983    1\n",
      "59985    0\n",
      "59995    1\n",
      "59998    0\n",
      "59999    1\n",
      "Name: training targets, Length: 12000, dtype: uint8\n",
      "(12000, 28, 28) (2000, 28, 28)\n",
      "(12000,) (2000,)\n",
      "\n",
      "(6000,) (6000,)\n",
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "MNIST = k.datasets.fashion_mnist.load_data()\n",
    "#MNIST = k.datasets.mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 0) | (y_train == 5)]\n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 0) | (y_test == 5)]\n",
    "X_test = X_test[y_test.index]\n",
    "\n",
    "y_train[y_train==5] = 1\n",
    "y_test[y_test==5] = 1\n",
    "X_train[X_train==5] = 1\n",
    "X_test[X_test==5] = 1\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print()\n",
    "print(y_train[y_train == 0].shape, y_train[y_train == 1].shape)\n",
    "print(y_test[y_test == 0].shape, y_test[y_test == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 65,921\n",
      "Trainable params: 65,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = \"random_normal\" # random_normal or glorot_uniform\n",
    "keras_model = k.Sequential([ \n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.MaxPooling2D((3,3)),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.3877\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4299\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2367\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1562\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1190\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0966\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0817\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0717\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0643\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0589\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 1000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### ( 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    #print(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple, trainable=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + \" \" + str(self.output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (neurons, input_shape[1])\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.01\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "            \n",
    "        self.dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "        self.db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.001\n",
    "        self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform convolution\n",
    "        Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "        # Add bias\n",
    "        Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z.numpy()\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "        \n",
    "        dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.db = tf.raw_ops.BiasAddGrad(out_backprop=dZ).numpy()\n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.ksize = [1, pool_size, pool_size, 1]\n",
    "        self.strides = [1, pool_size, pool_size, 1]\n",
    "        output_shape = (input_shape[0], input_shape[1]//pool_size, input_shape[2]//pool_size, input_shape[3])\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        Z = tf.raw_ops.MaxPool(input=A_prev, ksize=self.ksize, strides=self.strides, data_format='NHWC', padding=\"VALID\").numpy()\n",
    "        self.cache = (A_prev, Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev, Z = self.cache\n",
    "        dA_prev = tf.raw_ops.MaxPoolGrad(orig_input=A_prev, orig_output=Z, grad=dZ, ksize=self.ksize, strides=self.strides, padding=\"VALID\", data_format='NHWC').numpy()\n",
    "        return dA_prev\n",
    "    \n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "           \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (np.prod(shape), m)\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.flatten().reshape(m,np.prod(shape)).T\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.T.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        self.parameters = dict()\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, verbose): \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        for epoch in range(epochs):\n",
    "            # FORWARD PROP\n",
    "            Z = X\n",
    "            for layer in self.layers:    \n",
    "                if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                    Z = layer.forward(Z, Y)\n",
    "                else:\n",
    "                    Z = layer.forward(Z)\n",
    "                #print(layer, Z.shape)\n",
    "            \n",
    "            # COST FUNCTION\n",
    "            #print(Z.shape, Z)\n",
    "            # Keras cost\n",
    "            loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "            TL = loss(Y, Z)\n",
    "            print(\"ALT COST\", TL)\n",
    "            \n",
    "            # our cost\n",
    "            cost = compute_cost(Z, Y)\n",
    "            history.append(cost)\n",
    "            if verbose == 1:\n",
    "                print(\"Cost epoch \", epoch, \": \", cost, end=\"\\n\\n\")\n",
    "            \n",
    "            # BACKWARD PROP\n",
    "            m = Y.shape[1]\n",
    "            dA = -(1/m)*(np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to Z\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                dA = layer.backward(dA)\n",
    "            \n",
    "            # UPDATE PARAMS\n",
    "            for layer in self.layers:\n",
    "                layer.update_params(learning_rate)\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                Z = layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = layer.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"-\"*25)\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    def _cost(self, X, Y):\n",
    "        epsilon=1e-7\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            if prop_layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "               #J1 = prop_layer.forward(Z+ epsilon, Y) \n",
    "               #J2 = prop_layer.forward(Z- epsilon, Y) \n",
    "                \n",
    "                #print(\"TESTING: \",(J1-J2)/(2*epsilon))\n",
    "                \n",
    "                Z = prop_layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = prop_layer.forward(Z)\n",
    "            #print(prop_layer, Z)\n",
    "        # COMPUTE COST\n",
    "        return compute_cost(Z, Y)\n",
    "    \n",
    "    def gradcheck(self, X, Y, epsilon=1e-7, numlayers=None):\n",
    "        self.approx_grads = []\n",
    "        self.true_grads = []\n",
    "        print(\"Starting loops...\")\n",
    "        j=1\n",
    "        for layer in self.layers[:numlayers]:\n",
    "            print(j, len(self.layers))\n",
    "            j += 1\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "                \n",
    "            for i in range(layer.W.size):\n",
    "                i = np.unravel_index(i, layer.W.shape)\n",
    "                Wi = layer.W[i]\n",
    "                layer.W[i] = Wi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi\n",
    "                #print(J1)\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "            print(\"Done W's\")  \n",
    "            for i in range(layer.b.size):\n",
    "                i = np.unravel_index(i, layer.b.shape)\n",
    "                bi = layer.b[i]\n",
    "                layer.b[i] = bi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.b[i] = bi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.b[i] = bi\n",
    "                #print((J1-J2))\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "            print(\"Done B's\")\n",
    "        \n",
    "        # FORWARD PROP\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            #print(prop_layer)\n",
    "            if prop_layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                Z = prop_layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = prop_layer.forward(Z)\n",
    "                \n",
    "        # BACKWARD PROP\n",
    "        m = Y.shape[1]\n",
    "        dA = -(1/m)*(np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        \n",
    "        for layer in self.layers[:numlayers]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            self.true_grads = np.concatenate((self.true_grads, layer.dW.flatten(), layer.db.flatten()))\n",
    "        return np.sqrt(np.sum(np.square(self.true_grads-self.approx_grads)))/(np.sqrt(np.sum(np.square(self.true_grads)))+np.sqrt(np.sum(np.square(self.approx_grads))))\n",
    "\n",
    "    \n",
    "class knn_differentiable(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__(input_shape, num_classes, False)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_features, batch_labels):\n",
    "        self.batch_features = np.transpose(batch_features).astype('float')\n",
    "#         print(\"BATCH_FEATURES\\n\", self.batch_features)\n",
    "#         print()\n",
    "        self.batch_labels = batch_labels.astype('float')\n",
    "        \n",
    "        self.distances = self.calc_distance_mtx(self.batch_features, self.batch_features)\n",
    "        #self.distances = np.divide(1, self.distances, where=self.distances!=0)\n",
    "        #print(\"DISTANCES\\n\",self.distances)\n",
    "        #print()\n",
    "        \n",
    "        self.class_0 = np.array(self.batch_labels[:] == 0).astype('float')\n",
    "        self.class_1 = np.array(self.batch_labels[:] == 1).astype('float')\n",
    "        self.num_0 = np.count_nonzero(self.batch_labels == 0)\n",
    "        self.num_1 = np.count_nonzero(self.batch_labels == 1)\n",
    "#         print(\"SIZES\", self.num_0, self.num_1)\n",
    "#         print()\n",
    "\n",
    "        self.aggregate = np.stack([np.sum(np.multiply(self.distances, self.class_0), 1) / self.num_0, np.sum(np.multiply(self.distances, self.class_1), 1) / self.num_1], axis=1)\n",
    "#         print(\"AGGREGATE\\n\",aggregate)\n",
    "#         print()\n",
    "        \n",
    "        exp = np.exp(-self.aggregate)\n",
    "#         print(\"EXP OF AGGREGATE\\n\",exp)\n",
    "#         print(exp[:,0].shape)\n",
    "#         print()\n",
    "        \n",
    "        self.softmax = np.divide(exp[:,1], np.sum(exp, 1))\n",
    "#         print(\"SOFTMAX\\n\",self.softmax[:50])\n",
    "#         print(\"=\"*20)\n",
    "#         print()\n",
    "        \n",
    "        return np.reshape(self.softmax, (1,self.distances.shape[0]))\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        # Overall what we need here:\n",
    "        # d(TL)/d(X = features) = d(TL)/D(L) * d(L)/d(S) * \n",
    "        #     [(d(S)/d(A_0) * d(A_0)/d(D_0) * d(D_0)/d(X)) + (d(S)/d(A_1) * d(A_1)/d(D_1) * d(D_1)/d(X))]\n",
    "        #\n",
    "        # Shapes:\n",
    "        # d(TL)/d(L)  -  (N x 1)           N = number of samples (images)\n",
    "        # d(L)/d(S)   -  (N x 1)\n",
    "        # d(S)/d(Ai)  -  (N x 1)\n",
    "        # d(Ai)/d(Di) -  (N x N)\n",
    "        # d(Di)/d(X)  -  (N x f)           f = number of features (output by cnn part)\n",
    "        # final output:  (N x f)\n",
    "        \n",
    "        \n",
    "        # d(TL)/d(L) = -1/m * sum(d(L)/d(S))  <-- Maybe don't need this at all, done before calling backward\n",
    "        \n",
    "        \n",
    "        # d(L)/d(S) = (Y/S) - (1-Y)/(1-S)    <-- Y = true labels, S = softmax labels from column 0 (softmax on class 0)\n",
    "        dL_dS = dA.T   # <-- Really dTL_dS\n",
    "        \n",
    "#         print(\"dL_dS\", dL_dS.shape, dL_dS[:25])\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(S)/d(A) = - (e^-A0 * e^-A1) / (e^-2A0 + e^-A0-A1 + e^-2A1)\n",
    "        # This calc is for A_0 and A_1, A_1 is positive version of this\n",
    "        #np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))\n",
    "        dS_dA0 = -np.divide(np.multiply(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])), \n",
    "                            np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))).reshape(self.distances.shape[0],1)\n",
    "        \n",
    "        # np.add(np.add(np.exp(-2*self.aggregate[:,0]), np.exp(-2*self.aggregate[:,1])), \n",
    "        #                           np.exp(np.multiply(self.aggregate[:,0], self.aggregate[:,1])))\n",
    "        \n",
    "        \n",
    "        dS_dA1 = -dS_dA0\n",
    "#         print(\"dS_dA0\", dS_dA0.shape, dS_dA0[:25]) (1000,) (1000,1)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(A)/d(D) = 1/len(class) * sum(d(D)/d(X))   <-- Same calc for A_0 and A_1, len(class) = how many of this class there are either num_0 or num_1\n",
    "        # possibly don't need this part either since its calculated within loops???\n",
    "        \n",
    "        \n",
    "        # d(D)/d(X) = 2(x_i - x_j)   <-- x_i and x_j are the two feature vectors used in the distance\n",
    "        #distances_0 = \n",
    "        dA0_dX = np.ones(self.batch_features.shape)\n",
    "        # iterate over rows in distance matrix\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD_dX = np.zeros(self.batch_features.shape[1])\n",
    "            # iterate over the columns in distance matrix\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                # only calculate derivatives for indices of class 0\n",
    "                if self.class_0[:,j] == 0:\n",
    "                    # first calc derivative of distance formula, then them all together\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD_dX = np.add(dD_dX, deriv)\n",
    "            # replace the corresponding derivatives for each row (sample)\n",
    "            dA0_dX[i,:] = dD_dX\n",
    "        \n",
    "        # Same as loop above but on indices of class 1\n",
    "        dA1_dX = np.ones(self.batch_features.shape)\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD_dX = np.zeros(self.batch_features.shape[1])\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                if self.class_0[:,j] == 1:\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD_dX = np.add(dD_dX, deriv)\n",
    "            dA1_dX[i,:] = dD_dX\n",
    "        \n",
    "\n",
    "        # divide by the number of samples in each class since these were averaged in forward\n",
    "        dA0_dX = dA0_dX * (1/self.num_0)\n",
    "        dA1_dX = dA1_dX * (1/self.num_1)\n",
    "#         print(\"dD0_dX\", dD0_dX.shape, dD0_dX)\n",
    "#         print()\n",
    "#         print(\"dD1_dX\", dD0_dX.shape, dD1_dX)\n",
    "#         print()\n",
    "\n",
    "        # add together the terms in brackets from the full formula\n",
    "        dS_dX = np.add(np.multiply(dS_dA0, dA0_dX), np.multiply(dS_dA1, dA1_dX))\n",
    "        # final step of the chain rule\n",
    "        dL_dX = np.multiply(dL_dS, dS_dX)\n",
    "#         print(\"dL_dX\", dL_dX.shape, dL_dX)\n",
    "        \n",
    "        print('='*50)\n",
    "        return dL_dX\n",
    "        #return np.zeros(self.batch_features.shape)\n",
    "    \n",
    "    def calc_distance_mtx(self, A, B):\n",
    "        \"\"\"\n",
    "        Computes squared pairwise distances between each elements of A and each elements of B.\n",
    "        Args:\n",
    "        A,    [m,d] matrix\n",
    "        B,    [n,d] matrix\n",
    "        Returns:\n",
    "        D,    [m,n] matrix of pairwise distances\n",
    "        \"\"\"\n",
    "        with tf.compat.v1.variable_scope('pairwise_dist'):\n",
    "            # squared norms of each row in A and B\n",
    "            na = tf.reduce_sum(tf.square(A), 1)\n",
    "            nb = tf.reduce_sum(tf.square(B), 1)\n",
    "\n",
    "            # na as a row and nb as a co\"lumn vectors\n",
    "            na = tf.reshape(na, [-1, 1])\n",
    "            nb = tf.reshape(nb, [1, -1])\n",
    "\n",
    "            # return pairwise euclidead difference matrix\n",
    "            D = tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0)\n",
    "        return D.numpy()\n",
    "    \n",
    "    def calc_cosine_sim(self, A, B):\n",
    "        from scipy.spatial.distance import pdist\n",
    "        from scipy.spatial.distance import squareform\n",
    "        cos_sim = squareform(pdist(A, metric='cosine'))\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT COST tf.Tensor(0.7120882272720337, shape=(), dtype=float64)\n",
      "Cost epoch  0 :  0.6850046708614388\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.712259829044342, shape=(), dtype=float64)\n",
      "Cost epoch  1 :  0.6857110099846532\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7123441696166992, shape=(), dtype=float64)\n",
      "Cost epoch  2 :  0.6860531609145192\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7123494744300842, shape=(), dtype=float64)\n",
      "Cost epoch  3 :  0.6860638538363617\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7122964262962341, shape=(), dtype=float64)\n",
      "Cost epoch  4 :  0.6858305253263527\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7121918201446533, shape=(), dtype=float64)\n",
      "Cost epoch  5 :  0.6853806515102314\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7120203971862793, shape=(), dtype=float64)\n",
      "Cost epoch  6 :  0.6846511239588726\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7117726802825928, shape=(), dtype=float64)\n",
      "Cost epoch  7 :  0.6836012599003416\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7114269137382507, shape=(), dtype=float64)\n",
      "Cost epoch  8 :  0.6821387480572687\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7109457850456238, shape=(), dtype=float64)\n",
      "Cost epoch  9 :  0.6801098436246592\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.710254430770874, shape=(), dtype=float64)\n",
      "Cost epoch  10 :  0.6772055136274527\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7092212438583374, shape=(), dtype=float64)\n",
      "Cost epoch  11 :  0.672884080905441\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7076361179351807, shape=(), dtype=float64)\n",
      "Cost epoch  12 :  0.6663026829645039\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7050820589065552, shape=(), dtype=float64)\n",
      "Cost epoch  13 :  0.6558130515910626\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.7005821466445923, shape=(), dtype=float64)\n",
      "Cost epoch  14 :  0.6376703329936262\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.691853940486908, shape=(), dtype=float64)\n",
      "Cost epoch  15 :  0.6036455613609619\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.6729758977890015, shape=(), dtype=float64)\n",
      "Cost epoch  16 :  0.5347399528617723\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.6325705647468567, shape=(), dtype=float64)\n",
      "Cost epoch  17 :  0.40644555572514546\n",
      "\n",
      "==================================================\n",
      "ALT COST tf.Tensor(0.5880746841430664, shape=(), dtype=float64)\n",
      "Cost epoch  18 :  0.3284289277940405\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "np.random.seed(10)\n",
    "m = 1000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(16, 7, (None, 28, 28, 1)),\n",
    "    #ReLU((None, 22, 22, 16)),\n",
    "    Maxpool((None, 22, 22, 16)),\n",
    "    Conv2D(16, 9, (None, 11, 11, 16)),\n",
    "    #ReLU((None, 8, 8, 32)),\n",
    "    Maxpool((None, 3, 3, 16)),\n",
    "    Flatten((None, 1, 1, 16)),\n",
    "    #Dense(32, (5184, None), \"relu\"),\n",
    "    #Dense(1, (32, None), \"sigmoid\")\n",
    "    knn_differentiable((64, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "history = model.fit(X, y, epochs=19, learning_rate=0.005, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loops...\n",
      "1 6\n",
      "Done W's\n",
      "Done B's\n",
      "2 6\n",
      "3 6\n",
      "Done W's\n",
      "Done B's\n",
      "4 6\n",
      "5 6\n",
      "6 6\n",
      "==================================================\n",
      "0.7606332886651564\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true grads</th>\n",
       "      <th>approx grads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.013448</td>\n",
       "      <td>0.052668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001167</td>\n",
       "      <td>-0.005450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008330</td>\n",
       "      <td>-0.024818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006284</td>\n",
       "      <td>-0.087154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.026957</td>\n",
       "      <td>-0.009739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21547</th>\n",
       "      <td>0.008351</td>\n",
       "      <td>-0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21548</th>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21549</th>\n",
       "      <td>-0.005742</td>\n",
       "      <td>-0.001199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>-0.010412</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21552 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       true grads  approx grads\n",
       "0       -0.013448      0.052668\n",
       "1       -0.001167     -0.005450\n",
       "2        0.008330     -0.024818\n",
       "3       -0.006284     -0.087154\n",
       "4       -0.026957     -0.009739\n",
       "...           ...           ...\n",
       "21547    0.008351     -0.000017\n",
       "21548    0.006610     -0.000027\n",
       "21549   -0.005742     -0.001199\n",
       "21550   -0.010412     -0.000043\n",
       "21551    0.006235      0.000099\n",
       "\n",
       "[21552 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "np.random.seed(10)\n",
    "m = 25\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(16, 7, (None, 28, 28, 1)),\n",
    "    #ReLU((None, 22, 22, 16)),\n",
    "    Maxpool((None, 22, 22, 16)),\n",
    "    Conv2D(16, 9, (None, 11, 11, 16)),\n",
    "    #ReLU((None, 8, 8, 32)),\n",
    "    Maxpool((None, 3, 3, 16)),\n",
    "    Flatten((None, 1, 1, 16)),\n",
    "    #Dense(32, (5184, None), \"relu\"),\n",
    "    #Dense(1, (32, None), \"sigmoid\")\n",
    "    knn_differentiable((64, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "print(model.gradcheck(X,y, epsilon=1e-7))\n",
    "pd.DataFrame(np.column_stack((model.true_grads, model.approx_grads)), columns=['true grads', 'approx grads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      "  1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      "  1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      "  1. 0. 1. 1.]]\n",
      "[[9.99782134e-01 9.97009523e-01 1.37337506e-04 9.46404816e-01\n",
      "  6.90704573e-05 9.74101262e-01 9.99809256e-01 9.99425688e-01\n",
      "  9.90399831e-01 9.98837756e-01 9.97187488e-01 9.98325601e-01\n",
      "  9.98242007e-01 1.05401795e-04 5.48773844e-04 9.99847259e-01\n",
      "  9.78525398e-01 9.98668656e-01 9.99542083e-01 7.42028968e-04\n",
      "  9.99935110e-01 3.70559929e-01 7.88044135e-05 3.67217609e-04\n",
      "  9.99983219e-01 9.95996675e-01 6.99167136e-07 9.71504899e-01\n",
      "  8.24863613e-05 9.99680162e-01 7.08464084e-05 9.94803851e-01\n",
      "  1.40228115e-02 9.99696846e-01 4.86904926e-05 1.55464046e-02\n",
      "  1.88438788e-01 7.93928477e-01 9.50881674e-01 7.06217329e-04\n",
      "  4.41659884e-02 3.92531689e-02 9.99576280e-01 9.93993961e-01\n",
      "  9.93816290e-01 9.94322819e-01 9.99157169e-01 4.15237331e-03\n",
      "  1.48067203e-04 5.27546132e-01 4.38966715e-03 1.63450628e-04\n",
      "  1.23768755e-02 9.52376530e-01 9.98872468e-01 3.87587832e-01\n",
      "  9.99446942e-01 9.97781421e-01 9.94530632e-01 1.60787719e-05\n",
      "  4.86276757e-04 9.99775268e-01 1.97950149e-05 7.25004176e-02\n",
      "  3.62644647e-02 1.47774692e-05 9.98346539e-01 8.79252406e-01\n",
      "  4.97242075e-01 9.97651820e-01 3.20038175e-05 9.97985882e-01\n",
      "  9.99889904e-01 4.97674932e-02 1.19301799e-03 5.81261548e-02\n",
      "  8.61764738e-05 7.70434254e-01 9.98211791e-01 9.99938807e-01\n",
      "  1.75523421e-04 8.61383837e-01 9.97079255e-01 4.01524058e-01\n",
      "  7.31077745e-05 1.26797604e-02 6.09477303e-01 7.13353232e-05\n",
      "  2.38047583e-01 8.24812334e-01 9.99257500e-01 4.18712576e-02\n",
      "  3.54793718e-03 2.30954358e-04 2.60853688e-04 9.53556778e-02\n",
      "  9.97581556e-01 9.99342602e-01 9.94965436e-01 9.87748206e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.862"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "m = 1000\n",
    "X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "predictions = model.predict(X, y)\n",
    "#print(predictions.type)\n",
    "print(y[:,:100])\n",
    "print(predictions[:,:100])\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.85764229e-04 9.85277430e-01 3.43833509e-03 9.99883427e-01\n",
      "  9.98405518e-01 6.71777884e-03 9.82325295e-01 9.98442956e-01\n",
      "  6.84530899e-04 1.14146341e-02 9.99710833e-01 9.99024262e-01\n",
      "  9.98767792e-01 9.93673534e-01 2.40007849e-02 9.72246170e-01\n",
      "  1.88699027e-01 9.99678365e-01 9.99919860e-01 3.60122599e-03\n",
      "  1.88073936e-04 5.93422144e-02 9.99450508e-01 8.27332646e-02\n",
      "  9.99028639e-01 9.99378269e-01 9.91855178e-01 9.99897943e-01\n",
      "  4.56351217e-03 1.10823195e-04 9.99507210e-01 1.70341742e-02\n",
      "  2.92148421e-05 9.38897628e-06 9.99811406e-01 9.99886164e-01\n",
      "  2.55873999e-04 9.87404683e-01 1.03608088e-01 9.63403672e-01\n",
      "  7.89872834e-03 9.40205654e-01 2.13537221e-04 9.99873371e-01\n",
      "  9.90419440e-01 9.97884752e-01 9.41665637e-01 9.51035538e-01\n",
      "  9.95740067e-01 1.64119429e-02 1.53793864e-03 9.89034541e-01\n",
      "  1.89272516e-05 1.84020714e-01 9.99743335e-01 4.44887153e-06\n",
      "  9.82705592e-01 6.34386128e-01 9.98136053e-01 9.89073819e-01\n",
      "  4.60323814e-06 1.25352190e-04 9.99793796e-01 9.99863420e-01\n",
      "  7.27715988e-01 9.99771037e-01 4.31477480e-05 9.99664113e-01\n",
      "  5.78231125e-03 9.96060705e-01 1.47622914e-05 1.81876899e-02\n",
      "  4.30467302e-03 9.93580183e-01 9.37126953e-01 9.99863780e-01\n",
      "  7.39459265e-01 9.75245988e-01 9.40143341e-01 6.89432216e-01\n",
      "  3.24695194e-03 9.99908858e-01 9.93389624e-01 9.99402269e-01\n",
      "  2.07239155e-03 9.97844785e-01 9.97845562e-01 9.97953993e-01\n",
      "  1.51652555e-04 9.86738050e-01 9.99704147e-01 9.95181512e-01\n",
      "  4.77811111e-02 2.84281981e-01 6.93432577e-01 1.63572004e-01\n",
      "  9.99419203e-01 9.96178124e-01 9.98776921e-01 9.99737058e-01\n",
      "  9.98558743e-01 4.12455626e-01 1.33812377e-05 2.50322345e-01\n",
      "  9.89836364e-01 8.39532519e-06 9.99269974e-01 9.52392122e-01\n",
      "  2.95091580e-02 4.25830284e-05 9.76417395e-01 9.98126323e-01\n",
      "  9.95782216e-01 4.50968841e-04 1.81830813e-02 9.99877007e-01\n",
      "  9.98268652e-01 8.85186795e-03 4.54458956e-01 9.99738797e-01\n",
      "  9.15559687e-01 9.38826913e-03 9.99865454e-01 9.99861212e-01\n",
      "  9.81843989e-01 1.77284074e-03 3.69065914e-02 4.78261610e-05\n",
      "  2.48902748e-04 9.92774981e-01 9.98963884e-01 2.41380706e-02\n",
      "  8.96982605e-06 8.20754448e-03 9.99799164e-01 9.98830618e-01\n",
      "  9.99849241e-01 9.28856456e-01 9.98032435e-01 9.96036743e-01\n",
      "  9.99405970e-01 9.90041894e-01 3.49089963e-05 6.49960727e-01\n",
      "  1.12799300e-01 2.01247285e-05 9.97965794e-01 9.99943297e-01\n",
      "  3.73778242e-05 7.64261936e-05 9.86389095e-01 9.97350306e-01\n",
      "  1.03628279e-05 9.99953019e-01 9.97120878e-01 9.95681768e-01\n",
      "  4.25790651e-05 9.99600108e-01 9.98343421e-01 2.25224559e-01\n",
      "  6.59671566e-03 6.33990159e-01 6.50150627e-02 1.69976662e-04\n",
      "  9.99836433e-01 8.88274635e-05 8.66479624e-04 2.05310806e-04\n",
      "  3.66705272e-02 3.98008882e-04 7.95666730e-01 9.99422113e-01\n",
      "  1.68085599e-05 9.99734223e-01 9.99796339e-01 9.98052093e-01\n",
      "  9.99459346e-01 9.68644176e-01 3.57703755e-04 5.07878227e-04\n",
      "  9.73765664e-01 6.25726111e-05 9.99951503e-01 9.65939956e-01\n",
      "  1.69638877e-03 2.14588265e-03 9.60630951e-01 9.99779528e-01\n",
      "  9.99369111e-01 9.80733304e-01 9.99591285e-01 9.99841429e-01\n",
      "  9.98633736e-01 7.53660438e-01 9.91966228e-01 9.99846960e-01\n",
      "  5.45029302e-01 9.98778754e-01 9.99384727e-01 3.91309059e-02\n",
      "  1.26173854e-04 1.62352426e-02 9.81915024e-01 9.98748021e-01\n",
      "  9.89149293e-01 9.98878402e-01 9.94384466e-01 1.70165332e-02\n",
      "  9.94020375e-01 9.89648265e-01 9.99949754e-01 1.16020298e-04\n",
      "  9.96368944e-01 9.99200179e-01 1.15342509e-03 9.99820786e-01\n",
      "  4.47751225e-06 9.96425394e-01 9.64447957e-01 9.02016012e-06\n",
      "  9.99848816e-01 9.98900661e-01 6.71829596e-01 9.04038686e-01\n",
      "  7.85841214e-02 7.12836373e-02 1.07352066e-01 9.79784898e-01\n",
      "  9.82199956e-01 9.99766873e-01 9.98586395e-01 9.95281556e-01\n",
      "  7.87398024e-01 8.14522317e-02 1.22054752e-03 4.61685485e-02\n",
      "  9.99839737e-01 9.92457545e-01 9.96001693e-01 5.84513154e-07\n",
      "  9.99184493e-01 9.95168958e-01 5.94392787e-01 9.99663021e-01\n",
      "  9.99862601e-01 9.99811457e-01 8.21122388e-04 9.99748232e-01\n",
      "  9.99766545e-01 9.94401528e-01 2.63086407e-01 9.99958749e-01\n",
      "  6.38537877e-02 9.84156292e-01 9.99669388e-01 9.99882540e-01\n",
      "  2.46183683e-03 9.97760032e-01 9.98726968e-01 2.48185494e-01\n",
      "  3.18171813e-04 9.98311471e-01 9.99952776e-01 9.99351624e-01\n",
      "  1.29539585e-05 9.93848773e-01 9.99942223e-01 6.28649348e-04\n",
      "  5.94634796e-02 2.07341393e-01 9.99036737e-01 9.98960262e-01\n",
      "  9.98730645e-01 9.98672672e-01 9.99753367e-01 9.99725314e-01\n",
      "  9.98180305e-01 9.77848802e-01 9.94935664e-01 9.98549166e-01\n",
      "  9.99219758e-01 3.37602312e-03 3.92086346e-01 8.94629063e-04\n",
      "  9.98197558e-01 9.98612222e-01 9.99915006e-01 4.11378275e-01\n",
      "  9.95382992e-01 9.99876996e-01 9.99707929e-01 9.67056654e-01\n",
      "  9.99887411e-01 9.87428984e-01 9.95224448e-01 7.33057752e-03\n",
      "  9.86219518e-01 8.35238734e-04 9.99240176e-01 5.78296002e-03\n",
      "  9.95731369e-01 9.90413471e-01 9.99318931e-01 9.99894127e-01\n",
      "  8.25455284e-01 8.02524510e-03 9.99805283e-01 7.54758795e-01\n",
      "  3.51551321e-05 2.67147556e-01 9.89806250e-01 4.07358838e-04\n",
      "  7.36038296e-01 9.96815641e-01 9.89220541e-01 9.99943173e-01\n",
      "  9.66345922e-01 6.93119176e-01 1.06306144e-04 2.25949795e-03\n",
      "  4.26245200e-02 4.86575520e-02 5.02513814e-06 9.99976007e-01\n",
      "  9.88501969e-01 9.43000085e-01 7.89376772e-01 1.51142825e-05\n",
      "  9.40070986e-01 9.99292434e-01 9.68591290e-01 1.13462088e-03\n",
      "  9.99916142e-01 9.07744043e-01 8.75857016e-01 1.37705051e-01\n",
      "  9.98836509e-01 1.51889866e-05 9.91711527e-01 9.97442123e-01\n",
      "  1.26035256e-01 9.99280020e-01 9.99127745e-01 9.99641289e-01\n",
      "  2.43887811e-01 9.99699376e-01 9.83776343e-01 9.99157208e-01\n",
      "  9.99645316e-01 3.74515003e-05 9.90475653e-01 9.98460527e-01\n",
      "  9.98077571e-01 2.21913599e-05 9.95439817e-01 9.97861628e-01\n",
      "  9.64064862e-03 9.99715473e-01 9.93189258e-01 9.87974332e-01\n",
      "  3.26219772e-06 9.25355528e-01 1.91113136e-06 3.74118695e-03\n",
      "  8.61397000e-01 4.72678544e-05 9.99834592e-01 9.82675972e-01\n",
      "  6.53501997e-03 9.91000465e-01 9.83834991e-01 9.99884062e-01\n",
      "  5.74640144e-01 9.98816810e-01 9.99410049e-01 4.27054264e-02\n",
      "  4.33415416e-05 9.99800787e-01 9.97592749e-01 5.84430513e-01\n",
      "  2.39692309e-05 5.35900212e-01 9.90872488e-01 8.78627292e-04\n",
      "  9.99540112e-01 1.77948292e-05 9.99693467e-01 2.87192147e-05\n",
      "  2.83756573e-01 9.99910476e-01 9.94644412e-01 7.98037949e-04\n",
      "  9.99038769e-01 9.99831748e-01 2.61549816e-05 9.99718936e-01\n",
      "  2.86041459e-05 9.99506412e-01 7.35024351e-01 1.05833900e-02\n",
      "  9.90885048e-01 9.99967629e-01 1.24922653e-03 3.49406653e-04\n",
      "  9.99751406e-01 7.73556356e-01 9.97787520e-01 9.99965846e-01\n",
      "  9.98471050e-01 7.03489065e-05 1.62766854e-01 7.55413901e-01\n",
      "  9.97730057e-01 5.92063544e-04 1.72863395e-04 9.96586880e-01\n",
      "  9.95048288e-01 9.74455410e-01 9.86194133e-01 9.99595608e-01\n",
      "  9.99568593e-01 6.93092702e-01 9.96758335e-01 7.60645734e-01\n",
      "  9.99663657e-01 6.78536362e-04 9.97374554e-01 4.08470427e-04\n",
      "  9.99564636e-01 9.35324066e-01 3.42016628e-02 8.26553853e-01\n",
      "  2.69188697e-02 1.59943825e-02 9.39951456e-01 9.99882523e-01\n",
      "  3.44161671e-04 3.27098513e-02 9.99913286e-01 1.21020038e-03\n",
      "  1.82634382e-04 9.97331615e-01 2.15214904e-04 7.11072175e-05\n",
      "  2.17046985e-01 9.90344063e-01 6.94342103e-01 3.43411532e-03\n",
      "  9.21169879e-01 9.99880824e-01 9.99971077e-01 9.99629676e-01\n",
      "  5.94213620e-03 9.96550968e-01 9.99960791e-01 9.99868964e-01\n",
      "  9.99715691e-01 9.99439284e-01 2.66504821e-03 9.98268695e-01\n",
      "  6.90420217e-04 6.43854166e-05 9.91100443e-01 2.11487776e-01\n",
      "  2.26806440e-04 9.89297415e-01 9.98638903e-01 2.42358682e-04\n",
      "  9.99012257e-01 9.83206872e-01 9.83212149e-01 5.05529154e-01\n",
      "  9.98855604e-01 1.64616560e-01 3.95399884e-01 9.99152372e-01\n",
      "  1.15524352e-03 1.72640670e-04 7.81907274e-01 5.54758140e-01\n",
      "  9.95815008e-01 9.97477345e-01 7.70630924e-01 3.25370962e-05\n",
      "  9.99268482e-01 9.95109188e-01 9.94253119e-01 7.20084792e-01\n",
      "  9.98900477e-01 8.65218929e-01 9.41823351e-01 4.56043978e-03\n",
      "  9.99420014e-01 9.93482324e-01 9.98695927e-01 9.88169494e-01\n",
      "  9.99101838e-01 9.99861706e-01 9.95236781e-01 9.97060740e-01\n",
      "  9.84732222e-01 9.82860256e-01 9.99371849e-01 4.44081127e-03\n",
      "  9.92386871e-01 9.98992799e-01 9.99547077e-01 9.99849941e-01\n",
      "  9.99722542e-01 9.90058590e-01 3.48035325e-01 9.99894635e-01\n",
      "  7.32732682e-06 9.99578764e-01 1.62698532e-06 9.99964379e-01\n",
      "  1.44441241e-06 9.98036620e-01 1.49933509e-06 8.13441317e-01\n",
      "  9.96861289e-01 9.99714795e-01 2.37700673e-05 8.68095590e-01\n",
      "  9.99958521e-01 9.97696671e-01 9.99854557e-01 9.86840731e-01\n",
      "  1.27629846e-04 2.87844697e-05 9.99507985e-01 8.35056970e-06\n",
      "  6.20839867e-01 2.25994152e-02 9.98928158e-01 7.60801678e-02\n",
      "  9.90168428e-01 9.98970737e-01 6.52564898e-06 2.76900533e-03\n",
      "  3.26284184e-03 8.33689581e-03 9.99690827e-01 2.22359822e-02\n",
      "  9.98707307e-01 9.88839940e-01 8.81949284e-05 9.98173887e-01\n",
      "  9.86719843e-01 9.99827079e-01 6.81379446e-02 9.99144476e-01\n",
      "  1.01554725e-02 8.28778512e-01 9.99821880e-01 9.87632764e-01\n",
      "  1.08608440e-01 9.76123827e-01 9.99878047e-01 9.99921223e-01\n",
      "  1.47673434e-05 4.45110903e-06 9.96164380e-01 1.00511501e-01\n",
      "  9.53667120e-03 8.65524329e-01 9.99457492e-01 9.97888603e-01\n",
      "  6.26128306e-04 1.77762469e-04 9.96132785e-01 9.94990745e-01\n",
      "  1.22903328e-02 7.69527996e-03 8.89209625e-01 1.06993080e-03\n",
      "  9.99698804e-01 9.93346494e-01 9.99819118e-01 8.87933979e-04\n",
      "  9.99275226e-01 9.99815507e-01 5.43276631e-06 9.99733765e-01\n",
      "  9.99755051e-01 9.99779098e-01 9.98050523e-01 9.99045588e-01\n",
      "  9.99840464e-01 4.03371206e-01 1.92922066e-04 9.91627878e-01\n",
      "  3.34201681e-05 9.99886194e-01 5.85886934e-01 3.93483716e-02\n",
      "  9.74379318e-01 6.61225915e-01 5.62763662e-06 9.93487334e-01\n",
      "  2.71791016e-05 9.99625387e-01 9.89640180e-01 9.14687068e-02\n",
      "  9.99921276e-01 3.90438265e-01 9.99705728e-01 9.99786905e-01\n",
      "  9.66931330e-01 9.98261591e-01 6.88272729e-01 9.98936179e-01\n",
      "  3.76498689e-03 9.97259882e-01 9.91369923e-01 9.85253083e-01\n",
      "  9.99538154e-01 1.24797853e-04 5.28648890e-03 9.77903642e-01\n",
      "  9.98841893e-01 4.21385879e-01 9.51058642e-01 4.16696403e-04\n",
      "  9.99639253e-01 9.99663757e-01 1.05202219e-05 5.99197733e-01\n",
      "  6.53861555e-04 9.99222441e-01 7.94049648e-02 9.94947910e-01\n",
      "  1.14886523e-05 5.78549391e-04 9.99778087e-01 9.99955484e-01\n",
      "  9.91701737e-01 9.99513118e-01 9.86477241e-01 9.93478488e-01\n",
      "  9.87618655e-01 9.95599944e-01 9.66705215e-01 1.94803417e-02\n",
      "  9.85971362e-01 9.98980327e-01 1.14758778e-03 1.10544347e-03\n",
      "  5.68284118e-04 6.12946565e-06 6.29820669e-04 9.99938391e-01\n",
      "  7.71638299e-01 9.99451224e-01 9.99586845e-01 1.02109406e-03\n",
      "  3.53280091e-05 4.38743212e-04 9.99939817e-01 9.99761600e-01\n",
      "  9.93180913e-01 9.99806264e-01 1.34412534e-04 9.99298174e-01\n",
      "  1.82762696e-03 9.97866579e-01 9.99737127e-01 9.95731342e-01\n",
      "  1.20126044e-03 1.59425916e-03 9.99065110e-01 2.14271475e-04\n",
      "  1.52134671e-03 2.34598676e-03 9.97908363e-01 9.91807439e-01\n",
      "  7.69565398e-03 4.05523758e-01 9.99634392e-01 1.84107857e-04\n",
      "  9.99650665e-01 4.69604646e-02 2.46441973e-06 9.40097908e-01\n",
      "  6.96900236e-01 9.99545332e-01 5.21489726e-05 5.46315461e-05\n",
      "  9.92606666e-01 5.59661181e-04 9.98954749e-01 7.78056346e-04\n",
      "  9.87413191e-01 9.94500731e-06 7.45863860e-01 9.98386143e-01\n",
      "  3.94105173e-04 9.61910576e-02 5.93442552e-06 9.99740810e-01\n",
      "  1.30612166e-01 7.99684383e-01 2.23898210e-01 9.99617561e-01\n",
      "  6.30686933e-03 9.96737738e-01 2.55271197e-03 9.99905099e-01\n",
      "  9.99591197e-01 9.97782030e-01 9.97699017e-01 9.90408040e-01\n",
      "  9.98374060e-01 8.55030028e-05 9.99661385e-01 9.99966373e-01\n",
      "  9.79943471e-01 9.99398152e-01 7.29411658e-05 4.44450549e-05\n",
      "  9.96321999e-01 1.35314725e-03 1.55742035e-01 3.82632066e-03\n",
      "  9.99584479e-01 1.23432910e-05 1.60914281e-04 9.86523732e-01\n",
      "  9.99769745e-01 6.78614184e-01 3.96405282e-05 9.98791357e-01\n",
      "  1.40009453e-03 9.69934820e-01 4.74449584e-04 9.97157700e-01\n",
      "  2.67122645e-03 1.57526898e-05 9.99548101e-01 1.17394057e-03\n",
      "  9.98950292e-01 9.12082790e-02 2.36685338e-03 9.32995177e-01\n",
      "  3.45818114e-01 9.94219941e-01 8.05326649e-03 9.97721491e-01\n",
      "  6.82966370e-04 5.45016544e-02 9.84257151e-01 9.99628346e-01\n",
      "  8.05647992e-02 7.01099069e-01 9.99926644e-01 1.20002889e-04\n",
      "  9.98885140e-01 8.48820328e-01 9.98001231e-01 9.99835985e-01\n",
      "  9.99172595e-01 3.52970962e-04 1.17704908e-03 9.99715642e-01\n",
      "  9.98981019e-01 1.38018887e-03 9.96293516e-01 9.99952644e-01\n",
      "  9.97485342e-01 4.86164894e-01 2.58682710e-03 3.23228724e-02\n",
      "  1.14839092e-05 9.99116766e-01 9.87614833e-01 9.98596961e-01\n",
      "  9.99733835e-01 9.97961247e-01 8.69833796e-01 9.93139432e-01\n",
      "  9.99322571e-01 9.28154755e-01 9.66439926e-01 5.07447068e-01\n",
      "  8.43858720e-04 1.57629816e-04 9.94912773e-01 9.97952263e-01\n",
      "  6.61820051e-01 2.33296874e-04 9.99534350e-01 9.56837214e-01\n",
      "  9.97420578e-01 9.94469910e-01 2.21526200e-03 9.99728005e-01\n",
      "  9.71741943e-01 9.99203770e-01 9.95847406e-01 9.81632163e-01\n",
      "  9.52687877e-01 9.98510084e-01 2.27052741e-03 9.91846004e-01\n",
      "  2.24723887e-03 9.71357245e-01 9.58308047e-01 9.57581414e-01\n",
      "  9.95663468e-01 9.97709429e-01 9.97740553e-01 7.62636703e-05\n",
      "  9.97818582e-01 9.99932230e-01 9.83657883e-01 9.99197333e-01\n",
      "  4.79387378e-01 9.97763990e-01 9.99809494e-01 6.27749536e-01\n",
      "  9.92731136e-01 9.41694115e-01 5.57330865e-01 9.99930196e-01\n",
      "  4.92629935e-04 1.73667314e-05 8.44601272e-04 9.94727021e-01\n",
      "  9.86056628e-01 1.87525461e-01 9.99567601e-01 1.49018684e-05\n",
      "  9.99901736e-01 6.31544486e-05 9.96825500e-01 9.99857474e-01\n",
      "  6.17451166e-03 9.90999756e-01 6.67149279e-05 9.99545516e-01\n",
      "  9.99596049e-01 9.97155177e-01 9.99914271e-01 8.71041218e-01\n",
      "  9.90381301e-01 8.21383586e-01 9.92382864e-01 9.97608627e-01\n",
      "  9.97989804e-01 4.81426872e-05 9.60866878e-01 9.99901676e-01\n",
      "  3.49412030e-05 9.99403425e-01 7.84680961e-06 9.99773264e-01\n",
      "  3.06133874e-04 9.99050379e-01 9.96219431e-01 8.13876172e-01\n",
      "  1.31920615e-06 9.99976856e-01 9.99617049e-01 2.00610317e-01\n",
      "  1.35988364e-01 4.67048396e-02 5.41957501e-01 1.86796620e-01\n",
      "  9.85140134e-01 7.94334125e-05 9.99689844e-01 9.95225438e-01\n",
      "  9.08863789e-06 4.65391727e-05 9.98921906e-01 9.99690063e-01\n",
      "  9.98315051e-01 9.99595910e-01 9.99870321e-01 3.98471299e-03\n",
      "  6.79546767e-01 9.91386631e-01 1.11894164e-02 9.99512669e-01\n",
      "  4.65640433e-05 9.67953258e-01 9.85493551e-01 6.78044011e-01\n",
      "  8.62589919e-02 6.60509066e-06 9.99255881e-01 1.87364763e-05\n",
      "  9.99691283e-01 4.65982186e-02 5.97051300e-01 4.23240371e-06\n",
      "  9.26762242e-01 9.99043331e-01 1.30404897e-01 1.37747175e-02\n",
      "  1.82556642e-04 9.99934010e-01 9.99920596e-01 1.45465890e-01\n",
      "  9.99962932e-01 9.95544990e-01 9.98973533e-01 3.80136367e-06\n",
      "  6.72467484e-01 9.99146400e-01 1.42913756e-04 6.03518124e-01\n",
      "  8.71444046e-01 9.99867478e-01 6.53363788e-06 4.11725788e-03\n",
      "  1.50784912e-02 9.99927348e-01 9.95061733e-01 1.28316909e-03\n",
      "  3.01195316e-01 9.99797608e-01 4.69102393e-05 6.69340346e-04\n",
      "  9.99885607e-01 9.97529213e-01 8.60396711e-01 8.10964097e-04\n",
      "  9.99217481e-01 3.12352892e-01 9.11630976e-01 3.59691944e-03\n",
      "  2.17543472e-03 2.30204705e-01 9.99865053e-01 9.98159196e-01\n",
      "  9.99623652e-01 1.23671071e-01 9.99768562e-01 9.86465356e-06\n",
      "  8.52015535e-05 9.84208306e-01 9.98235357e-01 6.47104542e-01\n",
      "  9.96585018e-01 9.96977176e-01 9.99605738e-01 1.41152751e-01\n",
      "  9.59817932e-01 9.99716953e-01 8.96327367e-01 9.93308968e-01\n",
      "  9.08007272e-01 9.99439798e-01 9.98601687e-01 9.71821884e-01\n",
      "  9.99640957e-01 9.99794696e-01 9.41263754e-02 4.48113083e-02\n",
      "  8.86721368e-01 9.99780672e-01 9.98960935e-01 2.42622976e-04\n",
      "  5.11526495e-04 9.99700839e-01 9.99768870e-01 9.26979973e-01\n",
      "  9.99728374e-01 5.58435952e-01 9.99593947e-01 3.97586149e-02\n",
      "  9.88864805e-01 3.81192189e-04 4.19491750e-03 9.99603328e-01\n",
      "  9.44736195e-01 9.99371820e-01 2.81575179e-04 4.06137441e-02\n",
      "  9.81610779e-01 9.98091347e-01 9.98380278e-01 3.88300495e-03\n",
      "  3.47930897e-02 9.99424206e-01 1.75854872e-04 6.81871621e-05\n",
      "  4.19784422e-02 7.10454795e-01 9.98038096e-01 9.99818730e-01\n",
      "  9.99961739e-01 9.99831421e-01 9.92561544e-01 1.60032875e-03\n",
      "  9.98993036e-01 9.89704175e-01 9.38564763e-01 9.90599319e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "m = 1000\n",
    "X = X_train[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "y = y_train[n:m].values.reshape(1,m-n).astype(float)\n",
    "predictions = model.predict(X, y)\n",
    "#print(predictions.type)\n",
    "#print(y)\n",
    "print(predictions)\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT COST tf.Tensor(0.7142612934112549, shape=(), dtype=float64)\n",
      "Cost epoch  0 :  0.6940665632459494\n",
      "\n",
      "ALT COST tf.Tensor(0.6497443318367004, shape=(), dtype=float64)\n",
      "Cost epoch  1 :  0.5840913218143071\n",
      "\n",
      "ALT COST tf.Tensor(0.592679500579834, shape=(), dtype=float64)\n",
      "Cost epoch  2 :  0.3394713114595835\n",
      "\n",
      "ALT COST tf.Tensor(0.5629366040229797, shape=(), dtype=float64)\n",
      "Cost epoch  3 :  0.20405979355619652\n",
      "\n",
      "ALT COST tf.Tensor(0.532133936882019, shape=(), dtype=float64)\n",
      "Cost epoch  4 :  0.12614162924324812\n",
      "\n",
      "ALT COST tf.Tensor(0.5272680521011353, shape=(), dtype=float64)\n",
      "Cost epoch  5 :  0.08991339563562274\n",
      "\n",
      "ALT COST tf.Tensor(0.5159204602241516, shape=(), dtype=float64)\n",
      "Cost epoch  6 :  0.07079479584934406\n",
      "\n",
      "ALT COST tf.Tensor(0.5122106075286865, shape=(), dtype=float64)\n",
      "Cost epoch  7 :  0.050889096907715634\n",
      "\n",
      "ALT COST tf.Tensor(0.5089820623397827, shape=(), dtype=float64)\n",
      "Cost epoch  8 :  0.043277339519545495\n",
      "\n",
      "ALT COST tf.Tensor(0.5075609683990479, shape=(), dtype=float64)\n",
      "Cost epoch  9 :  0.03861985898051965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "np.random.seed(10)\n",
    "m = 1000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Define the layers of the model\n",
    "layers = [\n",
    "    Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "    #ReLU((None, 22, 22, 16)),\n",
    "    Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "    #Conv2D(32, 7, (None, 8, 11, 16)),\n",
    "    #ReLU((None, 8, 8, 32)),\n",
    "    #Maxpool((None, 3, 3, 16)),\n",
    "    Flatten((None, 8, 8, 32)),\n",
    "    Dense(32, (2048, None), \"relu\"),\n",
    "    Dense(1, (32, None), \"sigmoid\")\n",
    "    #knn_differentiable((64, None), 2)\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = Model(layers)\n",
    "history = model.fit(X, y, epochs=10, learning_rate=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.77207326e-01 9.94833097e-01 9.79809248e-05 9.97895028e-01\n",
      "  7.40929712e-05 1.24724993e-02 9.29602996e-01 8.97062191e-01\n",
      "  1.46400582e-02 9.73817108e-01 3.87985278e-02 9.89197861e-01\n",
      "  9.91299714e-01 3.40512292e-05 5.25271307e-05 9.73182202e-01\n",
      "  5.49985125e-02 9.78384162e-01 9.83781698e-01 3.21816606e-04\n",
      "  9.65557178e-01 9.56128453e-03 3.62392639e-05 7.24394553e-05\n",
      "  8.01361664e-01 9.93379140e-01 5.11446515e-04 1.08092491e-02\n",
      "  2.34868243e-05 9.32955984e-01 2.71818451e-03 1.12832052e-02\n",
      "  1.67413933e-03 8.71499702e-01 3.24737336e-05 4.76850096e-04\n",
      "  1.11995270e-03 3.79755283e-03 9.91219081e-01 9.64353398e-05\n",
      "  1.95889369e-03 1.79939381e-03 9.93425270e-01 9.94218885e-01\n",
      "  9.93874154e-01 9.67405856e-01 9.84409320e-01 1.48489755e-04\n",
      "  1.63816046e-04 2.44439352e-03 8.22548622e-05 1.59960185e-05\n",
      "  4.50116427e-04 7.48105174e-03 9.83351023e-01 9.97437988e-01\n",
      "  9.87832287e-01 9.98105418e-01 9.96103842e-01 5.67227076e-05\n",
      "  5.63560243e-05 9.58004898e-01 1.54702089e-03 2.55725244e-03\n",
      "  3.22981192e-04 2.63930394e-03 9.87227943e-01 1.07690382e-02\n",
      "  1.58722249e-03 9.88390561e-01 7.02109288e-04 9.95792375e-01\n",
      "  8.85574759e-01 3.94129700e-04 1.68756173e-04 5.80485072e-04\n",
      "  1.88535833e-05 9.78558782e-01 9.23925002e-01 9.46832423e-01\n",
      "  3.48809128e-05 9.91591162e-01 9.94154788e-01 4.59453506e-03\n",
      "  1.27339312e-02 3.21734425e-04 1.38275178e-03 1.50060160e-05\n",
      "  9.70708722e-04 9.99181241e-01 9.93100901e-01 6.95594284e-04\n",
      "  9.36944961e-05 1.27706677e-04 6.05466725e-05 9.99728112e-01\n",
      "  9.97078029e-01 3.43411580e-01 9.98462378e-01 9.99075943e-01\n",
      "  9.98138595e-01 1.16724429e-01 3.36674568e-04 9.96929732e-01\n",
      "  9.79355144e-01 2.93952207e-04 9.55417596e-01 3.47024001e-02\n",
      "  9.69949301e-01 8.96213961e-03 9.62871551e-01 9.96122824e-01\n",
      "  9.39335203e-03 4.97502893e-03 9.91700836e-04 2.75047036e-05\n",
      "  3.67717841e-06 9.94419944e-01 3.70528523e-03 8.93828688e-01\n",
      "  1.06104708e-04 7.76140070e-05 9.71562484e-01 8.83538610e-05\n",
      "  9.96297968e-01 5.51888086e-05 1.04579329e-02 9.60533635e-01\n",
      "  3.22048562e-02 4.88269566e-03 7.95543882e-01 1.33693525e-04\n",
      "  9.15733766e-01 6.06638903e-04 2.23083728e-05 1.16174640e-02\n",
      "  6.20548460e-04 9.91163132e-01 9.39998084e-01 8.98752252e-05\n",
      "  9.90506297e-01 1.31511544e-04 1.29791459e-01 9.79636362e-01\n",
      "  8.76925418e-01 4.20231765e-05 7.97685638e-03 2.46110703e-05\n",
      "  9.59250486e-01 9.93685070e-01 9.28012101e-01 9.84299390e-01\n",
      "  9.98596559e-01 9.97616481e-01 9.92409183e-01 9.84141651e-01\n",
      "  9.48092342e-01 9.43213117e-01 1.14916268e-02 4.39509101e-02\n",
      "  9.60552312e-01 9.77602717e-01 1.70061866e-01 3.30593846e-05\n",
      "  1.97187749e-03 9.76564761e-01 1.19135737e-01 9.81263893e-01\n",
      "  9.49614638e-01 9.99704144e-01 9.93378287e-01 1.19753916e-02\n",
      "  4.25551406e-05 3.63760485e-04 9.98959158e-01 9.15294426e-01\n",
      "  3.53258486e-03 9.70166433e-02 2.98100527e-01 1.30582962e-05\n",
      "  9.86054088e-01 9.92449775e-01 4.35125601e-04 1.26589072e-04\n",
      "  9.94419680e-01 1.80687956e-04 9.00012814e-05 9.86574967e-01\n",
      "  1.89157693e-05 9.96809484e-01 7.13363810e-03 9.72944594e-01\n",
      "  3.88771737e-03 9.93967462e-01 1.55643265e-03 9.44537054e-01\n",
      "  9.83893381e-01 1.29218308e-01 9.49072557e-01 9.90332921e-01\n",
      "  9.93648543e-01 9.01849149e-01 1.36275931e-02 9.93760481e-01\n",
      "  9.04340516e-01 7.21119560e-05 9.96424536e-01 6.50744203e-04\n",
      "  2.63826669e-05 9.95258453e-01 1.45009125e-02 9.98729812e-01\n",
      "  8.94591094e-01 9.73447569e-01 9.95917731e-01 9.48588900e-01\n",
      "  9.80968820e-01 8.52204194e-01 9.80599842e-03 9.93547248e-01\n",
      "  1.34207596e-03 9.78088126e-01 7.18111556e-02 9.92828879e-01\n",
      "  3.02617060e-05 7.64624545e-02 9.27259539e-01 9.06854691e-01\n",
      "  9.34709392e-01 9.46801071e-01 9.94410165e-01 7.28709772e-03\n",
      "  2.74669890e-03 6.53511096e-03 9.76160597e-01 9.90680631e-01\n",
      "  3.63192697e-04 4.97786464e-02 2.06531159e-03 9.90922759e-01\n",
      "  8.73207999e-03 9.49436530e-01 9.73315436e-01 9.91999580e-01\n",
      "  3.68537839e-03 1.29613912e-04 3.75840559e-05 9.90879354e-01\n",
      "  1.65815771e-04 1.71324728e-05 9.94736135e-01 9.71856688e-01\n",
      "  8.38786883e-04 3.87064478e-03 9.96554284e-01 9.40264066e-01\n",
      "  9.94135664e-01 9.24130384e-03 2.44631757e-05 1.00532203e-04\n",
      "  9.80460072e-01 9.55900058e-01 9.85692524e-01 9.88595775e-01\n",
      "  6.94194419e-04 8.47960583e-04 9.32749728e-01 8.67580516e-06\n",
      "  1.07855322e-05 1.15877870e-03 9.99142606e-01 2.94548793e-05\n",
      "  9.84917578e-01 3.69032566e-01 6.22580148e-05 9.81967148e-01\n",
      "  9.90938581e-01 5.45103796e-03 9.66301214e-01 9.87676189e-01\n",
      "  8.94190367e-01 2.79812929e-05 9.82091242e-01 9.95256948e-01\n",
      "  7.50359784e-01 9.99050288e-01 9.77648380e-01 9.74518258e-01\n",
      "  1.25268270e-04 2.13292179e-05 8.70665375e-06 9.95694115e-01\n",
      "  8.36404483e-01 5.53564055e-05 9.58989583e-01 9.95603457e-01\n",
      "  9.45021580e-03 4.24973918e-02 4.44516799e-03 9.83985125e-01\n",
      "  9.87706433e-01 6.50037943e-02 9.89281484e-01 1.97903456e-02\n",
      "  9.84129364e-01 9.90665082e-01 9.54137088e-01 7.51131914e-04\n",
      "  3.70335249e-04 9.69930259e-01 1.69421716e-03 9.99789813e-01\n",
      "  9.06746825e-01 2.74842789e-03 1.54011317e-04 9.99390873e-01\n",
      "  9.98646109e-01 3.28011138e-02 9.80026197e-01 1.32213249e-03\n",
      "  9.38590075e-01 2.70209910e-01 9.05911484e-01 7.08781886e-04\n",
      "  9.98561948e-01 7.49795011e-05 9.97946348e-01 9.70341269e-01\n",
      "  9.90049890e-01 9.98720164e-01 8.03807848e-05 9.30108586e-01\n",
      "  9.54739145e-01 4.57379639e-05 8.49226881e-01 7.20809047e-05\n",
      "  7.10032251e-02 7.56779765e-01 1.55489082e-01 4.28874594e-02\n",
      "  7.68938080e-02 9.40245817e-01 4.27323001e-05 9.70246241e-01\n",
      "  9.94208117e-01 4.41607043e-04 9.91687547e-01 9.83631829e-01\n",
      "  9.41248430e-01 3.78241601e-05 9.95344793e-01 5.53795872e-03\n",
      "  9.99026863e-01 9.94762062e-01 9.92203494e-01 2.12584722e-05\n",
      "  2.40790759e-03 5.92004263e-01 9.99611888e-01 9.90748967e-01\n",
      "  9.54111386e-01 9.64976068e-01 3.16183093e-05 3.28405053e-04\n",
      "  1.00903906e-02 7.62226409e-03 9.94170832e-01 1.46169109e-02\n",
      "  2.19088105e-04 2.01604433e-04 1.72207445e-03 9.55789813e-03\n",
      "  8.00566599e-03 9.93073578e-01 9.77105105e-01 2.29127182e-03\n",
      "  7.89618861e-04 4.79447733e-03 5.67602009e-02 9.97297193e-01\n",
      "  9.95626828e-01 2.36228558e-02 9.32945637e-01 9.99968945e-01\n",
      "  2.18408410e-04 6.26595843e-05 9.75220409e-01 9.85372123e-01\n",
      "  4.38901487e-04 1.16552279e-04 5.96149084e-05 9.89062405e-01\n",
      "  1.61779169e-03 9.43842971e-01 1.06222299e-03 9.10672869e-01\n",
      "  9.86237613e-01 9.19291157e-01 8.24948659e-03 9.98171514e-01\n",
      "  1.98759199e-05 2.58109836e-03 9.55813270e-01 9.89712724e-01\n",
      "  9.71916871e-04 4.05047517e-04 9.98132854e-01 2.60201012e-03\n",
      "  9.54491091e-01 9.94168481e-01 1.34795404e-04 9.46085980e-01\n",
      "  9.89984274e-01 9.75531124e-01 4.66982431e-04 1.65688748e-02\n",
      "  1.94053127e-04 1.11567846e-02 9.57982529e-01 1.00090138e-03\n",
      "  9.31943761e-01 9.64815001e-01 9.76569908e-01 9.94533691e-01\n",
      "  9.80810742e-01 8.86409240e-01 5.34867513e-05 9.83481645e-01\n",
      "  1.28438673e-05 8.75237420e-01 9.74727428e-01 2.18196482e-01\n",
      "  5.50913199e-01 1.06494835e-04 9.67554559e-01 2.49202507e-03\n",
      "  9.67589424e-01 9.80660554e-01 9.96186526e-01 1.23148656e-04\n",
      "  3.14992058e-03 9.99519246e-01 9.82996097e-01 2.53484284e-04\n",
      "  9.66289357e-01 1.39281864e-02 7.71890806e-05 1.41364429e-01\n",
      "  8.68872571e-06 9.94941210e-01 5.23246967e-04 9.69194643e-01\n",
      "  1.42286702e-01 9.83952791e-01 2.55093367e-03 9.95895738e-01\n",
      "  9.85217088e-01 7.26545643e-03 9.66032505e-01 6.33615373e-05\n",
      "  1.78567567e-02 9.19771947e-01 1.22385126e-02 7.10700412e-06\n",
      "  3.41009506e-03 7.14910130e-03 9.86697259e-01 9.74913033e-01\n",
      "  8.34022167e-06 9.96367718e-01 1.72062487e-02 9.97174811e-01\n",
      "  9.78766434e-01 4.31074356e-02 2.42582718e-05 4.99890379e-05\n",
      "  9.86755759e-01 2.36867924e-02 9.76346763e-06 9.94497458e-01\n",
      "  9.81935811e-01 2.21469163e-05 1.00814165e-04 9.97552195e-01\n",
      "  2.95880563e-03 7.88647819e-04 9.41386925e-01 1.05759433e-03\n",
      "  9.91897024e-01 9.98484730e-01 9.94332451e-01 3.47309249e-05\n",
      "  6.56268414e-04 9.91302670e-01 9.69925020e-01 1.04644168e-01\n",
      "  9.10479838e-03 9.96115271e-01 5.87381962e-04 9.54252640e-01\n",
      "  9.95831318e-01 1.90543870e-03 3.32306060e-04 9.46563561e-01\n",
      "  1.34870916e-04 9.68790856e-01 9.71946045e-01 9.57837539e-01\n",
      "  9.32906714e-01 7.40463761e-02 9.98561825e-01 4.14511690e-02\n",
      "  9.99314672e-01 4.74398383e-06 2.45164367e-05 3.94445631e-04\n",
      "  9.82263926e-01 9.90127649e-01 9.74472219e-01 3.75862882e-02\n",
      "  1.48403118e-05 5.60932141e-05 1.47680101e-04 5.78126356e-02\n",
      "  5.40956401e-02 1.84030475e-04 2.49048968e-04 9.42129726e-01\n",
      "  9.98483169e-01 3.90915677e-05 1.78408980e-06 5.68011294e-05\n",
      "  9.48477275e-01 9.57227023e-01 9.61903731e-01 3.27578685e-03\n",
      "  9.84986389e-01 1.82438264e-01 1.03174979e-05 9.99779096e-01\n",
      "  1.78416676e-04 9.38296667e-01 9.62912276e-01 3.07701995e-03\n",
      "  1.04945998e-04 9.96613548e-01 8.94417989e-06 9.96435445e-01\n",
      "  5.48526364e-05 8.30255323e-03 5.58415725e-05 9.99233493e-01\n",
      "  1.51794875e-05 9.31935626e-01 1.55848022e-03 9.58810768e-01\n",
      "  4.84527001e-02 6.58485888e-05 3.84383756e-03 9.85680725e-01\n",
      "  9.83888420e-01 9.74264167e-01 5.19241822e-04 4.84022517e-01\n",
      "  9.82243513e-01 2.21691855e-03 1.42734336e-04 9.78914853e-01\n",
      "  9.94299279e-01 2.29945978e-01 2.33049629e-03 8.04606975e-01\n",
      "  9.98134763e-01 3.14673196e-05 2.78884007e-05 9.26734007e-03\n",
      "  9.96811042e-01 5.30780076e-02 1.20544425e-04 7.39762810e-01\n",
      "  9.87938181e-01 1.12611207e-03 1.91302917e-05 3.96450675e-04\n",
      "  1.16649350e-04 9.93081634e-01 2.92563455e-02 4.53415361e-04\n",
      "  9.56667081e-01 1.01358326e-01 9.97993119e-01 4.33049499e-05\n",
      "  9.95060160e-01 9.70316506e-01 5.60374098e-04 8.33923244e-01\n",
      "  9.97599479e-01 9.93763216e-01 5.64643127e-03 9.97843281e-01\n",
      "  9.71727731e-01 9.97711296e-01 9.96877243e-01 2.43068358e-01\n",
      "  2.54091667e-05 3.00534566e-04 8.16824580e-02 3.86879496e-04\n",
      "  2.08256350e-03 3.40230341e-03 5.80718738e-04 8.33372569e-05\n",
      "  9.94509865e-01 7.44706052e-01 9.59730950e-01 9.71682202e-01\n",
      "  9.98209263e-01 2.02118123e-04 5.65558929e-03 9.93400771e-01\n",
      "  9.93689358e-01 1.37543078e-02 8.23620871e-04 9.98747178e-01\n",
      "  9.85914860e-01 8.99484184e-01 9.87176034e-01 1.07908837e-04\n",
      "  1.41405094e-05 9.89002175e-01 2.66032354e-01 1.35092128e-02\n",
      "  2.54363912e-05 8.15618216e-01 9.99033152e-01 9.87923554e-01\n",
      "  2.45072878e-04 2.82750625e-03 9.02087514e-01 9.93208589e-01\n",
      "  8.88874973e-05 9.90077906e-01 2.00059196e-02 9.96564787e-01\n",
      "  7.18719514e-05 7.32619154e-03 4.71157084e-05 2.96410866e-03\n",
      "  6.94749159e-05 4.32223239e-01 9.99642661e-01 1.71711447e-04\n",
      "  2.26536466e-02 8.97025318e-03 5.86915453e-05 2.38518173e-01\n",
      "  9.71431994e-01 3.78766903e-02 9.81809968e-01 1.75614796e-02\n",
      "  9.62652847e-01 9.89758403e-01 9.84933677e-01 1.29146301e-03\n",
      "  2.80333571e-04 9.06626124e-01 9.28239578e-05 9.50021331e-01\n",
      "  9.87741703e-01 8.57166199e-01 9.98340659e-01 9.99078166e-01\n",
      "  9.88748190e-01 9.85036385e-01 9.22782490e-01 2.10270062e-05\n",
      "  6.08380530e-01 3.68342600e-01 1.13646100e-03 2.04094890e-04\n",
      "  9.81323596e-01 9.41475197e-01 1.09706561e-01 9.94549130e-01\n",
      "  1.13971260e-01 3.22112275e-03 9.99237049e-01 8.78737422e-01\n",
      "  9.47793726e-01 1.56144696e-02 2.62064152e-03 9.69758013e-01\n",
      "  9.99233333e-01 9.59795181e-01 9.92191323e-01 1.95942706e-04\n",
      "  1.67820608e-02 4.63262390e-04 9.86425418e-01 7.64468410e-03\n",
      "  9.79051640e-01 9.98573157e-04 5.97442204e-05 9.92108120e-01\n",
      "  1.46684956e-04 9.84970155e-01 9.88341987e-01 3.94950691e-04\n",
      "  9.10248306e-01 9.07810080e-01 1.36439219e-05 1.08965080e-04\n",
      "  9.90864085e-01 9.49451244e-01 3.79394744e-05 9.89686585e-01\n",
      "  4.24437953e-01 7.25467791e-05 9.68330578e-01 1.31533788e-03\n",
      "  5.11542795e-05 3.69418158e-03 5.08351655e-02 9.99666018e-01\n",
      "  9.94085480e-01 9.20663467e-01 2.10243144e-05 9.98322027e-01\n",
      "  6.88775072e-03 9.64851751e-01 9.94755219e-01 9.31004139e-01\n",
      "  8.99757368e-01 9.80201197e-01 9.97921442e-01 9.08081796e-01\n",
      "  4.06296031e-04 9.99118352e-01 1.91505708e-04 3.03504886e-01\n",
      "  9.95805016e-01 1.19875437e-04 1.35954738e-02 1.18173156e-05\n",
      "  4.10627327e-05 8.20535726e-02 1.24640108e-03 8.18176988e-05\n",
      "  3.21359178e-05 4.88161584e-03 9.98170052e-01 7.84953055e-04\n",
      "  1.07049932e-03 9.46793034e-01 2.28560610e-04 3.70657109e-03\n",
      "  9.72941134e-01 1.50918605e-03 9.98486334e-01 2.87929053e-03\n",
      "  9.83927908e-01 9.91823757e-01 7.00082958e-03 9.47441147e-01\n",
      "  9.98159812e-01 9.76202805e-01 9.97636145e-01 9.53074396e-01\n",
      "  2.09124387e-03 2.18620788e-04 9.33379149e-01 8.97187193e-01\n",
      "  9.75920583e-01 9.94794980e-03 3.87498293e-03 1.65298576e-02\n",
      "  1.12159911e-02 9.17322635e-04 1.64256281e-04 9.55847914e-01\n",
      "  5.02774057e-01 5.44805134e-03 9.68583010e-01 9.78228522e-01\n",
      "  9.61782399e-01 9.70691321e-01 9.88792373e-01 1.06073081e-03\n",
      "  8.07197847e-01 1.67853363e-04 1.01938938e-02 2.66691248e-05\n",
      "  9.72684045e-01 8.76542585e-02 4.77810384e-05 7.77361857e-03\n",
      "  9.93958656e-01 1.30003571e-01 3.27123495e-05 7.84035307e-06\n",
      "  9.98896393e-01 8.76744070e-04 9.56296689e-01 9.63211226e-01\n",
      "  9.78532048e-01 4.60821505e-05 9.76404736e-01 7.45214724e-04\n",
      "  9.58438497e-01 9.97474775e-01 9.95367434e-01 8.10400715e-05\n",
      "  2.05473891e-05 6.58292267e-04 9.79094395e-01 7.65131776e-03\n",
      "  6.10704506e-05 9.61607379e-01 9.71780029e-01 2.15699886e-02\n",
      "  7.31563078e-05 5.27313256e-05 9.99552139e-01 8.97050520e-02\n",
      "  1.70514517e-03 9.98107866e-01 9.75571939e-01 2.21062610e-02\n",
      "  9.68565540e-01 1.52179117e-03 9.91198788e-01 9.58860711e-01\n",
      "  9.95314248e-01 9.86276597e-01 9.44555704e-01 3.33653035e-04\n",
      "  2.17960509e-02 1.70386208e-04 1.33992209e-04 9.99486375e-01\n",
      "  9.98772229e-01 7.57150395e-06 1.14478278e-04 2.11242289e-04\n",
      "  5.87471133e-03 9.71500515e-01 1.62132694e-05 9.60780587e-01\n",
      "  9.74556426e-01 9.93753748e-01 4.54206643e-04 3.49952206e-04\n",
      "  9.99381691e-01 2.57983934e-04 7.01981063e-04 3.19680091e-04\n",
      "  5.33835966e-05 9.95508236e-05 1.54901425e-03 6.83441150e-05\n",
      "  9.93861183e-01 2.62931945e-05 9.96296930e-01 9.45541034e-01\n",
      "  8.48226047e-04 9.75369527e-01 9.32946445e-01 1.00581180e-02\n",
      "  9.97167968e-01 9.99136448e-01 9.77587577e-01 9.98135948e-01\n",
      "  9.79119226e-01 9.25391307e-01 9.97852573e-01 9.54366911e-01\n",
      "  9.96077988e-01 9.97586941e-01 2.99359977e-05 4.04333728e-05\n",
      "  9.39823253e-01 4.12781772e-04 9.29548257e-01 3.50841644e-03\n",
      "  9.68799724e-01 2.08915619e-01 1.26177739e-04 1.82384896e-05\n",
      "  9.76418760e-01 9.86274899e-01 1.78055479e-04 1.10848517e-02\n",
      "  6.71689459e-04 2.65857440e-05 2.63059122e-03 9.97454145e-01\n",
      "  2.37144319e-01 1.41471265e-02 2.55051962e-04 1.02783273e-03\n",
      "  9.56700532e-01 3.91232566e-05 9.78861327e-01 9.95483861e-01\n",
      "  9.95211589e-01 1.00035309e-03 9.95466070e-01 8.71491520e-01\n",
      "  8.55232922e-05 9.98624490e-01 8.00628420e-06 9.96904628e-01\n",
      "  9.95717418e-01 1.51900604e-02 8.47631568e-03 7.49798299e-05\n",
      "  9.89454183e-01 9.69475647e-01 9.60474077e-01 9.93589631e-01\n",
      "  6.96253225e-05 9.97475066e-01 5.92158978e-03 2.28696147e-01\n",
      "  2.70504302e-03 9.80022002e-01 9.99264745e-01 5.71422718e-03\n",
      "  3.75097274e-04 9.66471235e-01 9.90085156e-01 9.30616733e-05\n",
      "  2.41394942e-04 9.91076235e-01 9.93775511e-01 9.88800365e-02\n",
      "  9.41194235e-01 9.97096093e-01 2.04412986e-02 9.89774715e-01\n",
      "  9.89078224e-01 9.94695686e-01 1.78496438e-04 9.90689113e-01\n",
      "  2.92902157e-04 5.69708223e-05 8.65099435e-01 1.20610133e-05\n",
      "  9.86633355e-01 9.87506850e-01 9.95786270e-01 2.32896270e-02\n",
      "  9.71085206e-01 9.41982792e-01 4.51053028e-05 9.71512675e-01\n",
      "  2.70097776e-04 9.99442675e-01 9.08272323e-01 9.93166208e-01\n",
      "  3.08547863e-05 9.26111388e-01 8.87777742e-01 4.71383469e-04\n",
      "  9.28122441e-04 9.93133106e-01 9.70717977e-01 9.75368938e-01\n",
      "  9.92183749e-01 3.76857058e-04 9.87344760e-01 7.32429406e-04\n",
      "  5.54668081e-05 2.71747368e-02 2.43446548e-05 9.99689533e-01\n",
      "  9.55476230e-01 9.99681456e-01 1.04995352e-03 2.49551057e-04\n",
      "  2.76952602e-03 6.74665176e-03 9.71743269e-01 1.43522322e-02\n",
      "  9.50383581e-01 1.26404799e-05 9.95399188e-01 9.94142311e-01\n",
      "  1.46587828e-04 1.39397709e-01 9.51951814e-01 6.65644511e-03\n",
      "  1.87788540e-05 9.45632647e-01 9.46321096e-05 9.88860832e-01\n",
      "  3.44369191e-01 9.96879902e-01 9.98334195e-01 2.72634249e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "m = 1000\n",
    "X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "predictions = model.predict(X, y)\n",
    "#print(predictions.type)\n",
    "#print(y)\n",
    "print(predictions)\n",
    "accuracy_score(y.flatten(), predictions.flatten().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
