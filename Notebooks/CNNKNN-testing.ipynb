{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5        0\n",
      "7        0\n",
      "19       1\n",
      "22       1\n",
      "24       1\n",
      "        ..\n",
      "59982    1\n",
      "59984    1\n",
      "59986    1\n",
      "59990    1\n",
      "59993    0\n",
      "Name: training targets, Length: 12000, dtype: uint8\n",
      "(12000, 28, 28) (2000, 28, 28)\n",
      "(12000,) (2000,)\n",
      "\n",
      "(6000,) (6000,)\n",
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "MNIST = k.datasets.fashion_mnist.load_data()\n",
    "#MNIST = k.datasets.mnist.load_data()\n",
    "# Seperate dataset\n",
    "training = MNIST[0]\n",
    "X_train = training[0]\n",
    "y_train = pd.Series(training[1], name=\"training targets\")\n",
    "testing = MNIST[1]\n",
    "X_test = testing[0]\n",
    "y_test = pd.Series(testing[1], name=\"testing targets\")\n",
    "# Keep only 1s and 0s for binary classification problem\n",
    "y_train = y_train[(y_train == 2) | (y_train == 4)]\n",
    "X_train = X_train[y_train.index]\n",
    "y_test = y_test[(y_test == 2) | (y_test == 4)]\n",
    "X_test = X_test[y_test.index]\n",
    "\n",
    "y_train[y_train==2] = 0\n",
    "y_test[y_test==2] = 0\n",
    "X_train[X_train==2] = 0\n",
    "X_test[X_test==2] = 0\n",
    "\n",
    "y_train[y_train==4] = 1\n",
    "y_test[y_test==4] = 1\n",
    "X_train[X_train==4] = 1\n",
    "X_test[X_test==4] = 1\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print()\n",
    "print(y_train[y_train == 0].shape, y_train[y_train == 1].shape)\n",
    "print(y_test[y_test == 0].shape, y_test[y_test == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(16,6))\n",
    "for i in range(24):\n",
    "    fig = plt.subplot(3, 8, i+1)\n",
    "    fig.set_axis_off()\n",
    "    plt.imshow(X_train[i+1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 65,921\n",
      "Trainable params: 65,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = \"random_normal\" # random_normal or glorot_uniform\n",
    "keras_model = k.Sequential([ \n",
    "    k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.MaxPooling2D((3,3)),\n",
    "    #k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    k.layers.Flatten(),\n",
    "    k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "])\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.1952\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0755\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0583\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0508\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0465\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0434\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0411\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0391\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0371\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0360\n"
     ]
    }
   ],
   "source": [
    "# Select only m samples for fast training time during debugging\n",
    "m = 12000\n",
    "X = X_train[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_train[:m].values.reshape(1,m)\n",
    "# Compile model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "# Train model\n",
    "history = keras_model.fit(x=X, y=y.flatten(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 2000\n",
    "X = X_test[:m, :, :].reshape((m, 28, 28, 1))\n",
    "y = y_test[:m].values.reshape(1,m)\n",
    "\n",
    "predictions = keras_model.predict_classes(X)\n",
    "accuracy_score(predictions, y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = tf.raw_ops.Sigmoid(x=Z).numpy()\n",
    "    cache = A\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = tf.raw_ops.Relu(features=Z).numpy()\n",
    "    \n",
    "    cache = A\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = tf.raw_ops.ReluGrad(gradients=dA, features=Z).numpy()\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    A = cache\n",
    "    dZ = tf.raw_ops.SigmoidGrad(y=A, dy=dA).numpy()\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    logprods = np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)\n",
    "    cost = -1/m*np.sum(logprods)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Interface for layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple, trainable=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + \" \" + str(self.output_shape)\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, input_shape: tuple, activation: str):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        neurons (N) -- number of neurons\n",
    "        input_shape -- (N_prev, m)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \"\"\"\n",
    "        output_shape = (neurons, input_shape[1])\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.initialize_params()\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (N, N_prev)\n",
    "        self.b -- Biases, numpy array of shape (N, 1)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.neurons, self.input_shape[0]) * 0.05\n",
    "        self.b = np.zeros((self.neurons,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        A -- the output of the activation function, also called the post-activation value \n",
    "        \n",
    "        Defintions:\n",
    "        self.cache -- tuple of values (A_prev, activation_cache) stored for computing backward propagation efficiently\n",
    "\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif self.activation == \"relu\":\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (self.W.shape[0], A_prev.shape[1]))\n",
    "        self.cache = (A_prev, activation_cache)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for the dense layer with activation function\n",
    "\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient for current layer l \n",
    "       \n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        \n",
    "        Definitions:\n",
    "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, activation_cache = self.cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "        self.dZ = dZ \n",
    "        self.dW = np.dot(dZ, A_prev.T)/m\n",
    "        self.db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "\n",
    "        \n",
    "# class Conv2D(Layer):\n",
    "#     def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "#         \"\"\"\n",
    "#         Constructor for Conv2D layer.\n",
    "        \n",
    "#         Arguments:\n",
    "#         filters (C) -- number of filters\n",
    "#         filter_size (f) -- size of filters\n",
    "#         input_shape -- (m, H, W, C)\n",
    "#         \"\"\"\n",
    "#         output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "#         super().__init__(input_shape, output_shape)\n",
    "#         self.filters = filters\n",
    "#         self.filter_size = filter_size\n",
    "#         self.padding = padding\n",
    "#         self.stride = stride\n",
    "#         self.initialize_params()\n",
    "    \n",
    "#     def initialize_params(self):\n",
    "#         '''\n",
    "#         Definitions:\n",
    "#         self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "#         self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "#         '''\n",
    "#         self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.001\n",
    "#         self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "#     def forward(self, A_prev):\n",
    "#         \"\"\"\n",
    "#         Implements the forward propagation for a convolution function\n",
    "\n",
    "#         Arguments:\n",
    "#         A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "#         Returns:\n",
    "#         Z -- conv output\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Perform convolution\n",
    "#         Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "#         # Add bias\n",
    "#         Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "#         # Save information in \"cache\" for the backprop\n",
    "#         self.cache = A_prev\n",
    "#         # Return the output\n",
    "#         return Z.numpy()\n",
    "    \n",
    "    \n",
    "#     def backward(self, dZ):\n",
    "#         \"\"\"\n",
    "#         Implement the backward propagation for a convolution function\n",
    "        \n",
    "#         Arguments:\n",
    "#         dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "#         Returns:\n",
    "#         dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "#                    numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "#         Definitions:\n",
    "#         self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "#               numpy array of shape (f, f, C_prev, C)\n",
    "#         self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "#               numpy array of shape (1, 1, 1, C)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Retrieve information from \"cache\"\n",
    "#         A_prev = self.cache\n",
    "        \n",
    "#         dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "#         self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "#         self.db = tf.raw_ops.BiasAddGrad(out_backprop=dZ).numpy()\n",
    "#         return dA_prev\n",
    "    \n",
    "       \n",
    "#     def update_params(self, learning_rate):\n",
    "#         self.W = self.W-learning_rate*self.dW\n",
    "#         self.b = self.b-learning_rate*self.db\n",
    "    \n",
    "    \n",
    "#     def backward(self, dZ):\n",
    "#         \"\"\"\n",
    "#         Implement the backward propagation for a convolution function\n",
    "        \n",
    "#         Arguments:\n",
    "#         dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "#         Returns:\n",
    "#         dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "#                    numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "#         Definitions:\n",
    "#         self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "#               numpy array of shape (f, f, C_prev, C)\n",
    "#         self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "#               numpy array of shape (1, 1, 1, C)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Retrieve information from \"cache\"\n",
    "#         A_prev = self.cache\n",
    "#         m = A_prev.shape[0]\n",
    "#         self.dZ = dZ\n",
    "#         self.A_prev = A_prev\n",
    "#         dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "#         self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()/m\n",
    "#         self.db = np.average(np.sum(dZ, axis=(1,2)), axis=0)\n",
    "#         return dA_prev\n",
    "    \n",
    "       \n",
    "#     def update_params(self, learning_rate):\n",
    "#         self.W = self.W-learning_rate*self.dW\n",
    "#         self.b = self.b-learning_rate*self.db\n",
    "      \n",
    "    \n",
    "# Multiple layer conv   \n",
    "    \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters: int, filter_size: int, input_shape: tuple, padding=\"VALID\", stride=1):\n",
    "        \"\"\"\n",
    "        Constructor for Conv2D layer.\n",
    "        \n",
    "        Arguments:\n",
    "        filters (C) -- number of filters\n",
    "        filter_size (f) -- size of filters\n",
    "        input_shape -- (m, H, W, C)\n",
    "        \"\"\"\n",
    "        output_shape = (input_shape[0], input_shape[1] - filter_size + 1, input_shape[2] - filter_size + 1, filters)\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.initialize_params()\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        '''\n",
    "        Definitions:\n",
    "        self.W -- Weights, numpy array of shape (f, f, C_prev, n_C)\n",
    "        self.b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.input_shape[3], self.filters) * 0.001\n",
    "        self.b = np.zeros((self.filters))\n",
    "        \n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "        \n",
    "        Returns:\n",
    "        Z -- conv output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform convolution\n",
    "        Z = tf.raw_ops.Conv2D(input=A_prev, filter=self.W, strides=[self.stride]*4, padding=self.padding)\n",
    "        # Add bias\n",
    "        Z = tf.raw_ops.BiasAdd(value=Z, bias=self.b)\n",
    "        \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        self.cache = A_prev\n",
    "        # Return the output\n",
    "        return Z.numpy()\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, H, W, C)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "                   \n",
    "        Definitions:\n",
    "        self.dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, C_prev, C)\n",
    "        self.db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, C)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        A_prev = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "        self.dZ = dZ\n",
    "        self.A_prev = A_prev\n",
    "        dA_prev = tf.raw_ops.Conv2DBackpropInput(input_sizes = A_prev.shape, filter = self.W, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()\n",
    "        self.dW = tf.raw_ops.Conv2DBackpropFilter(input = A_prev, filter_sizes = self.W.shape, out_backprop = dZ, strides=[self.stride]*4, padding=self.padding).numpy()/m\n",
    "        self.db = np.average(np.sum(dZ, axis=(1,2)), axis=0)\n",
    "        return dA_prev\n",
    "    \n",
    "       \n",
    "    def update_params(self, learning_rate):\n",
    "        self.W = self.W-learning_rate*self.dW\n",
    "        self.b = self.b-learning_rate*self.db\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.ksize = [1, pool_size, pool_size, 1]\n",
    "        self.strides = [1, pool_size, pool_size, 1]\n",
    "        output_shape = (input_shape[0], input_shape[1]//pool_size, input_shape[2]//pool_size, input_shape[3])\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        Z = tf.raw_ops.MaxPool(input=A_prev, ksize=self.ksize, strides=self.strides, data_format='NHWC', padding=\"VALID\").numpy()\n",
    "        self.cache = (A_prev, Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev, Z = self.cache\n",
    "        dA_prev = tf.raw_ops.MaxPoolGrad(orig_input=A_prev, orig_output=Z, grad=dZ, ksize=self.ksize, strides=self.strides, padding=\"VALID\", data_format='NHWC').numpy()\n",
    "        return dA_prev\n",
    "\n",
    "        \n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        output_shape = input_shape\n",
    "        super().__init__(input_shape, output_shape, False)\n",
    "\n",
    "      \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "        Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "        Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "\n",
    "        A = tf.raw_ops.Relu(features=Z).numpy()\n",
    "        self.cache = Z \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "        Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.cache\n",
    "        dZ = tf.raw_ops.ReluGrad(gradients=dA, features=Z).numpy()\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        m, *shape = input_shape\n",
    "        output_shape = (np.prod(shape), m)\n",
    "        super().__init__(input_shape, output_shape, trainable=False)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, *shape = A_prev.shape\n",
    "        self.cache = A_prev.shape\n",
    "        return A_prev.flatten().reshape(m,np.prod(shape)).T\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ.T.reshape(self.cache)\n",
    "    \n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, batch_size, verbose): \n",
    "        # Initialize parameters\n",
    "        history = list()\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X, Y.T))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size, drop_remainder=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "#                 print(\"\\nBATCH SIZE\", y_batch.numpy().T.shape,end=\"\\n\")\n",
    "                y_batch = y_batch.numpy().T\n",
    "                # FORWARD PROP\n",
    "                Z = x_batch\n",
    "                for layer in self.layers:    \n",
    "                    if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                        Z = layer.forward(Z, y_batch)\n",
    "                    else:\n",
    "                        Z = layer.forward(Z)\n",
    "                    #print(layer, Z.shape)\n",
    "\n",
    "                # COST FUNCTION\n",
    "                cost = compute_cost(Z, y_batch)\n",
    "                history.append(cost)\n",
    "                if verbose == 1:\n",
    "                    print(\"Step {:.0f} in epoch {:.0f} - Cost: {:.8f}\\r\".format(step, epoch, cost), end=\"\")\n",
    "\n",
    "                # BACKWARD PROP\n",
    "                m = y_batch.shape[1]\n",
    "                dA = - (1/m)*(np.divide(y_batch, Z) - np.divide(1 - y_batch, 1 - Z)) # derivative of cost with respect to Z\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    dA = layer.backward(dA)\n",
    "\n",
    "                # UPDATE PARAMS\n",
    "                for layer in self.layers:\n",
    "                    layer.update_params(learning_rate)\n",
    "            print(\"\\n\\n\", \"=\"*75, \"\\n\")\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            if layer.__str__().split()[0] == \"knn_differentiable\":\n",
    "                Z = layer.forward(Z, Y)\n",
    "            else:\n",
    "                Z = layer.forward(Z)\n",
    "        return Z\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"-\"*25)\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    def _cost(self, X, Y):\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            Z = prop_layer.forward(Z)\n",
    "        # COMPUTE COST\n",
    "        return compute_cost(Z, Y)\n",
    "    \n",
    "    def gradcheck(self, X, Y, epsilon=1e-7, start=None, end=None):\n",
    "        self.approx_grads = []\n",
    "        self.true_grads = []\n",
    "        for layer in self.layers[start:end]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            for i in range(layer.W.size):\n",
    "                i = np.unravel_index(i, layer.W.shape)\n",
    "                Wi = layer.W[i]\n",
    "                layer.W[i] = Wi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.W[i] = Wi\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "                \n",
    "            for i in range(layer.b.size):\n",
    "                i = np.unravel_index(i, layer.b.shape)\n",
    "                bi = layer.b[i]\n",
    "                layer.b[i] = bi + epsilon\n",
    "                J1 = self._cost(X, Y)\n",
    "                layer.b[i] = bi - epsilon\n",
    "                J2 = self._cost(X, Y)\n",
    "                layer.b[i] = bi\n",
    "                self.approx_grads.append((J1-J2)/(2*epsilon))\n",
    "        \n",
    "        # FORWARD PROP\n",
    "        Z = X\n",
    "        for prop_layer in self.layers:\n",
    "            Z = prop_layer.forward(Z)\n",
    "        # BACKWARD PROP\n",
    "        dA = - (np.divide(Y, Z) - np.divide(1 - Y, 1 - Z)) # derivative of cost with respect to AL\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        \n",
    "        for layer in self.layers[start:end]:\n",
    "            if not layer.trainable:\n",
    "                continue\n",
    "            self.true_grads = np.concatenate((self.true_grads, layer.dW.flatten(), layer.db.flatten()))\n",
    "        self.approx_grads = np.array(self.approx_grads)\n",
    "        self.true_grads = np.array(self.true_grads)\n",
    "        return np.sqrt(np.sum(np.square(self.true_grads-self.approx_grads)))/(np.sqrt(np.sum(np.square(self.true_grads)))+np.sqrt(np.sum(np.square(self.approx_grads))))\n",
    "    \n",
    "        \n",
    "    \n",
    "class knn_differentiable(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__(input_shape, num_classes, False)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_features, batch_labels):\n",
    "        self.batch_features = np.transpose(batch_features).astype('float')\n",
    "#         print(\"BATCH_FEATURES\\n\", self.batch_features)\n",
    "#         print()\n",
    "        self.batch_labels = batch_labels.astype('float')\n",
    "        \n",
    "        self.distances = self.calc_distance_mtx(self.batch_features, self.batch_features) * (1/1)\n",
    "        #self.distances = np.divide(1, self.distances, where=self.distances!=0)\n",
    "#         print(\"DISTANCES\\n\",self.distances,self.distances.shape)\n",
    "#         print()\n",
    "        \n",
    "        self.class_0 = np.array(self.batch_labels[:] == 0).astype('float')\n",
    "        self.class_1 = np.array(self.batch_labels[:] == 1).astype('float')\n",
    "        self.num_0 = np.count_nonzero(self.batch_labels == 0)\n",
    "        self.num_1 = np.count_nonzero(self.batch_labels == 1)\n",
    "#         print(\"SIZES\", self.num_0, self.num_1)\n",
    "#         print()\n",
    "\n",
    "        self.aggregate = np.stack([np.sum(np.multiply(self.distances, self.class_0), 1) / self.num_0, np.sum(np.multiply(self.distances, self.class_1), 1) / self.num_1], axis=1)\n",
    "#         print(\"AGGREGATE\\n\",self.aggregate,self.aggregate.shape)\n",
    "#         print()\n",
    "        \n",
    "        exp = np.exp(-self.aggregate)\n",
    "#         print(\"EXP OF AGGREGATE\\n\",exp)\n",
    "#         print(exp[:,0].shape)\n",
    "#         print()\n",
    "        \n",
    "        self.softmax = np.divide(exp[:,1], np.sum(exp, 1))\n",
    "#         print(\"SOFTMAX\\n\",self.softmax[:50])\n",
    "#         print(\"=\"*20)\n",
    "#         print()\n",
    "        \n",
    "        return np.reshape(self.softmax, (1,self.distances.shape[0]))\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        # Overall what we need here:\n",
    "        # d(TL)/d(X = features) = d(TL)/D(L) * d(L)/d(S) * \n",
    "        #     [(d(S)/d(A_0) * d(A_0)/d(D_0) * d(D_0)/d(X)) + (d(S)/d(A_1) * d(A_1)/d(D_1) * d(D_1)/d(X))]\n",
    "        #\n",
    "        # Shapes:\n",
    "        # d(TL)/d(L)  -  (N x 1)           N = number of samples (images)\n",
    "        # d(L)/d(S)   -  (N x 1)\n",
    "        # d(S)/d(Ai)  -  (N x 1)\n",
    "        # d(Ai)/d(Di) -  (N x N)\n",
    "        # d(Di)/d(X)  -  (N x f)           f = number of features (output by cnn part)\n",
    "        # final output:  (N x f)\n",
    "        \n",
    "        \n",
    "        # d(TL)/d(L) = -1/m * sum(d(L)/d(S))  <-- Maybe don't need this at all, done before calling backward\n",
    "        \n",
    "        \n",
    "        # d(L)/d(S) = (Y/S) - (1-Y)/(1-S)    <-- Y = true labels, S = softmax labels from column 0 (softmax on class 0)\n",
    "        dL_dS = dA.T   # <-- Really dTL_dS\n",
    "        \n",
    "#         print(\"dL_dS\", dL_dS.shape, dL_dS[:25])\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(S)/d(A) = - (e^-A0 * e^-A1) / (e^-2A0 + e^-A0-A1 + e^-2A1)\n",
    "        # This calc is for A_0 and A_1, A_1 is positive version of this\n",
    "        #np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))\n",
    "        dS_dA0 = -np.divide(np.multiply(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])), \n",
    "                            np.square(np.add(np.exp(-self.aggregate[:,0]), np.exp(-self.aggregate[:,1])))).reshape(self.distances.shape[0],1)\n",
    "        \n",
    "        # np.add(np.add(np.exp(-2*self.aggregate[:,0]), np.exp(-2*self.aggregate[:,1])), \n",
    "        #                           np.exp(np.multiply(self.aggregate[:,0], self.aggregate[:,1])))\n",
    "        \n",
    "        \n",
    "        dS_dA1 = -dS_dA0\n",
    "#         print(\"dS_dA0\", dS_dA0.shape, dS_dA0[:25]) (1000,) (1000,1)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "        # d(A)/d(D) = 1/len(class) * sum(d(D)/d(X))   <-- Same calc for A_0 and A_1, len(class) = how many of this class there are either num_0 or num_1\n",
    "        # possibly don't need this part either since its calculated within loops???\n",
    "        \n",
    "        \n",
    "        # d(D)/d(X) = 2(x_i - x_j)   <-- x_i and x_j are the two feature vectors used in the distance\n",
    "        #distances_0 = \n",
    "        dA0_dX = np.ones(self.batch_features.shape)\n",
    "        # iterate over rows in distance matrix\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD_dX = np.zeros(self.batch_features.shape[1])\n",
    "            # iterate over the columns in distance matrix\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                # only calculate derivatives for indices of class 0\n",
    "                if self.class_0[:,j] == 0:\n",
    "                    # first calc derivative of distance formula, then them all together\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD_dX = np.add(dD_dX, deriv)\n",
    "            # replace the corresponding derivatives for each row (sample)\n",
    "            dA0_dX[i,:] = dD_dX\n",
    "        \n",
    "        # Same as loop above but on indices of class 1\n",
    "        dA1_dX = np.ones(self.batch_features.shape)\n",
    "        for i in range(self.distances.shape[0]):\n",
    "            dD_dX = np.zeros(self.batch_features.shape[1])\n",
    "            for j in range(self.distances.shape[1]):\n",
    "                if self.class_0[:,j] == 1:\n",
    "                    deriv = -2*(self.batch_features[i,:] - self.batch_features[j,:])\n",
    "                    dD_dX = np.add(dD_dX, deriv)\n",
    "            dA1_dX[i,:] = dD_dX\n",
    "        \n",
    "\n",
    "        # divide by the number of samples in each class since these were averaged in forward\n",
    "        dA0_dX = dA0_dX * (1/self.num_0) * (1/1)\n",
    "        dA1_dX = dA1_dX * (1/self.num_1) * (1/1)\n",
    "#         print(\"dD0_dX\", dD0_dX.shape, dD0_dX)\n",
    "#         print()\n",
    "#         print(\"dD1_dX\", dD0_dX.shape, dD1_dX)\n",
    "#         print()\n",
    "\n",
    "        # add together the terms in brackets from the full formula\n",
    "        dS_dX = np.add(np.multiply(dS_dA0, dA0_dX), np.multiply(dS_dA1, dA1_dX))\n",
    "        # final step of the chain rule\n",
    "        dL_dX = np.multiply(dL_dS, dS_dX)\n",
    "#         print(\"dL_dX\", dL_dX.shape, dL_dX)\n",
    "        \n",
    "#         print('='*50)\n",
    "        return dL_dX\n",
    "        #return np.zeros(self.batch_features.shape)\n",
    "    \n",
    "    def calc_distance_mtx(self, A, B):\n",
    "        \"\"\"\n",
    "        Computes squared pairwise distances between each elements of A and each elements of B.\n",
    "        Args:\n",
    "        A,    [m,d] matrix\n",
    "        B,    [n,d] matrix\n",
    "        Returns:\n",
    "        D,    [m,n] matrix of pairwise distances\n",
    "        \"\"\"\n",
    "        with tf.compat.v1.variable_scope('pairwise_dist'):\n",
    "            # squared norms of each row in A and B\n",
    "            na = tf.reduce_sum(tf.square(A), 1)\n",
    "            nb = tf.reduce_sum(tf.square(B), 1)\n",
    "\n",
    "            # na as a row and nb as a co\"lumn vectors\n",
    "            na = tf.reshape(na, [-1, 1])\n",
    "            nb = tf.reshape(nb, [1, -1])\n",
    "\n",
    "            # return pairwise euclidead difference matrix\n",
    "            D = tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0)\n",
    "        return D.numpy()\n",
    "    \n",
    "    def calc_cosine_sim(self, A, B):\n",
    "        from scipy.spatial.distance import pdist\n",
    "        from scipy.spatial.distance import squareform\n",
    "        cos_sim = squareform(pdist(A, metric='cosine'))\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Code below for knn layer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture for Handwritten Digits\n",
    "#         Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "#         Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "#         Flatten((None, 8, 8, 32)),\n",
    "#         knn_differentiable((2048, None), 2)\n",
    "\n",
    "#         Dense(32, (2048, None), \"relu\"),\n",
    "#         Dense(1, (32, None), \"sigmoid\")\n",
    "\n",
    "\n",
    "\n",
    "# Architecture for Fashion\n",
    "#         Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "#         Maxpool((None, 26, 26, 32), pool_size=2),\n",
    "#         Conv2D(64, 3, (None, 13, 13, 32)),\n",
    "#         Maxpool((None, 11, 11, 64), pool_size=2),\n",
    "#         Flatten((None, 5, 5, 64)),\n",
    "#         knn_differentiable((1600, None), 2)\n",
    "\n",
    "#         Dense(128, (1600, None), \"relu\"),\n",
    "#         Dense(1, (128, None), \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 in epoch 0 - Cost: 0.69224815\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 1 - Cost: 0.69224815\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 2 - Cost: 0.69224815\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 3 - Cost: 0.69224814\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 4 - Cost: 0.69224814\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 5 - Cost: 0.69224814\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 6 - Cost: 0.69224815\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 7 - Cost: 0.69224814\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 8 - Cost: 0.69224814\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 0 in epoch 9 - Cost: 0.69224815\r"
     ]
    }
   ],
   "source": [
    "# Change below three parameters for experiments \n",
    "train_size = 1000\n",
    "batch_size = 1000\n",
    "learning = 0.005\n",
    "\n",
    "\n",
    "\n",
    "times = []\n",
    "accuracy_lst = [0,0,0,0]\n",
    "for i in range(1):\n",
    "    # Select only m samples\n",
    "    m = train_size\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "    y = y_train[:m].values.reshape(1,m)\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 13, 13, 32)),\n",
    "        Maxpool((None, 11, 11, 64), pool_size=2),\n",
    "        Flatten((None, 5, 5, 64)),\n",
    "        knn_differentiable((1600, None), 2)\n",
    "    ]\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=12, learning_rate=learning, verbose=1, batch_size=batch_size)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Runs through each testing batch size for the table and gathers accuracies\n",
    "    batch_sizes = [2000, 1000, batch_size, 15]\n",
    "    for j in range(len(batch_sizes)):\n",
    "        m = batch_sizes[j]\n",
    "        n = 0\n",
    "        accuracies = []\n",
    "        for l in range(y_test.values.shape[0]//m):\n",
    "            X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "            y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "            predictions = model.predict(X, y)\n",
    "            #print(predictions[:,:10])\n",
    "            score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "            accuracies.append(score)\n",
    "\n",
    "            n = m\n",
    "            m += batch_sizes[j]\n",
    "\n",
    "        print(accuracies)\n",
    "        avg_acc = np.average(accuracies)\n",
    "        accuracy_lst[j] += avg_acc\n",
    "        print(\"Accuracy\", j+1, \"-\", avg_acc)\n",
    "        print()\n",
    "    print(\"+\"*50, \"end of loop\", i+1)\n",
    "    print()\n",
    "\n",
    "# Output average execution time and average accuracies in a list of the same order as shown in tables\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracies:\", np.divide(accuracy_lst, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.898      0.898      0.90574597 0.91328321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment code for fully connected layers (MNIST datasets done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 187 in epoch 0 - Cost: 0.54812546\n",
      "\n",
      " =========================================================================== \n",
      "\n",
      "Step 22 in epoch 1 - Cost: 0.55419837\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17140/1266723322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mex_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17140/4253403678.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, epochs, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                     \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m                 \u001b[1;31m# UPDATE PARAMS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17140/4253403678.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dZ)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2DBackpropInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2DBackpropFilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBiasAddGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_backprop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[1;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[1;32m--> 404\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m       return conv2d_backprop_filter_eager_fallback(\n\u001b[0m\u001b[0;32m   1098\u001b[0m           \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter_eager_fallback\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[0;32m   1189\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m-> 1191\u001b[1;33m   _result = _execute.execute(b\"Conv2DBackpropFilter\", 1, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m   1192\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m   1193\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Research\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = []\n",
    "accuracy_lst = []\n",
    "for k in range(10):\n",
    "    # Select only m samples for fast training time during debugging\n",
    "    #np.random.seed(10)\n",
    "    m = 6000\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "    y = y_train[:m].values.reshape(1,m)\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=2),\n",
    "        Conv2D(64, 3, (None, 13, 13, 32)),\n",
    "        Maxpool((None, 11, 11, 64), pool_size=2),\n",
    "        Flatten((None, 5, 5, 64)),\n",
    "        Dense(128, (1600, None), \"relu\"),\n",
    "        Dense(1, (128, None), \"sigmoid\")\n",
    "    ]\n",
    "\n",
    "    #     k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    #     k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    #     k.layers.MaxPooling2D((3,3)),\n",
    "    #     k.layers.Flatten(),\n",
    "    #     k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    #     k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=0.00001, verbose=1, batch_size=32)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 2000\n",
    "    n = 0\n",
    "    m = batch_size\n",
    "    accuracies = []\n",
    "    #print(y_test.values.shape[0]//m)\n",
    "    for i in range(y_test.values.shape[0]//m):\n",
    "        X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "        y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "        predictions = model.predict(X, y)\n",
    "        #print(predictions.type)\n",
    "        #print(y[:,:100])\n",
    "        print(predictions[:,:10])\n",
    "        score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "        #print(score)\n",
    "        accuracies.append(score)\n",
    "\n",
    "        n = m\n",
    "        m += batch_size\n",
    "        #print(m,n)\n",
    "\n",
    "    print(accuracies)\n",
    "    avg_acc = np.average(accuracies)\n",
    "    accuracy_lst.append(avg_acc)\n",
    "    print(\"Average accuracy:\", avg_acc)\n",
    "    print()\n",
    "    print(\"+\"*50, \"end of loop\", k+1)\n",
    "    print()\n",
    "    \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracy:\", np.average(accuracy_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9408000 into shape (12115,28,28,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17140/2275813350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#np.random.seed(10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12115\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Define the layers of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 9408000 into shape (12115,28,28,1)"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "accuracy_lst = []\n",
    "for k in range(10):\n",
    "    # Select only m samples for fast training time during debugging\n",
    "    #np.random.seed(10)\n",
    "    m = 12115\n",
    "    X = X_train[:m, :, :].reshape((m, 28, 28, 1)).astype(float)\n",
    "    y = y_train[:m].values.reshape(1,m)\n",
    "    # Define the layers of the model\n",
    "    layers = [\n",
    "        Conv2D(32, 3, (None, 28, 28, 1)),\n",
    "        Maxpool((None, 26, 26, 32), pool_size=3),\n",
    "        #Conv2D(32, 3, (None, 8, 8, 32)),\n",
    "        Flatten((None, 8, 8, 32)),\n",
    "        Dense(32, (2048, None), \"relu\"),\n",
    "        Dense(1, (32, None), \"sigmoid\")\n",
    "    ]\n",
    "\n",
    "    #     k.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    #     k.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer=initializer),\n",
    "    #     k.layers.MaxPooling2D((3,3)),\n",
    "    #     k.layers.Flatten(),\n",
    "    #     k.layers.Dense(32, activation=\"relu\", kernel_initializer=initializer),\n",
    "    #     k.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)\n",
    "\n",
    "    # Create and train model\n",
    "    model = Model(layers)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, epochs=10, learning_rate=0.0002, verbose=1, batch_size=32)\n",
    "    end_time = time.time()\n",
    "    ex_time = end_time-start_time\n",
    "    times.append(ex_time)\n",
    "    print(\"Execution Time:\", ex_time)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 2115\n",
    "    n = 0\n",
    "    m = batch_size\n",
    "    accuracies = []\n",
    "    #print(y_test.values.shape[0]//m)\n",
    "    for i in range(y_test.values.shape[0]//m):\n",
    "        X = X_test[n:m, :, :].reshape((m-n, 28, 28, 1)).astype(float)\n",
    "        y = y_test[n:m].values.reshape(1,m-n).astype(float)\n",
    "        predictions = model.predict(X, y)\n",
    "        #print(predictions.type)\n",
    "        #print(y[:,:100])\n",
    "        print(predictions[:,:100])\n",
    "        score = accuracy_score(y.flatten(), predictions.flatten().round())\n",
    "        #print(score)\n",
    "        accuracies.append(score)\n",
    "\n",
    "        n = m\n",
    "        m += batch_size\n",
    "        #print(m,n)\n",
    "\n",
    "    print(accuracies)\n",
    "    avg_acc = np.average(accuracies)\n",
    "    accuracy_lst.append(avg_acc)\n",
    "    print(\"Average accuracy:\", avg_acc)\n",
    "    print()\n",
    "    print(\"+\"*50, \"end of loop\", k+1)\n",
    "    print()\n",
    "    \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Average Execution Time:\", np.average(times), \"  ---  \", \"Average Accuracy:\", np.average(accuracy_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
